{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to add any functions, import statements, and variables.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c5294b630e41>\", line 5, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\matplotlib\\pyplot.py\", line 69, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\matplotlib\\backends\\__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1] loss: 467.4664001464844\n",
      "[step: 2] loss: 71.1634750366211\n",
      "[step: 3] loss: 1492.25732421875\n",
      "[step: 4] loss: 52.24040985107422\n",
      "[step: 5] loss: 160.373291015625\n",
      "[step: 6] loss: 228.22186279296875\n",
      "[step: 7] loss: 204.85009765625\n",
      "[step: 8] loss: 154.68817138671875\n",
      "[step: 9] loss: 105.41128540039062\n",
      "[step: 10] loss: 69.23163604736328\n",
      "[step: 11] loss: 55.19346618652344\n",
      "[step: 12] loss: 66.52235412597656\n",
      "[step: 13] loss: 82.0766372680664\n",
      "[step: 14] loss: 80.37244415283203\n",
      "[step: 15] loss: 68.62311553955078\n",
      "[step: 16] loss: 58.73807907104492\n",
      "[step: 17] loss: 54.69040298461914\n",
      "[step: 18] loss: 55.129817962646484\n",
      "[step: 19] loss: 57.541568756103516\n",
      "[step: 20] loss: 59.97977828979492\n",
      "[step: 21] loss: 61.3826789855957\n",
      "[step: 22] loss: 61.44755935668945\n",
      "[step: 23] loss: 60.394195556640625\n",
      "[step: 24] loss: 58.71413803100586\n",
      "[step: 25] loss: 56.9549674987793\n",
      "[step: 26] loss: 55.564998626708984\n",
      "[step: 27] loss: 54.80377197265625\n",
      "[step: 28] loss: 54.71357727050781\n",
      "[step: 29] loss: 55.144344329833984\n",
      "[step: 30] loss: 55.822715759277344\n",
      "[step: 31] loss: 56.44782257080078\n",
      "[step: 32] loss: 56.786293029785156\n",
      "[step: 33] loss: 56.735321044921875\n",
      "[step: 34] loss: 56.33366012573242\n",
      "[step: 35] loss: 55.723121643066406\n",
      "[step: 36] loss: 55.08311462402344\n",
      "[step: 37] loss: 54.56709289550781\n",
      "[step: 38] loss: 54.26150131225586\n",
      "[step: 39] loss: 54.17399978637695\n",
      "[step: 40] loss: 54.246620178222656\n",
      "[step: 41] loss: 54.38386535644531\n",
      "[step: 42] loss: 54.485435485839844\n",
      "[step: 43] loss: 54.47459030151367\n",
      "[step: 44] loss: 54.31590270996094\n",
      "[step: 45] loss: 54.019126892089844\n",
      "[step: 46] loss: 53.62896728515625\n",
      "[step: 47] loss: 53.20486831665039\n",
      "[step: 48] loss: 52.796836853027344\n",
      "[step: 49] loss: 52.424888610839844\n",
      "[step: 50] loss: 52.0681266784668\n",
      "[step: 51] loss: 51.66617965698242\n",
      "[step: 52] loss: 51.13209533691406\n",
      "[step: 53] loss: 50.373172760009766\n",
      "[step: 54] loss: 49.317928314208984\n",
      "[step: 55] loss: 47.95621109008789\n",
      "[step: 56] loss: 46.409385681152344\n",
      "[step: 57] loss: 44.98823547363281\n",
      "[step: 58] loss: 43.82734298706055\n",
      "[step: 59] loss: 41.82606887817383\n",
      "[step: 60] loss: 38.61551284790039\n",
      "[step: 61] loss: 36.13581085205078\n",
      "[step: 62] loss: 34.34150314331055\n",
      "[step: 63] loss: 31.841045379638672\n",
      "[step: 64] loss: 30.20956039428711\n",
      "[step: 65] loss: 29.96700668334961\n",
      "[step: 66] loss: 29.010997772216797\n",
      "[step: 67] loss: 29.98208236694336\n",
      "[step: 68] loss: 29.95565414428711\n",
      "[step: 69] loss: 30.296804428100586\n",
      "[step: 70] loss: 30.448795318603516\n",
      "[step: 71] loss: 29.824474334716797\n",
      "[step: 72] loss: 29.781143188476562\n",
      "[step: 73] loss: 29.044296264648438\n",
      "[step: 74] loss: 28.89706802368164\n",
      "[step: 75] loss: 28.815670013427734\n",
      "[step: 76] loss: 28.573223114013672\n",
      "[step: 77] loss: 28.798856735229492\n",
      "[step: 78] loss: 28.79926300048828\n",
      "[step: 79] loss: 28.734214782714844\n",
      "[step: 80] loss: 28.844619750976562\n",
      "[step: 81] loss: 28.700180053710938\n",
      "[step: 82] loss: 28.529773712158203\n",
      "[step: 83] loss: 28.470035552978516\n",
      "[step: 84] loss: 28.234180450439453\n",
      "[step: 85] loss: 28.02817726135254\n",
      "[step: 86] loss: 27.919822692871094\n",
      "[step: 87] loss: 27.70805549621582\n",
      "[step: 88] loss: 27.55963706970215\n",
      "[step: 89] loss: 27.481124877929688\n",
      "[step: 90] loss: 27.328018188476562\n",
      "[step: 91] loss: 27.243122100830078\n",
      "[step: 92] loss: 27.18976593017578\n",
      "[step: 93] loss: 27.082927703857422\n",
      "[step: 94] loss: 27.035663604736328\n",
      "[step: 95] loss: 26.983440399169922\n",
      "[step: 96] loss: 26.89468002319336\n",
      "[step: 97] loss: 26.85042953491211\n",
      "[step: 98] loss: 26.78057861328125\n",
      "[step: 99] loss: 26.697011947631836\n",
      "[step: 100] loss: 26.641992568969727\n",
      "[step: 101] loss: 26.55426025390625\n",
      "[step: 102] loss: 26.469650268554688\n",
      "[step: 103] loss: 26.396425247192383\n",
      "[step: 104] loss: 26.29737091064453\n",
      "[step: 105] loss: 26.211952209472656\n",
      "[step: 106] loss: 26.12038803100586\n",
      "[step: 107] loss: 26.011516571044922\n",
      "[step: 108] loss: 25.913841247558594\n",
      "[step: 109] loss: 25.80032730102539\n",
      "[step: 110] loss: 25.68309211730957\n",
      "[step: 111] loss: 25.566829681396484\n",
      "[step: 112] loss: 25.432157516479492\n",
      "[step: 113] loss: 25.300193786621094\n",
      "[step: 114] loss: 25.158279418945312\n",
      "[step: 115] loss: 25.00883674621582\n",
      "[step: 116] loss: 24.858497619628906\n",
      "[step: 117] loss: 24.692935943603516\n",
      "[step: 118] loss: 24.526775360107422\n",
      "[step: 119] loss: 24.350065231323242\n",
      "[step: 120] loss: 24.16854476928711\n",
      "[step: 121] loss: 23.980527877807617\n",
      "[step: 122] loss: 23.78209686279297\n",
      "[step: 123] loss: 23.58161163330078\n",
      "[step: 124] loss: 23.37270736694336\n",
      "[step: 125] loss: 23.163639068603516\n",
      "[step: 126] loss: 22.94659996032715\n",
      "[step: 127] loss: 22.731653213500977\n",
      "[step: 128] loss: 22.51508903503418\n",
      "[step: 129] loss: 22.305274963378906\n",
      "[step: 130] loss: 22.09906005859375\n",
      "[step: 131] loss: 21.907617568969727\n",
      "[step: 132] loss: 21.731563568115234\n",
      "[step: 133] loss: 21.57958221435547\n",
      "[step: 134] loss: 21.453392028808594\n",
      "[step: 135] loss: 21.356557846069336\n",
      "[step: 136] loss: 21.29035186767578\n",
      "[step: 137] loss: 21.24720573425293\n",
      "[step: 138] loss: 21.222482681274414\n",
      "[step: 139] loss: 21.2093505859375\n",
      "[step: 140] loss: 21.199642181396484\n",
      "[step: 141] loss: 21.189861297607422\n",
      "[step: 142] loss: 21.18056297302246\n",
      "[step: 143] loss: 21.17324447631836\n",
      "[step: 144] loss: 21.167892456054688\n",
      "[step: 145] loss: 21.16254234313965\n",
      "[step: 146] loss: 21.154325485229492\n",
      "[step: 147] loss: 21.14093017578125\n",
      "[step: 148] loss: 21.122060775756836\n",
      "[step: 149] loss: 21.0992374420166\n",
      "[step: 150] loss: 21.075214385986328\n",
      "[step: 151] loss: 21.052337646484375\n",
      "[step: 152] loss: 21.031944274902344\n",
      "[step: 153] loss: 21.014575958251953\n",
      "[step: 154] loss: 21.000518798828125\n",
      "[step: 155] loss: 20.99010467529297\n",
      "[step: 156] loss: 20.982831954956055\n",
      "[step: 157] loss: 20.977523803710938\n",
      "[step: 158] loss: 20.973388671875\n",
      "[step: 159] loss: 20.970191955566406\n",
      "[step: 160] loss: 20.967464447021484\n",
      "[step: 161] loss: 20.964576721191406\n",
      "[step: 162] loss: 20.961332321166992\n",
      "[step: 163] loss: 20.957700729370117\n",
      "[step: 164] loss: 20.953384399414062\n",
      "[step: 165] loss: 20.948135375976562\n",
      "[step: 166] loss: 20.94206428527832\n",
      "[step: 167] loss: 20.935407638549805\n",
      "[step: 168] loss: 20.928281784057617\n",
      "[step: 169] loss: 20.92081642150879\n",
      "[step: 170] loss: 20.913244247436523\n",
      "[step: 171] loss: 20.905925750732422\n",
      "[step: 172] loss: 20.89913558959961\n",
      "[step: 173] loss: 20.89306640625\n",
      "[step: 174] loss: 20.88783073425293\n",
      "[step: 175] loss: 20.8834228515625\n",
      "[step: 176] loss: 20.879777908325195\n",
      "[step: 177] loss: 20.87672996520996\n",
      "[step: 178] loss: 20.874080657958984\n",
      "[step: 179] loss: 20.871593475341797\n",
      "[step: 180] loss: 20.869064331054688\n",
      "[step: 181] loss: 20.866313934326172\n",
      "[step: 182] loss: 20.863239288330078\n",
      "[step: 183] loss: 20.859827041625977\n",
      "[step: 184] loss: 20.85614776611328\n",
      "[step: 185] loss: 20.85232162475586\n",
      "[step: 186] loss: 20.848554611206055\n",
      "[step: 187] loss: 20.845163345336914\n",
      "[step: 188] loss: 20.842782974243164\n",
      "[step: 189] loss: 20.842731475830078\n",
      "[step: 190] loss: 20.84815788269043\n",
      "[step: 191] loss: 20.866018295288086\n",
      "[step: 192] loss: 20.909135818481445\n",
      "[step: 193] loss: 20.994043350219727\n",
      "[step: 194] loss: 21.090560913085938\n",
      "[step: 195] loss: 21.132667541503906\n",
      "[step: 196] loss: 20.992244720458984\n",
      "[step: 197] loss: 20.836013793945312\n",
      "[step: 198] loss: 20.838069915771484\n",
      "[step: 199] loss: 20.945419311523438\n",
      "[step: 200] loss: 20.9687557220459\n",
      "[step: 201] loss: 20.85812759399414\n",
      "[step: 202] loss: 20.804912567138672\n",
      "[step: 203] loss: 20.867767333984375\n",
      "[step: 204] loss: 20.903982162475586\n",
      "[step: 205] loss: 20.848018646240234\n",
      "[step: 206] loss: 20.793933868408203\n",
      "[step: 207] loss: 20.822647094726562\n",
      "[step: 208] loss: 20.862167358398438\n",
      "[step: 209] loss: 20.833436965942383\n",
      "[step: 210] loss: 20.788105010986328\n",
      "[step: 211] loss: 20.79143524169922\n",
      "[step: 212] loss: 20.82176399230957\n",
      "[step: 213] loss: 20.82311248779297\n",
      "[step: 214] loss: 20.79081916809082\n",
      "[step: 215] loss: 20.772260665893555\n",
      "[step: 216] loss: 20.784534454345703\n",
      "[step: 217] loss: 20.800251007080078\n",
      "[step: 218] loss: 20.794897079467773\n",
      "[step: 219] loss: 20.773563385009766\n",
      "[step: 220] loss: 20.761119842529297\n",
      "[step: 221] loss: 20.765960693359375\n",
      "[step: 222] loss: 20.775938034057617\n",
      "[step: 223] loss: 20.77713966369629\n",
      "[step: 224] loss: 20.766468048095703\n",
      "[step: 225] loss: 20.753740310668945\n",
      "[step: 226] loss: 20.747486114501953\n",
      "[step: 227] loss: 20.74917984008789\n",
      "[step: 228] loss: 20.754009246826172\n",
      "[step: 229] loss: 20.756153106689453\n",
      "[step: 230] loss: 20.75375747680664\n",
      "[step: 231] loss: 20.747318267822266\n",
      "[step: 232] loss: 20.740020751953125\n",
      "[step: 233] loss: 20.73382568359375\n",
      "[step: 234] loss: 20.729814529418945\n",
      "[step: 235] loss: 20.72783851623535\n",
      "[step: 236] loss: 20.727373123168945\n",
      "[step: 237] loss: 20.728120803833008\n",
      "[step: 238] loss: 20.730138778686523\n",
      "[step: 239] loss: 20.73467254638672\n",
      "[step: 240] loss: 20.743511199951172\n",
      "[step: 241] loss: 20.762723922729492\n",
      "[step: 242] loss: 20.799978256225586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 243] loss: 20.88075828552246\n",
      "[step: 244] loss: 21.019710540771484\n",
      "[step: 245] loss: 21.269060134887695\n",
      "[step: 246] loss: 21.44988250732422\n",
      "[step: 247] loss: 21.4472713470459\n",
      "[step: 248] loss: 21.00048828125\n",
      "[step: 249] loss: 20.708751678466797\n",
      "[step: 250] loss: 20.882568359375\n",
      "[step: 251] loss: 21.073476791381836\n",
      "[step: 252] loss: 20.910602569580078\n",
      "[step: 253] loss: 20.706056594848633\n",
      "[step: 254] loss: 20.84280014038086\n",
      "[step: 255] loss: 20.94533348083496\n",
      "[step: 256] loss: 20.762897491455078\n",
      "[step: 257] loss: 20.721942901611328\n",
      "[step: 258] loss: 20.85396957397461\n",
      "[step: 259] loss: 20.79787826538086\n",
      "[step: 260] loss: 20.69478416442871\n",
      "[step: 261] loss: 20.761720657348633\n",
      "[step: 262] loss: 20.79488182067871\n",
      "[step: 263] loss: 20.711214065551758\n",
      "[step: 264] loss: 20.70081901550293\n",
      "[step: 265] loss: 20.76028060913086\n",
      "[step: 266] loss: 20.736587524414062\n",
      "[step: 267] loss: 20.68413734436035\n",
      "[step: 268] loss: 20.712566375732422\n",
      "[step: 269] loss: 20.737857818603516\n",
      "[step: 270] loss: 20.69728660583496\n",
      "[step: 271] loss: 20.67998504638672\n",
      "[step: 272] loss: 20.709056854248047\n",
      "[step: 273] loss: 20.708831787109375\n",
      "[step: 274] loss: 20.67843246459961\n",
      "[step: 275] loss: 20.676124572753906\n",
      "[step: 276] loss: 20.694799423217773\n",
      "[step: 277] loss: 20.688884735107422\n",
      "[step: 278] loss: 20.66819953918457\n",
      "[step: 279] loss: 20.669368743896484\n",
      "[step: 280] loss: 20.681278228759766\n",
      "[step: 281] loss: 20.675458908081055\n",
      "[step: 282] loss: 20.661218643188477\n",
      "[step: 283] loss: 20.66027069091797\n",
      "[step: 284] loss: 20.66781997680664\n",
      "[step: 285] loss: 20.666505813598633\n",
      "[step: 286] loss: 20.656295776367188\n",
      "[step: 287] loss: 20.65103530883789\n",
      "[step: 288] loss: 20.654220581054688\n",
      "[step: 289] loss: 20.656875610351562\n",
      "[step: 290] loss: 20.65304946899414\n",
      "[step: 291] loss: 20.646005630493164\n",
      "[step: 292] loss: 20.642183303833008\n",
      "[step: 293] loss: 20.642864227294922\n",
      "[step: 294] loss: 20.644588470458984\n",
      "[step: 295] loss: 20.644081115722656\n",
      "[step: 296] loss: 20.640609741210938\n",
      "[step: 297] loss: 20.636093139648438\n",
      "[step: 298] loss: 20.63232421875\n",
      "[step: 299] loss: 20.630184173583984\n",
      "[step: 300] loss: 20.62945556640625\n",
      "[step: 301] loss: 20.629499435424805\n",
      "[step: 302] loss: 20.629907608032227\n",
      "[step: 303] loss: 20.63048553466797\n",
      "[step: 304] loss: 20.631717681884766\n",
      "[step: 305] loss: 20.634082794189453\n",
      "[step: 306] loss: 20.639482498168945\n",
      "[step: 307] loss: 20.650266647338867\n",
      "[step: 308] loss: 20.674142837524414\n",
      "[step: 309] loss: 20.72245979309082\n",
      "[step: 310] loss: 20.831867218017578\n",
      "[step: 311] loss: 21.04271125793457\n",
      "[step: 312] loss: 21.480182647705078\n",
      "[step: 313] loss: 21.976669311523438\n",
      "[step: 314] loss: 22.335317611694336\n",
      "[step: 315] loss: 21.517932891845703\n",
      "[step: 316] loss: 20.67178726196289\n",
      "[step: 317] loss: 20.97056770324707\n",
      "[step: 318] loss: 21.32419204711914\n",
      "[step: 319] loss: 20.819536209106445\n",
      "[step: 320] loss: 20.735050201416016\n",
      "[step: 321] loss: 21.09208869934082\n",
      "[step: 322] loss: 20.767189025878906\n",
      "[step: 323] loss: 20.723546981811523\n",
      "[step: 324] loss: 20.94061279296875\n",
      "[step: 325] loss: 20.665687561035156\n",
      "[step: 326] loss: 20.759769439697266\n",
      "[step: 327] loss: 20.801420211791992\n",
      "[step: 328] loss: 20.62254524230957\n",
      "[step: 329] loss: 20.769025802612305\n",
      "[step: 330] loss: 20.692041397094727\n",
      "[step: 331] loss: 20.641904830932617\n",
      "[step: 332] loss: 20.740636825561523\n",
      "[step: 333] loss: 20.632802963256836\n",
      "[step: 334] loss: 20.67368507385254\n",
      "[step: 335] loss: 20.692426681518555\n",
      "[step: 336] loss: 20.619855880737305\n",
      "[step: 337] loss: 20.68405532836914\n",
      "[step: 338] loss: 20.644641876220703\n",
      "[step: 339] loss: 20.628416061401367\n",
      "[step: 340] loss: 20.667510986328125\n",
      "[step: 341] loss: 20.616649627685547\n",
      "[step: 342] loss: 20.63703155517578\n",
      "[step: 343] loss: 20.638839721679688\n",
      "[step: 344] loss: 20.609344482421875\n",
      "[step: 345] loss: 20.63679313659668\n",
      "[step: 346] loss: 20.61553955078125\n",
      "[step: 347] loss: 20.612558364868164\n",
      "[step: 348] loss: 20.62618064880371\n",
      "[step: 349] loss: 20.603818893432617\n",
      "[step: 350] loss: 20.614349365234375\n",
      "[step: 351] loss: 20.61263084411621\n",
      "[step: 352] loss: 20.59909439086914\n",
      "[step: 353] loss: 20.610485076904297\n",
      "[step: 354] loss: 20.601459503173828\n",
      "[step: 355] loss: 20.596479415893555\n",
      "[step: 356] loss: 20.603702545166016\n",
      "[step: 357] loss: 20.593727111816406\n",
      "[step: 358] loss: 20.593185424804688\n",
      "[step: 359] loss: 20.596885681152344\n",
      "[step: 360] loss: 20.588491439819336\n",
      "[step: 361] loss: 20.588953018188477\n",
      "[step: 362] loss: 20.590757369995117\n",
      "[step: 363] loss: 20.58415985107422\n",
      "[step: 364] loss: 20.584230422973633\n",
      "[step: 365] loss: 20.585277557373047\n",
      "[step: 366] loss: 20.58006477355957\n",
      "[step: 367] loss: 20.57921028137207\n",
      "[step: 368] loss: 20.580158233642578\n",
      "[step: 369] loss: 20.576261520385742\n",
      "[step: 370] loss: 20.5743350982666\n",
      "[step: 371] loss: 20.57510757446289\n",
      "[step: 372] loss: 20.572654724121094\n",
      "[step: 373] loss: 20.569963455200195\n",
      "[step: 374] loss: 20.570068359375\n",
      "[step: 375] loss: 20.56896209716797\n",
      "[step: 376] loss: 20.56617546081543\n",
      "[step: 377] loss: 20.56511116027832\n",
      "[step: 378] loss: 20.564769744873047\n",
      "[step: 379] loss: 20.562803268432617\n",
      "[step: 380] loss: 20.560733795166016\n",
      "[step: 381] loss: 20.560001373291016\n",
      "[step: 382] loss: 20.55914306640625\n",
      "[step: 383] loss: 20.557262420654297\n",
      "[step: 384] loss: 20.5555419921875\n",
      "[step: 385] loss: 20.554656982421875\n",
      "[step: 386] loss: 20.553726196289062\n",
      "[step: 387] loss: 20.552125930786133\n",
      "[step: 388] loss: 20.550460815429688\n",
      "[step: 389] loss: 20.54929542541504\n",
      "[step: 390] loss: 20.548355102539062\n",
      "[step: 391] loss: 20.547134399414062\n",
      "[step: 392] loss: 20.545602798461914\n",
      "[step: 393] loss: 20.544143676757812\n",
      "[step: 394] loss: 20.542953491210938\n",
      "[step: 395] loss: 20.541908264160156\n",
      "[step: 396] loss: 20.540773391723633\n",
      "[step: 397] loss: 20.53945541381836\n",
      "[step: 398] loss: 20.538063049316406\n",
      "[step: 399] loss: 20.53672218322754\n",
      "[step: 400] loss: 20.535497665405273\n",
      "[step: 401] loss: 20.534364700317383\n",
      "[step: 402] loss: 20.533252716064453\n",
      "[step: 403] loss: 20.532127380371094\n",
      "[step: 404] loss: 20.530963897705078\n",
      "[step: 405] loss: 20.529781341552734\n",
      "[step: 406] loss: 20.528583526611328\n",
      "[step: 407] loss: 20.52740478515625\n",
      "[step: 408] loss: 20.526260375976562\n",
      "[step: 409] loss: 20.52519416809082\n",
      "[step: 410] loss: 20.524250030517578\n",
      "[step: 411] loss: 20.523595809936523\n",
      "[step: 412] loss: 20.523527145385742\n",
      "[step: 413] loss: 20.524883270263672\n",
      "[step: 414] loss: 20.529651641845703\n",
      "[step: 415] loss: 20.54351043701172\n",
      "[step: 416] loss: 20.581186294555664\n",
      "[step: 417] loss: 20.688146591186523\n",
      "[step: 418] loss: 20.978492736816406\n",
      "[step: 419] loss: 21.7948055267334\n",
      "[step: 420] loss: 23.447858810424805\n",
      "[step: 421] loss: 25.69508171081543\n",
      "[step: 422] loss: 23.608781814575195\n",
      "[step: 423] loss: 20.71933364868164\n",
      "[step: 424] loss: 22.665172576904297\n",
      "[step: 425] loss: 21.266681671142578\n",
      "[step: 426] loss: 21.283605575561523\n",
      "[step: 427] loss: 21.743019104003906\n",
      "[step: 428] loss: 20.825756072998047\n",
      "[step: 429] loss: 21.651775360107422\n",
      "[step: 430] loss: 20.685321807861328\n",
      "[step: 431] loss: 21.398591995239258\n",
      "[step: 432] loss: 20.687185287475586\n",
      "[step: 433] loss: 21.198917388916016\n",
      "[step: 434] loss: 20.69264793395996\n",
      "[step: 435] loss: 21.002317428588867\n",
      "[step: 436] loss: 20.690465927124023\n",
      "[step: 437] loss: 20.854799270629883\n",
      "[step: 438] loss: 20.708663940429688\n",
      "[step: 439] loss: 20.768203735351562\n",
      "[step: 440] loss: 20.720312118530273\n",
      "[step: 441] loss: 20.688159942626953\n",
      "[step: 442] loss: 20.73173713684082\n",
      "[step: 443] loss: 20.639556884765625\n",
      "[step: 444] loss: 20.74566650390625\n",
      "[step: 445] loss: 20.605249404907227\n",
      "[step: 446] loss: 20.727378845214844\n",
      "[step: 447] loss: 20.58468246459961\n",
      "[step: 448] loss: 20.6994686126709\n",
      "[step: 449] loss: 20.585628509521484\n",
      "[step: 450] loss: 20.665647506713867\n",
      "[step: 451] loss: 20.58951759338379\n",
      "[step: 452] loss: 20.62812614440918\n",
      "[step: 453] loss: 20.598712921142578\n",
      "[step: 454] loss: 20.60032844543457\n",
      "[step: 455] loss: 20.60818099975586\n",
      "[step: 456] loss: 20.579294204711914\n",
      "[step: 457] loss: 20.607196807861328\n",
      "[step: 458] loss: 20.567134857177734\n",
      "[step: 459] loss: 20.600788116455078\n",
      "[step: 460] loss: 20.56415367126465\n",
      "[step: 461] loss: 20.59002685546875\n",
      "[step: 462] loss: 20.56277847290039\n",
      "[step: 463] loss: 20.576637268066406\n",
      "[step: 464] loss: 20.56304359436035\n",
      "[step: 465] loss: 20.56600570678711\n",
      "[step: 466] loss: 20.56345558166504\n",
      "[step: 467] loss: 20.55702018737793\n",
      "[step: 468] loss: 20.5612850189209\n",
      "[step: 469] loss: 20.550569534301758\n",
      "[step: 470] loss: 20.558395385742188\n",
      "[step: 471] loss: 20.546649932861328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 472] loss: 20.55414581298828\n",
      "[step: 473] loss: 20.543060302734375\n",
      "[step: 474] loss: 20.549102783203125\n",
      "[step: 475] loss: 20.54036521911621\n",
      "[step: 476] loss: 20.544387817382812\n",
      "[step: 477] loss: 20.537883758544922\n",
      "[step: 478] loss: 20.53957176208496\n",
      "[step: 479] loss: 20.535215377807617\n",
      "[step: 480] loss: 20.535358428955078\n",
      "[step: 481] loss: 20.532800674438477\n",
      "[step: 482] loss: 20.531646728515625\n",
      "[step: 483] loss: 20.530065536499023\n",
      "[step: 484] loss: 20.528051376342773\n",
      "[step: 485] loss: 20.52724266052246\n",
      "[step: 486] loss: 20.52484130859375\n",
      "[step: 487] loss: 20.52434539794922\n",
      "[step: 488] loss: 20.52163314819336\n",
      "[step: 489] loss: 20.521230697631836\n",
      "[step: 490] loss: 20.518569946289062\n",
      "[step: 491] loss: 20.518217086791992\n",
      "[step: 492] loss: 20.515649795532227\n",
      "[step: 493] loss: 20.515159606933594\n",
      "[step: 494] loss: 20.51275062561035\n",
      "[step: 495] loss: 20.512197494506836\n",
      "[step: 496] loss: 20.50998306274414\n",
      "[step: 497] loss: 20.509300231933594\n",
      "[step: 498] loss: 20.5072021484375\n",
      "[step: 499] loss: 20.506404876708984\n",
      "[step: 500] loss: 20.50444793701172\n",
      "[step: 501] loss: 20.503555297851562\n",
      "[step: 502] loss: 20.50170135498047\n",
      "[step: 503] loss: 20.500707626342773\n",
      "[step: 504] loss: 20.498939514160156\n",
      "[step: 505] loss: 20.497894287109375\n",
      "[step: 506] loss: 20.4962215423584\n",
      "[step: 507] loss: 20.4951171875\n",
      "[step: 508] loss: 20.493511199951172\n",
      "[step: 509] loss: 20.49236297607422\n",
      "[step: 510] loss: 20.490833282470703\n",
      "[step: 511] loss: 20.48965072631836\n",
      "[step: 512] loss: 20.4881649017334\n",
      "[step: 513] loss: 20.486942291259766\n",
      "[step: 514] loss: 20.485509872436523\n",
      "[step: 515] loss: 20.48426055908203\n",
      "[step: 516] loss: 20.482868194580078\n",
      "[step: 517] loss: 20.481582641601562\n",
      "[step: 518] loss: 20.4802303314209\n",
      "[step: 519] loss: 20.478931427001953\n",
      "[step: 520] loss: 20.477611541748047\n",
      "[step: 521] loss: 20.47629165649414\n",
      "[step: 522] loss: 20.474998474121094\n",
      "[step: 523] loss: 20.47367286682129\n",
      "[step: 524] loss: 20.472400665283203\n",
      "[step: 525] loss: 20.471073150634766\n",
      "[step: 526] loss: 20.46981430053711\n",
      "[step: 527] loss: 20.468486785888672\n",
      "[step: 528] loss: 20.467235565185547\n",
      "[step: 529] loss: 20.465919494628906\n",
      "[step: 530] loss: 20.464662551879883\n",
      "[step: 531] loss: 20.463363647460938\n",
      "[step: 532] loss: 20.46210289001465\n",
      "[step: 533] loss: 20.4608211517334\n",
      "[step: 534] loss: 20.459552764892578\n",
      "[step: 535] loss: 20.45828628540039\n",
      "[step: 536] loss: 20.457015991210938\n",
      "[step: 537] loss: 20.455766677856445\n",
      "[step: 538] loss: 20.454498291015625\n",
      "[step: 539] loss: 20.4532527923584\n",
      "[step: 540] loss: 20.451990127563477\n",
      "[step: 541] loss: 20.450746536254883\n",
      "[step: 542] loss: 20.449493408203125\n",
      "[step: 543] loss: 20.448251724243164\n",
      "[step: 544] loss: 20.44700813293457\n",
      "[step: 545] loss: 20.44576644897461\n",
      "[step: 546] loss: 20.444534301757812\n",
      "[step: 547] loss: 20.443296432495117\n",
      "[step: 548] loss: 20.44207000732422\n",
      "[step: 549] loss: 20.440837860107422\n",
      "[step: 550] loss: 20.439613342285156\n",
      "[step: 551] loss: 20.438390731811523\n",
      "[step: 552] loss: 20.43716812133789\n",
      "[step: 553] loss: 20.435951232910156\n",
      "[step: 554] loss: 20.434734344482422\n",
      "[step: 555] loss: 20.43352699279785\n",
      "[step: 556] loss: 20.432315826416016\n",
      "[step: 557] loss: 20.431110382080078\n",
      "[step: 558] loss: 20.42990493774414\n",
      "[step: 559] loss: 20.42870330810547\n",
      "[step: 560] loss: 20.427505493164062\n",
      "[step: 561] loss: 20.426307678222656\n",
      "[step: 562] loss: 20.42511749267578\n",
      "[step: 563] loss: 20.42392349243164\n",
      "[step: 564] loss: 20.422739028930664\n",
      "[step: 565] loss: 20.421552658081055\n",
      "[step: 566] loss: 20.42037010192871\n",
      "[step: 567] loss: 20.419189453125\n",
      "[step: 568] loss: 20.418014526367188\n",
      "[step: 569] loss: 20.416839599609375\n",
      "[step: 570] loss: 20.41567039489746\n",
      "[step: 571] loss: 20.414499282836914\n",
      "[step: 572] loss: 20.413330078125\n",
      "[step: 573] loss: 20.412168502807617\n",
      "[step: 574] loss: 20.411006927490234\n",
      "[step: 575] loss: 20.409849166870117\n",
      "[step: 576] loss: 20.40869140625\n",
      "[step: 577] loss: 20.40753936767578\n",
      "[step: 578] loss: 20.406387329101562\n",
      "[step: 579] loss: 20.40523910522461\n",
      "[step: 580] loss: 20.404094696044922\n",
      "[step: 581] loss: 20.4029483795166\n",
      "[step: 582] loss: 20.401805877685547\n",
      "[step: 583] loss: 20.400667190551758\n",
      "[step: 584] loss: 20.3995304107666\n",
      "[step: 585] loss: 20.398395538330078\n",
      "[step: 586] loss: 20.39726448059082\n",
      "[step: 587] loss: 20.396135330200195\n",
      "[step: 588] loss: 20.39500617980957\n",
      "[step: 589] loss: 20.393878936767578\n",
      "[step: 590] loss: 20.39275550842285\n",
      "[step: 591] loss: 20.391633987426758\n",
      "[step: 592] loss: 20.390514373779297\n",
      "[step: 593] loss: 20.3893985748291\n",
      "[step: 594] loss: 20.388280868530273\n",
      "[step: 595] loss: 20.387165069580078\n",
      "[step: 596] loss: 20.38605308532715\n",
      "[step: 597] loss: 20.384944915771484\n",
      "[step: 598] loss: 20.383834838867188\n",
      "[step: 599] loss: 20.382728576660156\n",
      "[step: 600] loss: 20.381620407104492\n",
      "[step: 601] loss: 20.38051414489746\n",
      "[step: 602] loss: 20.379411697387695\n",
      "[step: 603] loss: 20.378307342529297\n",
      "[step: 604] loss: 20.377208709716797\n",
      "[step: 605] loss: 20.376108169555664\n",
      "[step: 606] loss: 20.37500762939453\n",
      "[step: 607] loss: 20.373910903930664\n",
      "[step: 608] loss: 20.372812271118164\n",
      "[step: 609] loss: 20.371715545654297\n",
      "[step: 610] loss: 20.370620727539062\n",
      "[step: 611] loss: 20.369525909423828\n",
      "[step: 612] loss: 20.368431091308594\n",
      "[step: 613] loss: 20.36733627319336\n",
      "[step: 614] loss: 20.366241455078125\n",
      "[step: 615] loss: 20.365150451660156\n",
      "[step: 616] loss: 20.364055633544922\n",
      "[step: 617] loss: 20.362960815429688\n",
      "[step: 618] loss: 20.361867904663086\n",
      "[step: 619] loss: 20.360774993896484\n",
      "[step: 620] loss: 20.359683990478516\n",
      "[step: 621] loss: 20.35858726501465\n",
      "[step: 622] loss: 20.357494354248047\n",
      "[step: 623] loss: 20.356399536132812\n",
      "[step: 624] loss: 20.355300903320312\n",
      "[step: 625] loss: 20.354206085205078\n",
      "[step: 626] loss: 20.353107452392578\n",
      "[step: 627] loss: 20.352012634277344\n",
      "[step: 628] loss: 20.35091209411621\n",
      "[step: 629] loss: 20.34981346130371\n",
      "[step: 630] loss: 20.348712921142578\n",
      "[step: 631] loss: 20.347610473632812\n",
      "[step: 632] loss: 20.346506118774414\n",
      "[step: 633] loss: 20.34539794921875\n",
      "[step: 634] loss: 20.34429168701172\n",
      "[step: 635] loss: 20.343183517456055\n",
      "[step: 636] loss: 20.342071533203125\n",
      "[step: 637] loss: 20.340959548950195\n",
      "[step: 638] loss: 20.339845657348633\n",
      "[step: 639] loss: 20.338727951049805\n",
      "[step: 640] loss: 20.337608337402344\n",
      "[step: 641] loss: 20.33648681640625\n",
      "[step: 642] loss: 20.335363388061523\n",
      "[step: 643] loss: 20.33423614501953\n",
      "[step: 644] loss: 20.333106994628906\n",
      "[step: 645] loss: 20.331974029541016\n",
      "[step: 646] loss: 20.33083724975586\n",
      "[step: 647] loss: 20.32969856262207\n",
      "[step: 648] loss: 20.32855796813965\n",
      "[step: 649] loss: 20.32741355895996\n",
      "[step: 650] loss: 20.326263427734375\n",
      "[step: 651] loss: 20.32511329650879\n",
      "[step: 652] loss: 20.323957443237305\n",
      "[step: 653] loss: 20.322799682617188\n",
      "[step: 654] loss: 20.321636199951172\n",
      "[step: 655] loss: 20.32046890258789\n",
      "[step: 656] loss: 20.31929588317871\n",
      "[step: 657] loss: 20.31812286376953\n",
      "[step: 658] loss: 20.316940307617188\n",
      "[step: 659] loss: 20.315757751464844\n",
      "[step: 660] loss: 20.3145694732666\n",
      "[step: 661] loss: 20.313377380371094\n",
      "[step: 662] loss: 20.312179565429688\n",
      "[step: 663] loss: 20.310977935791016\n",
      "[step: 664] loss: 20.309770584106445\n",
      "[step: 665] loss: 20.30855941772461\n",
      "[step: 666] loss: 20.307340621948242\n",
      "[step: 667] loss: 20.306119918823242\n",
      "[step: 668] loss: 20.304893493652344\n",
      "[step: 669] loss: 20.303659439086914\n",
      "[step: 670] loss: 20.30242347717285\n",
      "[step: 671] loss: 20.301179885864258\n",
      "[step: 672] loss: 20.299930572509766\n",
      "[step: 673] loss: 20.298683166503906\n",
      "[step: 674] loss: 20.297441482543945\n",
      "[step: 675] loss: 20.2962589263916\n",
      "[step: 676] loss: 20.295312881469727\n",
      "[step: 677] loss: 20.295425415039062\n",
      "[step: 678] loss: 20.300189971923828\n",
      "[step: 679] loss: 20.326160430908203\n",
      "[step: 680] loss: 20.45128059387207\n",
      "[step: 681] loss: 21.038803100585938\n",
      "[step: 682] loss: 23.614131927490234\n",
      "[step: 683] loss: 30.94903564453125\n",
      "[step: 684] loss: 33.41282272338867\n",
      "[step: 685] loss: 22.77960968017578\n",
      "[step: 686] loss: 28.434179306030273\n",
      "[step: 687] loss: 21.283220291137695\n",
      "[step: 688] loss: 25.652103424072266\n",
      "[step: 689] loss: 21.18195152282715\n",
      "[step: 690] loss: 24.674827575683594\n",
      "[step: 691] loss: 21.42889976501465\n",
      "[step: 692] loss: 22.261018753051758\n",
      "[step: 693] loss: 22.926191329956055\n",
      "[step: 694] loss: 21.209489822387695\n",
      "[step: 695] loss: 22.495166778564453\n",
      "[step: 696] loss: 21.761743545532227\n",
      "[step: 697] loss: 20.76348304748535\n",
      "[step: 698] loss: 21.94712257385254\n",
      "[step: 699] loss: 20.868438720703125\n",
      "[step: 700] loss: 21.358654022216797\n",
      "[step: 701] loss: 21.22418975830078\n",
      "[step: 702] loss: 20.53544044494629\n",
      "[step: 703] loss: 21.22006607055664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 704] loss: 20.869213104248047\n",
      "[step: 705] loss: 20.802574157714844\n",
      "[step: 706] loss: 21.10408592224121\n",
      "[step: 707] loss: 20.60018539428711\n",
      "[step: 708] loss: 20.724517822265625\n",
      "[step: 709] loss: 20.895814895629883\n",
      "[step: 710] loss: 20.606420516967773\n",
      "[step: 711] loss: 20.823612213134766\n",
      "[step: 712] loss: 20.60842514038086\n",
      "[step: 713] loss: 20.541584014892578\n",
      "[step: 714] loss: 20.728199005126953\n",
      "[step: 715] loss: 20.594345092773438\n",
      "[step: 716] loss: 20.587249755859375\n",
      "[step: 717] loss: 20.643110275268555\n",
      "[step: 718] loss: 20.49539566040039\n",
      "[step: 719] loss: 20.552566528320312\n",
      "[step: 720] loss: 20.60149574279785\n",
      "[step: 721] loss: 20.513280868530273\n",
      "[step: 722] loss: 20.562694549560547\n",
      "[step: 723] loss: 20.5189266204834\n",
      "[step: 724] loss: 20.478347778320312\n",
      "[step: 725] loss: 20.538965225219727\n",
      "[step: 726] loss: 20.509292602539062\n",
      "[step: 727] loss: 20.488000869750977\n",
      "[step: 728] loss: 20.507427215576172\n",
      "[step: 729] loss: 20.465171813964844\n",
      "[step: 730] loss: 20.472265243530273\n",
      "[step: 731] loss: 20.494266510009766\n",
      "[step: 732] loss: 20.46314811706543\n",
      "[step: 733] loss: 20.469770431518555\n",
      "[step: 734] loss: 20.46244239807129\n",
      "[step: 735] loss: 20.44426727294922\n",
      "[step: 736] loss: 20.461780548095703\n",
      "[step: 737] loss: 20.45410919189453\n",
      "[step: 738] loss: 20.441112518310547\n",
      "[step: 739] loss: 20.446285247802734\n",
      "[step: 740] loss: 20.433929443359375\n",
      "[step: 741] loss: 20.433557510375977\n",
      "[step: 742] loss: 20.439733505249023\n",
      "[step: 743] loss: 20.427560806274414\n",
      "[step: 744] loss: 20.426624298095703\n",
      "[step: 745] loss: 20.424785614013672\n",
      "[step: 746] loss: 20.417842864990234\n",
      "[step: 747] loss: 20.421463012695312\n",
      "[step: 748] loss: 20.417367935180664\n",
      "[step: 749] loss: 20.41083335876465\n",
      "[step: 750] loss: 20.41140365600586\n",
      "[step: 751] loss: 20.406871795654297\n",
      "[step: 752] loss: 20.405357360839844\n",
      "[step: 753] loss: 20.40545082092285\n",
      "[step: 754] loss: 20.399831771850586\n",
      "[step: 755] loss: 20.398387908935547\n",
      "[step: 756] loss: 20.396982192993164\n",
      "[step: 757] loss: 20.39370346069336\n",
      "[step: 758] loss: 20.393362045288086\n",
      "[step: 759] loss: 20.390335083007812\n",
      "[step: 760] loss: 20.387155532836914\n",
      "[step: 761] loss: 20.386394500732422\n",
      "[step: 762] loss: 20.38373374938965\n",
      "[step: 763] loss: 20.38201141357422\n",
      "[step: 764] loss: 20.380393981933594\n",
      "[step: 765] loss: 20.37734603881836\n",
      "[step: 766] loss: 20.376007080078125\n",
      "[step: 767] loss: 20.37430763244629\n",
      "[step: 768] loss: 20.372058868408203\n",
      "[step: 769] loss: 20.37061882019043\n",
      "[step: 770] loss: 20.36829376220703\n",
      "[step: 771] loss: 20.36639404296875\n",
      "[step: 772] loss: 20.365018844604492\n",
      "[step: 773] loss: 20.362884521484375\n",
      "[step: 774] loss: 20.361186981201172\n",
      "[step: 775] loss: 20.35936737060547\n",
      "[step: 776] loss: 20.357370376586914\n",
      "[step: 777] loss: 20.35592269897461\n",
      "[step: 778] loss: 20.354084014892578\n",
      "[step: 779] loss: 20.352243423461914\n",
      "[step: 780] loss: 20.350616455078125\n",
      "[step: 781] loss: 20.3487548828125\n",
      "[step: 782] loss: 20.34716033935547\n",
      "[step: 783] loss: 20.345502853393555\n",
      "[step: 784] loss: 20.343669891357422\n",
      "[step: 785] loss: 20.34206199645996\n",
      "[step: 786] loss: 20.340354919433594\n",
      "[step: 787] loss: 20.338695526123047\n",
      "[step: 788] loss: 20.337108612060547\n",
      "[step: 789] loss: 20.33536720275879\n",
      "[step: 790] loss: 20.333749771118164\n",
      "[step: 791] loss: 20.332138061523438\n",
      "[step: 792] loss: 20.330482482910156\n",
      "[step: 793] loss: 20.328903198242188\n",
      "[step: 794] loss: 20.327247619628906\n",
      "[step: 795] loss: 20.325634002685547\n",
      "[step: 796] loss: 20.324068069458008\n",
      "[step: 797] loss: 20.322446823120117\n",
      "[step: 798] loss: 20.320877075195312\n",
      "[step: 799] loss: 20.319286346435547\n",
      "[step: 800] loss: 20.317699432373047\n",
      "[step: 801] loss: 20.3161563873291\n",
      "[step: 802] loss: 20.31458282470703\n",
      "[step: 803] loss: 20.313026428222656\n",
      "[step: 804] loss: 20.311479568481445\n",
      "[step: 805] loss: 20.309925079345703\n",
      "[step: 806] loss: 20.308401107788086\n",
      "[step: 807] loss: 20.30686378479004\n",
      "[step: 808] loss: 20.305334091186523\n",
      "[step: 809] loss: 20.30381965637207\n",
      "[step: 810] loss: 20.30230140686035\n",
      "[step: 811] loss: 20.300800323486328\n",
      "[step: 812] loss: 20.299297332763672\n",
      "[step: 813] loss: 20.29779624938965\n",
      "[step: 814] loss: 20.296310424804688\n",
      "[step: 815] loss: 20.29482650756836\n",
      "[step: 816] loss: 20.29334831237793\n",
      "[step: 817] loss: 20.291879653930664\n",
      "[step: 818] loss: 20.290409088134766\n",
      "[step: 819] loss: 20.288955688476562\n",
      "[step: 820] loss: 20.287498474121094\n",
      "[step: 821] loss: 20.286052703857422\n",
      "[step: 822] loss: 20.284610748291016\n",
      "[step: 823] loss: 20.283174514770508\n",
      "[step: 824] loss: 20.281749725341797\n",
      "[step: 825] loss: 20.280330657958984\n",
      "[step: 826] loss: 20.278911590576172\n",
      "[step: 827] loss: 20.27750587463379\n",
      "[step: 828] loss: 20.27610206604004\n",
      "[step: 829] loss: 20.274707794189453\n",
      "[step: 830] loss: 20.273319244384766\n",
      "[step: 831] loss: 20.27193832397461\n",
      "[step: 832] loss: 20.270565032958984\n",
      "[step: 833] loss: 20.269195556640625\n",
      "[step: 834] loss: 20.267833709716797\n",
      "[step: 835] loss: 20.266481399536133\n",
      "[step: 836] loss: 20.265134811401367\n",
      "[step: 837] loss: 20.2637939453125\n",
      "[step: 838] loss: 20.26246452331543\n",
      "[step: 839] loss: 20.261137008666992\n",
      "[step: 840] loss: 20.25982093811035\n",
      "[step: 841] loss: 20.25851058959961\n",
      "[step: 842] loss: 20.2572078704834\n",
      "[step: 843] loss: 20.25591278076172\n",
      "[step: 844] loss: 20.254627227783203\n",
      "[step: 845] loss: 20.253347396850586\n",
      "[step: 846] loss: 20.2520751953125\n",
      "[step: 847] loss: 20.250812530517578\n",
      "[step: 848] loss: 20.249553680419922\n",
      "[step: 849] loss: 20.248306274414062\n",
      "[step: 850] loss: 20.247066497802734\n",
      "[step: 851] loss: 20.245834350585938\n",
      "[step: 852] loss: 20.244611740112305\n",
      "[step: 853] loss: 20.243396759033203\n",
      "[step: 854] loss: 20.242189407348633\n",
      "[step: 855] loss: 20.240991592407227\n",
      "[step: 856] loss: 20.23980140686035\n",
      "[step: 857] loss: 20.23862075805664\n",
      "[step: 858] loss: 20.237445831298828\n",
      "[step: 859] loss: 20.236282348632812\n",
      "[step: 860] loss: 20.235126495361328\n",
      "[step: 861] loss: 20.233978271484375\n",
      "[step: 862] loss: 20.232837677001953\n",
      "[step: 863] loss: 20.231706619262695\n",
      "[step: 864] loss: 20.230587005615234\n",
      "[step: 865] loss: 20.229475021362305\n",
      "[step: 866] loss: 20.228370666503906\n",
      "[step: 867] loss: 20.227275848388672\n",
      "[step: 868] loss: 20.226192474365234\n",
      "[step: 869] loss: 20.225112915039062\n",
      "[step: 870] loss: 20.224044799804688\n",
      "[step: 871] loss: 20.222984313964844\n",
      "[step: 872] loss: 20.221935272216797\n",
      "[step: 873] loss: 20.22089385986328\n",
      "[step: 874] loss: 20.219858169555664\n",
      "[step: 875] loss: 20.218833923339844\n",
      "[step: 876] loss: 20.217819213867188\n",
      "[step: 877] loss: 20.216814041137695\n",
      "[step: 878] loss: 20.21581268310547\n",
      "[step: 879] loss: 20.214826583862305\n",
      "[step: 880] loss: 20.213844299316406\n",
      "[step: 881] loss: 20.212871551513672\n",
      "[step: 882] loss: 20.211910247802734\n",
      "[step: 883] loss: 20.210954666137695\n",
      "[step: 884] loss: 20.21000862121582\n",
      "[step: 885] loss: 20.20907211303711\n",
      "[step: 886] loss: 20.208141326904297\n",
      "[step: 887] loss: 20.20722007751465\n",
      "[step: 888] loss: 20.20630645751953\n",
      "[step: 889] loss: 20.205402374267578\n",
      "[step: 890] loss: 20.204505920410156\n",
      "[step: 891] loss: 20.203615188598633\n",
      "[step: 892] loss: 20.202735900878906\n",
      "[step: 893] loss: 20.201860427856445\n",
      "[step: 894] loss: 20.200992584228516\n",
      "[step: 895] loss: 20.200138092041016\n",
      "[step: 896] loss: 20.19928741455078\n",
      "[step: 897] loss: 20.198444366455078\n",
      "[step: 898] loss: 20.197608947753906\n",
      "[step: 899] loss: 20.196779251098633\n",
      "[step: 900] loss: 20.19595718383789\n",
      "[step: 901] loss: 20.19514274597168\n",
      "[step: 902] loss: 20.1943359375\n",
      "[step: 903] loss: 20.193531036376953\n",
      "[step: 904] loss: 20.192737579345703\n",
      "[step: 905] loss: 20.19194984436035\n",
      "[step: 906] loss: 20.19116973876953\n",
      "[step: 907] loss: 20.190391540527344\n",
      "[step: 908] loss: 20.189620971679688\n",
      "[step: 909] loss: 20.188858032226562\n",
      "[step: 910] loss: 20.188098907470703\n",
      "[step: 911] loss: 20.187347412109375\n",
      "[step: 912] loss: 20.186599731445312\n",
      "[step: 913] loss: 20.185855865478516\n",
      "[step: 914] loss: 20.18511962890625\n",
      "[step: 915] loss: 20.18438720703125\n",
      "[step: 916] loss: 20.18366050720215\n",
      "[step: 917] loss: 20.182937622070312\n",
      "[step: 918] loss: 20.182222366333008\n",
      "[step: 919] loss: 20.18151092529297\n",
      "[step: 920] loss: 20.180801391601562\n",
      "[step: 921] loss: 20.180097579956055\n",
      "[step: 922] loss: 20.179397583007812\n",
      "[step: 923] loss: 20.178701400756836\n",
      "[step: 924] loss: 20.178007125854492\n",
      "[step: 925] loss: 20.17732048034668\n",
      "[step: 926] loss: 20.1766357421875\n",
      "[step: 927] loss: 20.175954818725586\n",
      "[step: 928] loss: 20.175273895263672\n",
      "[step: 929] loss: 20.174598693847656\n",
      "[step: 930] loss: 20.173927307128906\n",
      "[step: 931] loss: 20.173259735107422\n",
      "[step: 932] loss: 20.17259407043457\n",
      "[step: 933] loss: 20.171932220458984\n",
      "[step: 934] loss: 20.1712703704834\n",
      "[step: 935] loss: 20.170612335205078\n",
      "[step: 936] loss: 20.16995620727539\n",
      "[step: 937] loss: 20.16930389404297\n",
      "[step: 938] loss: 20.168651580810547\n",
      "[step: 939] loss: 20.16800308227539\n",
      "[step: 940] loss: 20.167354583740234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 941] loss: 20.166711807250977\n",
      "[step: 942] loss: 20.166065216064453\n",
      "[step: 943] loss: 20.165424346923828\n",
      "[step: 944] loss: 20.164783477783203\n",
      "[step: 945] loss: 20.164146423339844\n",
      "[step: 946] loss: 20.163509368896484\n",
      "[step: 947] loss: 20.162872314453125\n",
      "[step: 948] loss: 20.16223907470703\n",
      "[step: 949] loss: 20.161605834960938\n",
      "[step: 950] loss: 20.160972595214844\n",
      "[step: 951] loss: 20.160343170166016\n",
      "[step: 952] loss: 20.159709930419922\n",
      "[step: 953] loss: 20.15907859802246\n",
      "[step: 954] loss: 20.158451080322266\n",
      "[step: 955] loss: 20.157821655273438\n",
      "[step: 956] loss: 20.157194137573242\n",
      "[step: 957] loss: 20.156566619873047\n",
      "[step: 958] loss: 20.155941009521484\n",
      "[step: 959] loss: 20.15531349182129\n",
      "[step: 960] loss: 20.15468406677246\n",
      "[step: 961] loss: 20.15406036376953\n",
      "[step: 962] loss: 20.15343475341797\n",
      "[step: 963] loss: 20.15280532836914\n",
      "[step: 964] loss: 20.152183532714844\n",
      "[step: 965] loss: 20.15155601501465\n",
      "[step: 966] loss: 20.150930404663086\n",
      "[step: 967] loss: 20.15030288696289\n",
      "[step: 968] loss: 20.149677276611328\n",
      "[step: 969] loss: 20.149051666259766\n",
      "[step: 970] loss: 20.148422241210938\n",
      "[step: 971] loss: 20.14779281616211\n",
      "[step: 972] loss: 20.147167205810547\n",
      "[step: 973] loss: 20.14653968811035\n",
      "[step: 974] loss: 20.14590835571289\n",
      "[step: 975] loss: 20.145278930664062\n",
      "[step: 976] loss: 20.144649505615234\n",
      "[step: 977] loss: 20.14401626586914\n",
      "[step: 978] loss: 20.143383026123047\n",
      "[step: 979] loss: 20.142749786376953\n",
      "[step: 980] loss: 20.14211654663086\n",
      "[step: 981] loss: 20.141481399536133\n",
      "[step: 982] loss: 20.140846252441406\n",
      "[step: 983] loss: 20.140209197998047\n",
      "[step: 984] loss: 20.139570236206055\n",
      "[step: 985] loss: 20.138931274414062\n",
      "[step: 986] loss: 20.138290405273438\n",
      "[step: 987] loss: 20.13764762878418\n",
      "[step: 988] loss: 20.137004852294922\n",
      "[step: 989] loss: 20.13636016845703\n",
      "[step: 990] loss: 20.135713577270508\n",
      "[step: 991] loss: 20.13506507873535\n",
      "[step: 992] loss: 20.134414672851562\n",
      "[step: 993] loss: 20.133766174316406\n",
      "[step: 994] loss: 20.13311195373535\n",
      "[step: 995] loss: 20.132457733154297\n",
      "[step: 996] loss: 20.131799697875977\n",
      "[step: 997] loss: 20.13114356994629\n",
      "[step: 998] loss: 20.130481719970703\n",
      "[step: 999] loss: 20.12982177734375\n",
      "[step: 1000] loss: 20.12915802001953\n",
      "RMSE: 2.333655637435189\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHiIUBEEQBA8RCoIgCILH/wf9W7BEdyMKPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2521d9f7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "def predict(file):\n",
    "    # train Parameters\n",
    "    seq_length = 6\n",
    "    data_dim = 1\n",
    "    hidden_dim = 12 # 내 맘대로 정해도 됨\n",
    "    output_dim = 1\n",
    "    learning_rate = 0.05\n",
    "    iterations = 1000\n",
    "    layer_num=3\n",
    "\n",
    "    # train Data: Open, High, Low, Volume, Close\n",
    "    origin_xy = np.loadtxt('train.csv', delimiter=',', skiprows=1, usecols=range(1,8))\n",
    "    xy = MinMaxScaler(origin_xy)\n",
    "    x = xy[:,:-1]\n",
    "    y = xy[:,-1]\n",
    "    x = x.reshape(-1,seq_length,data_dim)\n",
    "    y = y.reshape(-1,data_dim) \n",
    "\n",
    "    # train/validation split\n",
    "    train_size = int(len(y) * 0.7)\n",
    "    test_size = len(y) - train_size\n",
    "    trainX, validX = np.array(x[0:train_size]), np.array(x[train_size:])\n",
    "    trainY, validY = np.array(y[0:train_size]), np.array(y[train_size:])\n",
    "\n",
    "    #test data\n",
    "    test_x=np.loadtxt(file, delimiter=',', skiprows=1, usecols=range(1,7))\n",
    "    test_x = MinMaxScaler(test_x)\n",
    "    test_x = test_x.reshape(-1,seq_length, data_dim)\n",
    "    \n",
    "    # input place holders\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # build a LSTM network\n",
    "    cells=[]\n",
    "    for _ in range(layer_num):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "        #cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.7)\n",
    "        cells.append(cell)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "    # cost/loss\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # RMSE\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "    sess=tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i+1, step_loss))\n",
    "\n",
    "    # validation step\n",
    "    valid_predict = sess.run(Y_pred, feed_dict={X: validX})\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validY, predictions: valid_predict})\n",
    "    revised_rmse = rmse_val*(origin_xy.max()-origin_xy.min()+1e-7) + origin_xy.min()\n",
    "    print(\"RMSE: {}\".format(revised_rmse))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(validY)\n",
    "    plt.plot(valid_predict)\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Temperature\")\n",
    "    plt.show()\n",
    "    \n",
    "    #test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: test_x})\n",
    "    prediction_list = []\n",
    "    for i in test_predict:\n",
    "        revised_pred = i[0]*(origin_xy.max()-origin_xy.min()+1e-7) + origin_xy.min()\n",
    "        prediction_list.append(revised_pred)\n",
    "    return prediction_list\n",
    "\n",
    "def write_result(predictions):\n",
    "    # You don't need to modify this function.\n",
    "    with open('result.csv', 'w') as f:\n",
    "        f.write('Value\\n')\n",
    "        for l in predictions:\n",
    "            f.write('{}\\n'.format(l))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # You don't need to modify this function.\n",
    "    predictions = predict('test.csv')\n",
    "    write_result(predictions)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # You don't need to modify this part.\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
