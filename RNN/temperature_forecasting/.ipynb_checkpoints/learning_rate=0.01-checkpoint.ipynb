{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to add any functions, import statements, and variables.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1] loss: 562.6109619140625\n",
      "[step: 2] loss: 415.46368408203125\n",
      "[step: 3] loss: 290.0538635253906\n",
      "[step: 4] loss: 186.38279724121094\n",
      "[step: 5] loss: 106.91754150390625\n",
      "[step: 6] loss: 55.373905181884766\n",
      "[step: 7] loss: 35.82456970214844\n",
      "[step: 8] loss: 48.40688705444336\n",
      "[step: 9] loss: 77.23800659179688\n",
      "[step: 10] loss: 95.78650665283203\n",
      "[step: 11] loss: 95.37982177734375\n",
      "[step: 12] loss: 81.9999771118164\n",
      "[step: 13] loss: 64.45439910888672\n",
      "[step: 14] loss: 49.1888313293457\n",
      "[step: 15] loss: 39.2412223815918\n",
      "[step: 16] loss: 34.92992401123047\n",
      "[step: 17] loss: 35.03504943847656\n",
      "[step: 18] loss: 37.81855392456055\n",
      "[step: 19] loss: 41.65181350708008\n",
      "[step: 20] loss: 45.2923583984375\n",
      "[step: 21] loss: 47.94375991821289\n",
      "[step: 22] loss: 49.21292495727539\n",
      "[step: 23] loss: 49.03166198730469\n",
      "[step: 24] loss: 47.57313537597656\n",
      "[step: 25] loss: 45.17351150512695\n",
      "[step: 26] loss: 42.26048278808594\n",
      "[step: 27] loss: 39.28680419921875\n",
      "[step: 28] loss: 36.668006896972656\n",
      "[step: 29] loss: 34.72500228881836\n",
      "[step: 30] loss: 33.6363410949707\n",
      "[step: 31] loss: 33.40901565551758\n",
      "[step: 32] loss: 33.878841400146484\n",
      "[step: 33] loss: 34.74853515625\n",
      "[step: 34] loss: 35.66154479980469\n",
      "[step: 35] loss: 36.294673919677734\n",
      "[step: 36] loss: 36.44063186645508\n",
      "[step: 37] loss: 36.05189895629883\n",
      "[step: 38] loss: 35.23234558105469\n",
      "[step: 39] loss: 34.18474197387695\n",
      "[step: 40] loss: 33.138057708740234\n",
      "[step: 41] loss: 32.28111267089844\n",
      "[step: 42] loss: 31.721227645874023\n",
      "[step: 43] loss: 31.4736270904541\n",
      "[step: 44] loss: 31.476469039916992\n",
      "[step: 45] loss: 31.62127113342285\n",
      "[step: 46] loss: 31.787412643432617\n",
      "[step: 47] loss: 31.87216567993164\n",
      "[step: 48] loss: 31.81101417541504\n",
      "[step: 49] loss: 31.586257934570312\n",
      "[step: 50] loss: 31.224313735961914\n",
      "[step: 51] loss: 30.78373146057129\n",
      "[step: 52] loss: 30.337181091308594\n",
      "[step: 53] loss: 29.951711654663086\n",
      "[step: 54] loss: 29.671342849731445\n",
      "[step: 55] loss: 29.506847381591797\n",
      "[step: 56] loss: 29.435178756713867\n",
      "[step: 57] loss: 29.40933609008789\n",
      "[step: 58] loss: 29.375118255615234\n",
      "[step: 59] loss: 29.289722442626953\n",
      "[step: 60] loss: 29.13477325439453\n",
      "[step: 61] loss: 28.919904708862305\n",
      "[step: 62] loss: 28.67583656311035\n",
      "[step: 63] loss: 28.440784454345703\n",
      "[step: 64] loss: 28.245891571044922\n",
      "[step: 65] loss: 28.105361938476562\n",
      "[step: 66] loss: 28.013988494873047\n",
      "[step: 67] loss: 27.951946258544922\n",
      "[step: 68] loss: 27.89397621154785\n",
      "[step: 69] loss: 27.819204330444336\n",
      "[step: 70] loss: 27.71796989440918\n",
      "[step: 71] loss: 27.59383201599121\n",
      "[step: 72] loss: 27.46051597595215\n",
      "[step: 73] loss: 27.335405349731445\n",
      "[step: 74] loss: 27.232378005981445\n",
      "[step: 75] loss: 27.156522750854492\n",
      "[step: 76] loss: 27.103097915649414\n",
      "[step: 77] loss: 27.060625076293945\n",
      "[step: 78] loss: 27.01650619506836\n",
      "[step: 79] loss: 26.962738037109375\n",
      "[step: 80] loss: 26.898698806762695\n",
      "[step: 81] loss: 26.830507278442383\n",
      "[step: 82] loss: 26.766996383666992\n",
      "[step: 83] loss: 26.715282440185547\n",
      "[step: 84] loss: 26.677494049072266\n",
      "[step: 85] loss: 26.650487899780273\n",
      "[step: 86] loss: 26.62795066833496\n",
      "[step: 87] loss: 26.603879928588867\n",
      "[step: 88] loss: 26.575170516967773\n",
      "[step: 89] loss: 26.542667388916016\n",
      "[step: 90] loss: 26.510046005249023\n",
      "[step: 91] loss: 26.481426239013672\n",
      "[step: 92] loss: 26.459203720092773\n",
      "[step: 93] loss: 26.44290542602539\n",
      "[step: 94] loss: 26.429821014404297\n",
      "[step: 95] loss: 26.416616439819336\n",
      "[step: 96] loss: 26.401124954223633\n",
      "[step: 97] loss: 26.38326644897461\n",
      "[step: 98] loss: 26.364625930786133\n",
      "[step: 99] loss: 26.347339630126953\n",
      "[step: 100] loss: 26.332664489746094\n",
      "[step: 101] loss: 26.320466995239258\n",
      "[step: 102] loss: 26.309450149536133\n",
      "[step: 103] loss: 26.297992706298828\n",
      "[step: 104] loss: 26.285120010375977\n",
      "[step: 105] loss: 26.270856857299805\n",
      "[step: 106] loss: 26.256071090698242\n",
      "[step: 107] loss: 26.241750717163086\n",
      "[step: 108] loss: 26.228504180908203\n",
      "[step: 109] loss: 26.21615982055664\n",
      "[step: 110] loss: 26.20404052734375\n",
      "[step: 111] loss: 26.191448211669922\n",
      "[step: 112] loss: 26.178028106689453\n",
      "[step: 113] loss: 26.163982391357422\n",
      "[step: 114] loss: 26.149749755859375\n",
      "[step: 115] loss: 26.135822296142578\n",
      "[step: 116] loss: 26.122385025024414\n",
      "[step: 117] loss: 26.109270095825195\n",
      "[step: 118] loss: 26.09612464904785\n",
      "[step: 119] loss: 26.082674026489258\n",
      "[step: 120] loss: 26.06885528564453\n",
      "[step: 121] loss: 26.05484390258789\n",
      "[step: 122] loss: 26.040895462036133\n",
      "[step: 123] loss: 26.027177810668945\n",
      "[step: 124] loss: 26.013710021972656\n",
      "[step: 125] loss: 26.000356674194336\n",
      "[step: 126] loss: 25.986955642700195\n",
      "[step: 127] loss: 25.97342300415039\n",
      "[step: 128] loss: 25.959775924682617\n",
      "[step: 129] loss: 25.946142196655273\n",
      "[step: 130] loss: 25.932615280151367\n",
      "[step: 131] loss: 25.919240951538086\n",
      "[step: 132] loss: 25.90597915649414\n",
      "[step: 133] loss: 25.89275360107422\n",
      "[step: 134] loss: 25.879491806030273\n",
      "[step: 135] loss: 25.866180419921875\n",
      "[step: 136] loss: 25.85287857055664\n",
      "[step: 137] loss: 25.83962059020996\n",
      "[step: 138] loss: 25.82645606994629\n",
      "[step: 139] loss: 25.8133602142334\n",
      "[step: 140] loss: 25.800296783447266\n",
      "[step: 141] loss: 25.787246704101562\n",
      "[step: 142] loss: 25.774169921875\n",
      "[step: 143] loss: 25.761117935180664\n",
      "[step: 144] loss: 25.748065948486328\n",
      "[step: 145] loss: 25.735065460205078\n",
      "[step: 146] loss: 25.72211456298828\n",
      "[step: 147] loss: 25.709197998046875\n",
      "[step: 148] loss: 25.696285247802734\n",
      "[step: 149] loss: 25.683366775512695\n",
      "[step: 150] loss: 25.670467376708984\n",
      "[step: 151] loss: 25.65757179260254\n",
      "[step: 152] loss: 25.644710540771484\n",
      "[step: 153] loss: 25.631874084472656\n",
      "[step: 154] loss: 25.619070053100586\n",
      "[step: 155] loss: 25.60626983642578\n",
      "[step: 156] loss: 25.593488693237305\n",
      "[step: 157] loss: 25.580707550048828\n",
      "[step: 158] loss: 25.56795883178711\n",
      "[step: 159] loss: 25.55521583557129\n",
      "[step: 160] loss: 25.542499542236328\n",
      "[step: 161] loss: 25.529809951782227\n",
      "[step: 162] loss: 25.517131805419922\n",
      "[step: 163] loss: 25.504484176635742\n",
      "[step: 164] loss: 25.491838455200195\n",
      "[step: 165] loss: 25.479209899902344\n",
      "[step: 166] loss: 25.466604232788086\n",
      "[step: 167] loss: 25.454021453857422\n",
      "[step: 168] loss: 25.44145965576172\n",
      "[step: 169] loss: 25.428916931152344\n",
      "[step: 170] loss: 25.416391372680664\n",
      "[step: 171] loss: 25.40388298034668\n",
      "[step: 172] loss: 25.391399383544922\n",
      "[step: 173] loss: 25.378942489624023\n",
      "[step: 174] loss: 25.36648941040039\n",
      "[step: 175] loss: 25.35407257080078\n",
      "[step: 176] loss: 25.34166717529297\n",
      "[step: 177] loss: 25.329296112060547\n",
      "[step: 178] loss: 25.316930770874023\n",
      "[step: 179] loss: 25.30458641052246\n",
      "[step: 180] loss: 25.292261123657227\n",
      "[step: 181] loss: 25.27996253967285\n",
      "[step: 182] loss: 25.267690658569336\n",
      "[step: 183] loss: 25.25543212890625\n",
      "[step: 184] loss: 25.243196487426758\n",
      "[step: 185] loss: 25.230976104736328\n",
      "[step: 186] loss: 25.218786239624023\n",
      "[step: 187] loss: 25.20662498474121\n",
      "[step: 188] loss: 25.19447135925293\n",
      "[step: 189] loss: 25.18235206604004\n",
      "[step: 190] loss: 25.170238494873047\n",
      "[step: 191] loss: 25.15814971923828\n",
      "[step: 192] loss: 25.146087646484375\n",
      "[step: 193] loss: 25.1340389251709\n",
      "[step: 194] loss: 25.12200927734375\n",
      "[step: 195] loss: 25.110013961791992\n",
      "[step: 196] loss: 25.098031997680664\n",
      "[step: 197] loss: 25.086078643798828\n",
      "[step: 198] loss: 25.074148178100586\n",
      "[step: 199] loss: 25.062231063842773\n",
      "[step: 200] loss: 25.05033302307129\n",
      "[step: 201] loss: 25.038461685180664\n",
      "[step: 202] loss: 25.026607513427734\n",
      "[step: 203] loss: 25.014774322509766\n",
      "[step: 204] loss: 25.002965927124023\n",
      "[step: 205] loss: 24.991178512573242\n",
      "[step: 206] loss: 24.979421615600586\n",
      "[step: 207] loss: 24.967683792114258\n",
      "[step: 208] loss: 24.955957412719727\n",
      "[step: 209] loss: 24.944257736206055\n",
      "[step: 210] loss: 24.932579040527344\n",
      "[step: 211] loss: 24.92092514038086\n",
      "[step: 212] loss: 24.909284591674805\n",
      "[step: 213] loss: 24.897672653198242\n",
      "[step: 214] loss: 24.886077880859375\n",
      "[step: 215] loss: 24.874507904052734\n",
      "[step: 216] loss: 24.862960815429688\n",
      "[step: 217] loss: 24.8514404296875\n",
      "[step: 218] loss: 24.839920043945312\n",
      "[step: 219] loss: 24.82844352722168\n",
      "[step: 220] loss: 24.816974639892578\n",
      "[step: 221] loss: 24.805540084838867\n",
      "[step: 222] loss: 24.79410743713379\n",
      "[step: 223] loss: 24.782703399658203\n",
      "[step: 224] loss: 24.771329879760742\n",
      "[step: 225] loss: 24.75996971130371\n",
      "[step: 226] loss: 24.748632431030273\n",
      "[step: 227] loss: 24.73731231689453\n",
      "[step: 228] loss: 24.726030349731445\n",
      "[step: 229] loss: 24.714752197265625\n",
      "[step: 230] loss: 24.703495025634766\n",
      "[step: 231] loss: 24.692258834838867\n",
      "[step: 232] loss: 24.681045532226562\n",
      "[step: 233] loss: 24.669862747192383\n",
      "[step: 234] loss: 24.65869140625\n",
      "[step: 235] loss: 24.647541046142578\n",
      "[step: 236] loss: 24.636415481567383\n",
      "[step: 237] loss: 24.625308990478516\n",
      "[step: 238] loss: 24.614219665527344\n",
      "[step: 239] loss: 24.603151321411133\n",
      "[step: 240] loss: 24.592105865478516\n",
      "[step: 241] loss: 24.581079483032227\n",
      "[step: 242] loss: 24.57007598876953\n",
      "[step: 243] loss: 24.559083938598633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 244] loss: 24.548118591308594\n",
      "[step: 245] loss: 24.53717613220215\n",
      "[step: 246] loss: 24.52625274658203\n",
      "[step: 247] loss: 24.51534080505371\n",
      "[step: 248] loss: 24.504453659057617\n",
      "[step: 249] loss: 24.49359703063965\n",
      "[step: 250] loss: 24.482738494873047\n",
      "[step: 251] loss: 24.471912384033203\n",
      "[step: 252] loss: 24.46111488342285\n",
      "[step: 253] loss: 24.450326919555664\n",
      "[step: 254] loss: 24.439559936523438\n",
      "[step: 255] loss: 24.428804397583008\n",
      "[step: 256] loss: 24.418079376220703\n",
      "[step: 257] loss: 24.407373428344727\n",
      "[step: 258] loss: 24.396682739257812\n",
      "[step: 259] loss: 24.386005401611328\n",
      "[step: 260] loss: 24.37535285949707\n",
      "[step: 261] loss: 24.364721298217773\n",
      "[step: 262] loss: 24.354097366333008\n",
      "[step: 263] loss: 24.343509674072266\n",
      "[step: 264] loss: 24.332923889160156\n",
      "[step: 265] loss: 24.3223819732666\n",
      "[step: 266] loss: 24.31182289123535\n",
      "[step: 267] loss: 24.301294326782227\n",
      "[step: 268] loss: 24.29079818725586\n",
      "[step: 269] loss: 24.280303955078125\n",
      "[step: 270] loss: 24.26984405517578\n",
      "[step: 271] loss: 24.259384155273438\n",
      "[step: 272] loss: 24.24895477294922\n",
      "[step: 273] loss: 24.238540649414062\n",
      "[step: 274] loss: 24.22814178466797\n",
      "[step: 275] loss: 24.217758178710938\n",
      "[step: 276] loss: 24.207393646240234\n",
      "[step: 277] loss: 24.197052001953125\n",
      "[step: 278] loss: 24.186723709106445\n",
      "[step: 279] loss: 24.176410675048828\n",
      "[step: 280] loss: 24.16611099243164\n",
      "[step: 281] loss: 24.155841827392578\n",
      "[step: 282] loss: 24.145578384399414\n",
      "[step: 283] loss: 24.135339736938477\n",
      "[step: 284] loss: 24.125102996826172\n",
      "[step: 285] loss: 24.114892959594727\n",
      "[step: 286] loss: 24.10469627380371\n",
      "[step: 287] loss: 24.094512939453125\n",
      "[step: 288] loss: 24.084360122680664\n",
      "[step: 289] loss: 24.0742130279541\n",
      "[step: 290] loss: 24.064077377319336\n",
      "[step: 291] loss: 24.053970336914062\n",
      "[step: 292] loss: 24.043855667114258\n",
      "[step: 293] loss: 24.033775329589844\n",
      "[step: 294] loss: 24.02370262145996\n",
      "[step: 295] loss: 24.01364517211914\n",
      "[step: 296] loss: 24.00361442565918\n",
      "[step: 297] loss: 23.993587493896484\n",
      "[step: 298] loss: 23.983579635620117\n",
      "[step: 299] loss: 23.973590850830078\n",
      "[step: 300] loss: 23.963598251342773\n",
      "[step: 301] loss: 23.953636169433594\n",
      "[step: 302] loss: 23.943679809570312\n",
      "[step: 303] loss: 23.93374252319336\n",
      "[step: 304] loss: 23.923824310302734\n",
      "[step: 305] loss: 23.913909912109375\n",
      "[step: 306] loss: 23.904016494750977\n",
      "[step: 307] loss: 23.894132614135742\n",
      "[step: 308] loss: 23.884258270263672\n",
      "[step: 309] loss: 23.874404907226562\n",
      "[step: 310] loss: 23.864559173583984\n",
      "[step: 311] loss: 23.85472869873047\n",
      "[step: 312] loss: 23.844919204711914\n",
      "[step: 313] loss: 23.83510398864746\n",
      "[step: 314] loss: 23.825315475463867\n",
      "[step: 315] loss: 23.815540313720703\n",
      "[step: 316] loss: 23.805768966674805\n",
      "[step: 317] loss: 23.796003341674805\n",
      "[step: 318] loss: 23.7862606048584\n",
      "[step: 319] loss: 23.776531219482422\n",
      "[step: 320] loss: 23.766807556152344\n",
      "[step: 321] loss: 23.757102966308594\n",
      "[step: 322] loss: 23.74739646911621\n",
      "[step: 323] loss: 23.737716674804688\n",
      "[step: 324] loss: 23.728038787841797\n",
      "[step: 325] loss: 23.718374252319336\n",
      "[step: 326] loss: 23.708715438842773\n",
      "[step: 327] loss: 23.699068069458008\n",
      "[step: 328] loss: 23.68942642211914\n",
      "[step: 329] loss: 23.679807662963867\n",
      "[step: 330] loss: 23.67018699645996\n",
      "[step: 331] loss: 23.66058921813965\n",
      "[step: 332] loss: 23.650991439819336\n",
      "[step: 333] loss: 23.641407012939453\n",
      "[step: 334] loss: 23.631816864013672\n",
      "[step: 335] loss: 23.622257232666016\n",
      "[step: 336] loss: 23.61269760131836\n",
      "[step: 337] loss: 23.60314178466797\n",
      "[step: 338] loss: 23.59359359741211\n",
      "[step: 339] loss: 23.58406639099121\n",
      "[step: 340] loss: 23.574541091918945\n",
      "[step: 341] loss: 23.56502914428711\n",
      "[step: 342] loss: 23.55550193786621\n",
      "[step: 343] loss: 23.54599952697754\n",
      "[step: 344] loss: 23.536510467529297\n",
      "[step: 345] loss: 23.52701187133789\n",
      "[step: 346] loss: 23.517534255981445\n",
      "[step: 347] loss: 23.5080509185791\n",
      "[step: 348] loss: 23.498577117919922\n",
      "[step: 349] loss: 23.489116668701172\n",
      "[step: 350] loss: 23.47965431213379\n",
      "[step: 351] loss: 23.4702091217041\n",
      "[step: 352] loss: 23.460758209228516\n",
      "[step: 353] loss: 23.451318740844727\n",
      "[step: 354] loss: 23.441883087158203\n",
      "[step: 355] loss: 23.432451248168945\n",
      "[step: 356] loss: 23.42302703857422\n",
      "[step: 357] loss: 23.413602828979492\n",
      "[step: 358] loss: 23.404178619384766\n",
      "[step: 359] loss: 23.394771575927734\n",
      "[step: 360] loss: 23.385366439819336\n",
      "[step: 361] loss: 23.375959396362305\n",
      "[step: 362] loss: 23.366548538208008\n",
      "[step: 363] loss: 23.357158660888672\n",
      "[step: 364] loss: 23.347753524780273\n",
      "[step: 365] loss: 23.338367462158203\n",
      "[step: 366] loss: 23.328981399536133\n",
      "[step: 367] loss: 23.319580078125\n",
      "[step: 368] loss: 23.31019401550293\n",
      "[step: 369] loss: 23.300813674926758\n",
      "[step: 370] loss: 23.291427612304688\n",
      "[step: 371] loss: 23.28205108642578\n",
      "[step: 372] loss: 23.27266502380371\n",
      "[step: 373] loss: 23.26328468322754\n",
      "[step: 374] loss: 23.253910064697266\n",
      "[step: 375] loss: 23.244531631469727\n",
      "[step: 376] loss: 23.235143661499023\n",
      "[step: 377] loss: 23.22577476501465\n",
      "[step: 378] loss: 23.216392517089844\n",
      "[step: 379] loss: 23.207012176513672\n",
      "[step: 380] loss: 23.197628021240234\n",
      "[step: 381] loss: 23.188257217407227\n",
      "[step: 382] loss: 23.178869247436523\n",
      "[step: 383] loss: 23.169477462768555\n",
      "[step: 384] loss: 23.160097122192383\n",
      "[step: 385] loss: 23.150707244873047\n",
      "[step: 386] loss: 23.141324996948242\n",
      "[step: 387] loss: 23.131927490234375\n",
      "[step: 388] loss: 23.12253189086914\n",
      "[step: 389] loss: 23.113134384155273\n",
      "[step: 390] loss: 23.103727340698242\n",
      "[step: 391] loss: 23.094327926635742\n",
      "[step: 392] loss: 23.084911346435547\n",
      "[step: 393] loss: 23.075496673583984\n",
      "[step: 394] loss: 23.06608009338379\n",
      "[step: 395] loss: 23.056663513183594\n",
      "[step: 396] loss: 23.047225952148438\n",
      "[step: 397] loss: 23.03779411315918\n",
      "[step: 398] loss: 23.028356552124023\n",
      "[step: 399] loss: 23.018911361694336\n",
      "[step: 400] loss: 23.00946044921875\n",
      "[step: 401] loss: 23.000011444091797\n",
      "[step: 402] loss: 22.990541458129883\n",
      "[step: 403] loss: 22.981067657470703\n",
      "[step: 404] loss: 22.971590042114258\n",
      "[step: 405] loss: 22.962108612060547\n",
      "[step: 406] loss: 22.952611923217773\n",
      "[step: 407] loss: 22.943113327026367\n",
      "[step: 408] loss: 22.933589935302734\n",
      "[step: 409] loss: 22.924074172973633\n",
      "[step: 410] loss: 22.914548873901367\n",
      "[step: 411] loss: 22.90500831604004\n",
      "[step: 412] loss: 22.895463943481445\n",
      "[step: 413] loss: 22.88591766357422\n",
      "[step: 414] loss: 22.87635040283203\n",
      "[step: 415] loss: 22.866777420043945\n",
      "[step: 416] loss: 22.8571834564209\n",
      "[step: 417] loss: 22.84758949279785\n",
      "[step: 418] loss: 22.83797836303711\n",
      "[step: 419] loss: 22.828351974487305\n",
      "[step: 420] loss: 22.8187255859375\n",
      "[step: 421] loss: 22.809080123901367\n",
      "[step: 422] loss: 22.799423217773438\n",
      "[step: 423] loss: 22.789752960205078\n",
      "[step: 424] loss: 22.780071258544922\n",
      "[step: 425] loss: 22.770368576049805\n",
      "[step: 426] loss: 22.760656356811523\n",
      "[step: 427] loss: 22.750940322875977\n",
      "[step: 428] loss: 22.741207122802734\n",
      "[step: 429] loss: 22.731454849243164\n",
      "[step: 430] loss: 22.7216796875\n",
      "[step: 431] loss: 22.7119140625\n",
      "[step: 432] loss: 22.702112197875977\n",
      "[step: 433] loss: 22.692296981811523\n",
      "[step: 434] loss: 22.682466506958008\n",
      "[step: 435] loss: 22.672636032104492\n",
      "[step: 436] loss: 22.662761688232422\n",
      "[step: 437] loss: 22.65288734436035\n",
      "[step: 438] loss: 22.642995834350586\n",
      "[step: 439] loss: 22.63309097290039\n",
      "[step: 440] loss: 22.62315559387207\n",
      "[step: 441] loss: 22.61320686340332\n",
      "[step: 442] loss: 22.603248596191406\n",
      "[step: 443] loss: 22.593259811401367\n",
      "[step: 444] loss: 22.583253860473633\n",
      "[step: 445] loss: 22.573238372802734\n",
      "[step: 446] loss: 22.56320571899414\n",
      "[step: 447] loss: 22.553146362304688\n",
      "[step: 448] loss: 22.54306983947754\n",
      "[step: 449] loss: 22.53297996520996\n",
      "[step: 450] loss: 22.522857666015625\n",
      "[step: 451] loss: 22.512714385986328\n",
      "[step: 452] loss: 22.5025691986084\n",
      "[step: 453] loss: 22.492387771606445\n",
      "[step: 454] loss: 22.48219108581543\n",
      "[step: 455] loss: 22.471969604492188\n",
      "[step: 456] loss: 22.46173095703125\n",
      "[step: 457] loss: 22.451465606689453\n",
      "[step: 458] loss: 22.441179275512695\n",
      "[step: 459] loss: 22.430875778198242\n",
      "[step: 460] loss: 22.420555114746094\n",
      "[step: 461] loss: 22.410198211669922\n",
      "[step: 462] loss: 22.399822235107422\n",
      "[step: 463] loss: 22.389442443847656\n",
      "[step: 464] loss: 22.3790283203125\n",
      "[step: 465] loss: 22.368581771850586\n",
      "[step: 466] loss: 22.35813331604004\n",
      "[step: 467] loss: 22.34764862060547\n",
      "[step: 468] loss: 22.33713722229004\n",
      "[step: 469] loss: 22.326618194580078\n",
      "[step: 470] loss: 22.31606674194336\n",
      "[step: 471] loss: 22.30548858642578\n",
      "[step: 472] loss: 22.294898986816406\n",
      "[step: 473] loss: 22.284269332885742\n",
      "[step: 474] loss: 22.273635864257812\n",
      "[step: 475] loss: 22.262971878051758\n",
      "[step: 476] loss: 22.252288818359375\n",
      "[step: 477] loss: 22.241580963134766\n",
      "[step: 478] loss: 22.230850219726562\n",
      "[step: 479] loss: 22.220088958740234\n",
      "[step: 480] loss: 22.209327697753906\n",
      "[step: 481] loss: 22.19852638244629\n",
      "[step: 482] loss: 22.18770408630371\n",
      "[step: 483] loss: 22.1768741607666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 484] loss: 22.166017532348633\n",
      "[step: 485] loss: 22.155134201049805\n",
      "[step: 486] loss: 22.14423179626465\n",
      "[step: 487] loss: 22.133312225341797\n",
      "[step: 488] loss: 22.12236785888672\n",
      "[step: 489] loss: 22.111404418945312\n",
      "[step: 490] loss: 22.100431442260742\n",
      "[step: 491] loss: 22.08943748474121\n",
      "[step: 492] loss: 22.078413009643555\n",
      "[step: 493] loss: 22.06739044189453\n",
      "[step: 494] loss: 22.05633544921875\n",
      "[step: 495] loss: 22.045272827148438\n",
      "[step: 496] loss: 22.034194946289062\n",
      "[step: 497] loss: 22.023096084594727\n",
      "[step: 498] loss: 22.011980056762695\n",
      "[step: 499] loss: 22.000856399536133\n",
      "[step: 500] loss: 21.989721298217773\n",
      "[step: 501] loss: 21.97857093811035\n",
      "[step: 502] loss: 21.9674072265625\n",
      "[step: 503] loss: 21.95623779296875\n",
      "[step: 504] loss: 21.945068359375\n",
      "[step: 505] loss: 21.933881759643555\n",
      "[step: 506] loss: 21.922687530517578\n",
      "[step: 507] loss: 21.91148567199707\n",
      "[step: 508] loss: 21.90028953552246\n",
      "[step: 509] loss: 21.889083862304688\n",
      "[step: 510] loss: 21.877866744995117\n",
      "[step: 511] loss: 21.86665153503418\n",
      "[step: 512] loss: 21.855453491210938\n",
      "[step: 513] loss: 21.84424591064453\n",
      "[step: 514] loss: 21.833030700683594\n",
      "[step: 515] loss: 21.821836471557617\n",
      "[step: 516] loss: 21.810653686523438\n",
      "[step: 517] loss: 21.79947280883789\n",
      "[step: 518] loss: 21.788301467895508\n",
      "[step: 519] loss: 21.777137756347656\n",
      "[step: 520] loss: 21.76598358154297\n",
      "[step: 521] loss: 21.754850387573242\n",
      "[step: 522] loss: 21.74374771118164\n",
      "[step: 523] loss: 21.732648849487305\n",
      "[step: 524] loss: 21.721574783325195\n",
      "[step: 525] loss: 21.710535049438477\n",
      "[step: 526] loss: 21.69951629638672\n",
      "[step: 527] loss: 21.68852424621582\n",
      "[step: 528] loss: 21.67756462097168\n",
      "[step: 529] loss: 21.66663360595703\n",
      "[step: 530] loss: 21.655746459960938\n",
      "[step: 531] loss: 21.644899368286133\n",
      "[step: 532] loss: 21.63407325744629\n",
      "[step: 533] loss: 21.623310089111328\n",
      "[step: 534] loss: 21.61258888244629\n",
      "[step: 535] loss: 21.601911544799805\n",
      "[step: 536] loss: 21.591291427612305\n",
      "[step: 537] loss: 21.580724716186523\n",
      "[step: 538] loss: 21.570207595825195\n",
      "[step: 539] loss: 21.559768676757812\n",
      "[step: 540] loss: 21.549379348754883\n",
      "[step: 541] loss: 21.5390567779541\n",
      "[step: 542] loss: 21.52880859375\n",
      "[step: 543] loss: 21.518632888793945\n",
      "[step: 544] loss: 21.508527755737305\n",
      "[step: 545] loss: 21.498506546020508\n",
      "[step: 546] loss: 21.48855972290039\n",
      "[step: 547] loss: 21.478702545166016\n",
      "[step: 548] loss: 21.468929290771484\n",
      "[step: 549] loss: 21.459257125854492\n",
      "[step: 550] loss: 21.449674606323242\n",
      "[step: 551] loss: 21.44019317626953\n",
      "[step: 552] loss: 21.430805206298828\n",
      "[step: 553] loss: 21.42151641845703\n",
      "[step: 554] loss: 21.412338256835938\n",
      "[step: 555] loss: 21.403282165527344\n",
      "[step: 556] loss: 21.394315719604492\n",
      "[step: 557] loss: 21.3854923248291\n",
      "[step: 558] loss: 21.376771926879883\n",
      "[step: 559] loss: 21.368167877197266\n",
      "[step: 560] loss: 21.35969352722168\n",
      "[step: 561] loss: 21.351354598999023\n",
      "[step: 562] loss: 21.343128204345703\n",
      "[step: 563] loss: 21.335046768188477\n",
      "[step: 564] loss: 21.327089309692383\n",
      "[step: 565] loss: 21.319276809692383\n",
      "[step: 566] loss: 21.31159782409668\n",
      "[step: 567] loss: 21.304061889648438\n",
      "[step: 568] loss: 21.296667098999023\n",
      "[step: 569] loss: 21.289417266845703\n",
      "[step: 570] loss: 21.28231430053711\n",
      "[step: 571] loss: 21.275362014770508\n",
      "[step: 572] loss: 21.268543243408203\n",
      "[step: 573] loss: 21.261882781982422\n",
      "[step: 574] loss: 21.255373001098633\n",
      "[step: 575] loss: 21.249013900756836\n",
      "[step: 576] loss: 21.242809295654297\n",
      "[step: 577] loss: 21.236757278442383\n",
      "[step: 578] loss: 21.23085594177246\n",
      "[step: 579] loss: 21.22511100769043\n",
      "[step: 580] loss: 21.219511032104492\n",
      "[step: 581] loss: 21.214069366455078\n",
      "[step: 582] loss: 21.208786010742188\n",
      "[step: 583] loss: 21.203651428222656\n",
      "[step: 584] loss: 21.198650360107422\n",
      "[step: 585] loss: 21.193815231323242\n",
      "[step: 586] loss: 21.18912124633789\n",
      "[step: 587] loss: 21.184568405151367\n",
      "[step: 588] loss: 21.180160522460938\n",
      "[step: 589] loss: 21.175912857055664\n",
      "[step: 590] loss: 21.171796798706055\n",
      "[step: 591] loss: 21.16781997680664\n",
      "[step: 592] loss: 21.163976669311523\n",
      "[step: 593] loss: 21.160280227661133\n",
      "[step: 594] loss: 21.15669059753418\n",
      "[step: 595] loss: 21.153261184692383\n",
      "[step: 596] loss: 21.14993667602539\n",
      "[step: 597] loss: 21.146745681762695\n",
      "[step: 598] loss: 21.143686294555664\n",
      "[step: 599] loss: 21.140724182128906\n",
      "[step: 600] loss: 21.137893676757812\n",
      "[step: 601] loss: 21.13515853881836\n",
      "[step: 602] loss: 21.132537841796875\n",
      "[step: 603] loss: 21.130027770996094\n",
      "[step: 604] loss: 21.12761878967285\n",
      "[step: 605] loss: 21.125303268432617\n",
      "[step: 606] loss: 21.123079299926758\n",
      "[step: 607] loss: 21.120956420898438\n",
      "[step: 608] loss: 21.118915557861328\n",
      "[step: 609] loss: 21.116966247558594\n",
      "[step: 610] loss: 21.115095138549805\n",
      "[step: 611] loss: 21.11330223083496\n",
      "[step: 612] loss: 21.111583709716797\n",
      "[step: 613] loss: 21.109933853149414\n",
      "[step: 614] loss: 21.108356475830078\n",
      "[step: 615] loss: 21.10685157775879\n",
      "[step: 616] loss: 21.10540199279785\n",
      "[step: 617] loss: 21.104019165039062\n",
      "[step: 618] loss: 21.102676391601562\n",
      "[step: 619] loss: 21.101394653320312\n",
      "[step: 620] loss: 21.100173950195312\n",
      "[step: 621] loss: 21.098989486694336\n",
      "[step: 622] loss: 21.09786033630371\n",
      "[step: 623] loss: 21.096769332885742\n",
      "[step: 624] loss: 21.095720291137695\n",
      "[step: 625] loss: 21.09470558166504\n",
      "[step: 626] loss: 21.09372329711914\n",
      "[step: 627] loss: 21.092792510986328\n",
      "[step: 628] loss: 21.09187889099121\n",
      "[step: 629] loss: 21.091005325317383\n",
      "[step: 630] loss: 21.090166091918945\n",
      "[step: 631] loss: 21.08933448791504\n",
      "[step: 632] loss: 21.088531494140625\n",
      "[step: 633] loss: 21.087764739990234\n",
      "[step: 634] loss: 21.087011337280273\n",
      "[step: 635] loss: 21.086275100708008\n",
      "[step: 636] loss: 21.0855655670166\n",
      "[step: 637] loss: 21.084877014160156\n",
      "[step: 638] loss: 21.084197998046875\n",
      "[step: 639] loss: 21.083545684814453\n",
      "[step: 640] loss: 21.082897186279297\n",
      "[step: 641] loss: 21.08225440979004\n",
      "[step: 642] loss: 21.08165168762207\n",
      "[step: 643] loss: 21.081039428710938\n",
      "[step: 644] loss: 21.080440521240234\n",
      "[step: 645] loss: 21.079866409301758\n",
      "[step: 646] loss: 21.079286575317383\n",
      "[step: 647] loss: 21.07872200012207\n",
      "[step: 648] loss: 21.078161239624023\n",
      "[step: 649] loss: 21.077617645263672\n",
      "[step: 650] loss: 21.077072143554688\n",
      "[step: 651] loss: 21.07654571533203\n",
      "[step: 652] loss: 21.076017379760742\n",
      "[step: 653] loss: 21.07550621032715\n",
      "[step: 654] loss: 21.074989318847656\n",
      "[step: 655] loss: 21.074481964111328\n",
      "[step: 656] loss: 21.07398223876953\n",
      "[step: 657] loss: 21.073476791381836\n",
      "[step: 658] loss: 21.072996139526367\n",
      "[step: 659] loss: 21.0725040435791\n",
      "[step: 660] loss: 21.072019577026367\n",
      "[step: 661] loss: 21.071548461914062\n",
      "[step: 662] loss: 21.071063995361328\n",
      "[step: 663] loss: 21.070600509643555\n",
      "[step: 664] loss: 21.070133209228516\n",
      "[step: 665] loss: 21.06966781616211\n",
      "[step: 666] loss: 21.069215774536133\n",
      "[step: 667] loss: 21.06875228881836\n",
      "[step: 668] loss: 21.06829261779785\n",
      "[step: 669] loss: 21.06784439086914\n",
      "[step: 670] loss: 21.067398071289062\n",
      "[step: 671] loss: 21.06694793701172\n",
      "[step: 672] loss: 21.066511154174805\n",
      "[step: 673] loss: 21.066068649291992\n",
      "[step: 674] loss: 21.065629959106445\n",
      "[step: 675] loss: 21.0651912689209\n",
      "[step: 676] loss: 21.06476402282715\n",
      "[step: 677] loss: 21.0643310546875\n",
      "[step: 678] loss: 21.06389617919922\n",
      "[step: 679] loss: 21.06346893310547\n",
      "[step: 680] loss: 21.06304931640625\n",
      "[step: 681] loss: 21.062623977661133\n",
      "[step: 682] loss: 21.062198638916016\n",
      "[step: 683] loss: 21.061782836914062\n",
      "[step: 684] loss: 21.06136703491211\n",
      "[step: 685] loss: 21.06095314025879\n",
      "[step: 686] loss: 21.06053352355957\n",
      "[step: 687] loss: 21.060117721557617\n",
      "[step: 688] loss: 21.059717178344727\n",
      "[step: 689] loss: 21.059301376342773\n",
      "[step: 690] loss: 21.058900833129883\n",
      "[step: 691] loss: 21.058494567871094\n",
      "[step: 692] loss: 21.05808448791504\n",
      "[step: 693] loss: 21.057680130004883\n",
      "[step: 694] loss: 21.057279586791992\n",
      "[step: 695] loss: 21.056875228881836\n",
      "[step: 696] loss: 21.05647850036621\n",
      "[step: 697] loss: 21.056074142456055\n",
      "[step: 698] loss: 21.055681228637695\n",
      "[step: 699] loss: 21.05528450012207\n",
      "[step: 700] loss: 21.054887771606445\n",
      "[step: 701] loss: 21.05450439453125\n",
      "[step: 702] loss: 21.054101943969727\n",
      "[step: 703] loss: 21.053712844848633\n",
      "[step: 704] loss: 21.053327560424805\n",
      "[step: 705] loss: 21.05293846130371\n",
      "[step: 706] loss: 21.05254364013672\n",
      "[step: 707] loss: 21.052156448364258\n",
      "[step: 708] loss: 21.051767349243164\n",
      "[step: 709] loss: 21.0513858795166\n",
      "[step: 710] loss: 21.05100440979004\n",
      "[step: 711] loss: 21.050615310668945\n",
      "[step: 712] loss: 21.05023956298828\n",
      "[step: 713] loss: 21.049846649169922\n",
      "[step: 714] loss: 21.04947853088379\n",
      "[step: 715] loss: 21.049083709716797\n",
      "[step: 716] loss: 21.048709869384766\n",
      "[step: 717] loss: 21.048324584960938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 718] loss: 21.04795265197754\n",
      "[step: 719] loss: 21.047571182250977\n",
      "[step: 720] loss: 21.047195434570312\n",
      "[step: 721] loss: 21.04681968688965\n",
      "[step: 722] loss: 21.046443939208984\n",
      "[step: 723] loss: 21.046062469482422\n",
      "[step: 724] loss: 21.045696258544922\n",
      "[step: 725] loss: 21.045324325561523\n",
      "[step: 726] loss: 21.044946670532227\n",
      "[step: 727] loss: 21.044574737548828\n",
      "[step: 728] loss: 21.044204711914062\n",
      "[step: 729] loss: 21.043832778930664\n",
      "[step: 730] loss: 21.043453216552734\n",
      "[step: 731] loss: 21.043088912963867\n",
      "[step: 732] loss: 21.0427188873291\n",
      "[step: 733] loss: 21.04235076904297\n",
      "[step: 734] loss: 21.041975021362305\n",
      "[step: 735] loss: 21.041610717773438\n",
      "[step: 736] loss: 21.04123878479004\n",
      "[step: 737] loss: 21.040876388549805\n",
      "[step: 738] loss: 21.040502548217773\n",
      "[step: 739] loss: 21.040138244628906\n",
      "[step: 740] loss: 21.03976821899414\n",
      "[step: 741] loss: 21.039400100708008\n",
      "[step: 742] loss: 21.039033889770508\n",
      "[step: 743] loss: 21.038665771484375\n",
      "[step: 744] loss: 21.03830909729004\n",
      "[step: 745] loss: 21.037944793701172\n",
      "[step: 746] loss: 21.03757667541504\n",
      "[step: 747] loss: 21.037208557128906\n",
      "[step: 748] loss: 21.036840438842773\n",
      "[step: 749] loss: 21.036479949951172\n",
      "[step: 750] loss: 21.036117553710938\n",
      "[step: 751] loss: 21.035743713378906\n",
      "[step: 752] loss: 21.035383224487305\n",
      "[step: 753] loss: 21.035022735595703\n",
      "[step: 754] loss: 21.03466033935547\n",
      "[step: 755] loss: 21.034299850463867\n",
      "[step: 756] loss: 21.033931732177734\n",
      "[step: 757] loss: 21.033557891845703\n",
      "[step: 758] loss: 21.033201217651367\n",
      "[step: 759] loss: 21.032835006713867\n",
      "[step: 760] loss: 21.032472610473633\n",
      "[step: 761] loss: 21.032108306884766\n",
      "[step: 762] loss: 21.0317440032959\n",
      "[step: 763] loss: 21.03138542175293\n",
      "[step: 764] loss: 21.03101921081543\n",
      "[step: 765] loss: 21.03065299987793\n",
      "[step: 766] loss: 21.03030014038086\n",
      "[step: 767] loss: 21.029932022094727\n",
      "[step: 768] loss: 21.029569625854492\n",
      "[step: 769] loss: 21.02920150756836\n",
      "[step: 770] loss: 21.02884864807129\n",
      "[step: 771] loss: 21.028474807739258\n",
      "[step: 772] loss: 21.028114318847656\n",
      "[step: 773] loss: 21.027753829956055\n",
      "[step: 774] loss: 21.027389526367188\n",
      "[step: 775] loss: 21.027021408081055\n",
      "[step: 776] loss: 21.02665138244629\n",
      "[step: 777] loss: 21.026302337646484\n",
      "[step: 778] loss: 21.025928497314453\n",
      "[step: 779] loss: 21.025562286376953\n",
      "[step: 780] loss: 21.02519989013672\n",
      "[step: 781] loss: 21.024839401245117\n",
      "[step: 782] loss: 21.024477005004883\n",
      "[step: 783] loss: 21.024112701416016\n",
      "[step: 784] loss: 21.023752212524414\n",
      "[step: 785] loss: 21.02337646484375\n",
      "[step: 786] loss: 21.02301788330078\n",
      "[step: 787] loss: 21.022655487060547\n",
      "[step: 788] loss: 21.02229118347168\n",
      "[step: 789] loss: 21.02191925048828\n",
      "[step: 790] loss: 21.021549224853516\n",
      "[step: 791] loss: 21.02118492126465\n",
      "[step: 792] loss: 21.020822525024414\n",
      "[step: 793] loss: 21.02046012878418\n",
      "[step: 794] loss: 21.020090103149414\n",
      "[step: 795] loss: 21.019718170166016\n",
      "[step: 796] loss: 21.01935386657715\n",
      "[step: 797] loss: 21.01898765563965\n",
      "[step: 798] loss: 21.01862335205078\n",
      "[step: 799] loss: 21.01825523376465\n",
      "[step: 800] loss: 21.017892837524414\n",
      "[step: 801] loss: 21.017515182495117\n",
      "[step: 802] loss: 21.017147064208984\n",
      "[step: 803] loss: 21.016780853271484\n",
      "[step: 804] loss: 21.016408920288086\n",
      "[step: 805] loss: 21.016040802001953\n",
      "[step: 806] loss: 21.015674591064453\n",
      "[step: 807] loss: 21.015300750732422\n",
      "[step: 808] loss: 21.014938354492188\n",
      "[step: 809] loss: 21.01456069946289\n",
      "[step: 810] loss: 21.014184951782227\n",
      "[step: 811] loss: 21.013811111450195\n",
      "[step: 812] loss: 21.01344871520996\n",
      "[step: 813] loss: 21.013078689575195\n",
      "[step: 814] loss: 21.01270866394043\n",
      "[step: 815] loss: 21.0123348236084\n",
      "[step: 816] loss: 21.011951446533203\n",
      "[step: 817] loss: 21.01158332824707\n",
      "[step: 818] loss: 21.011207580566406\n",
      "[step: 819] loss: 21.010839462280273\n",
      "[step: 820] loss: 21.010461807250977\n",
      "[step: 821] loss: 21.01007652282715\n",
      "[step: 822] loss: 21.009708404541016\n",
      "[step: 823] loss: 21.00933265686035\n",
      "[step: 824] loss: 21.008960723876953\n",
      "[step: 825] loss: 21.008577346801758\n",
      "[step: 826] loss: 21.008203506469727\n",
      "[step: 827] loss: 21.007827758789062\n",
      "[step: 828] loss: 21.0074462890625\n",
      "[step: 829] loss: 21.007064819335938\n",
      "[step: 830] loss: 21.00667381286621\n",
      "[step: 831] loss: 21.00630760192871\n",
      "[step: 832] loss: 21.00592041015625\n",
      "[step: 833] loss: 21.00554847717285\n",
      "[step: 834] loss: 21.005163192749023\n",
      "[step: 835] loss: 21.004783630371094\n",
      "[step: 836] loss: 21.0044002532959\n",
      "[step: 837] loss: 21.004009246826172\n",
      "[step: 838] loss: 21.00362777709961\n",
      "[step: 839] loss: 21.003250122070312\n",
      "[step: 840] loss: 21.00286102294922\n",
      "[step: 841] loss: 21.002483367919922\n",
      "[step: 842] loss: 21.002092361450195\n",
      "[step: 843] loss: 21.0017032623291\n",
      "[step: 844] loss: 21.001314163208008\n",
      "[step: 845] loss: 21.000925064086914\n",
      "[step: 846] loss: 21.00053596496582\n",
      "[step: 847] loss: 21.000146865844727\n",
      "[step: 848] loss: 20.9997615814209\n",
      "[step: 849] loss: 20.99936866760254\n",
      "[step: 850] loss: 20.99898338317871\n",
      "[step: 851] loss: 20.99859046936035\n",
      "[step: 852] loss: 20.99818992614746\n",
      "[step: 853] loss: 20.997800827026367\n",
      "[step: 854] loss: 20.997413635253906\n",
      "[step: 855] loss: 20.997026443481445\n",
      "[step: 856] loss: 20.996623992919922\n",
      "[step: 857] loss: 20.996231079101562\n",
      "[step: 858] loss: 20.995838165283203\n",
      "[step: 859] loss: 20.995431900024414\n",
      "[step: 860] loss: 20.995031356811523\n",
      "[step: 861] loss: 20.99463653564453\n",
      "[step: 862] loss: 20.994243621826172\n",
      "[step: 863] loss: 20.993850708007812\n",
      "[step: 864] loss: 20.993452072143555\n",
      "[step: 865] loss: 20.9930362701416\n",
      "[step: 866] loss: 20.992639541625977\n",
      "[step: 867] loss: 20.99224090576172\n",
      "[step: 868] loss: 20.991836547851562\n",
      "[step: 869] loss: 20.991424560546875\n",
      "[step: 870] loss: 20.99102020263672\n",
      "[step: 871] loss: 20.990615844726562\n",
      "[step: 872] loss: 20.990217208862305\n",
      "[step: 873] loss: 20.989803314208984\n",
      "[step: 874] loss: 20.98939323425293\n",
      "[step: 875] loss: 20.98899269104004\n",
      "[step: 876] loss: 20.98858070373535\n",
      "[step: 877] loss: 20.988161087036133\n",
      "[step: 878] loss: 20.98775863647461\n",
      "[step: 879] loss: 20.987346649169922\n",
      "[step: 880] loss: 20.98693084716797\n",
      "[step: 881] loss: 20.98651695251465\n",
      "[step: 882] loss: 20.986103057861328\n",
      "[step: 883] loss: 20.985681533813477\n",
      "[step: 884] loss: 20.985267639160156\n",
      "[step: 885] loss: 20.9848575592041\n",
      "[step: 886] loss: 20.984432220458984\n",
      "[step: 887] loss: 20.984010696411133\n",
      "[step: 888] loss: 20.98358917236328\n",
      "[step: 889] loss: 20.983179092407227\n",
      "[step: 890] loss: 20.98275375366211\n",
      "[step: 891] loss: 20.982332229614258\n",
      "[step: 892] loss: 20.981904983520508\n",
      "[step: 893] loss: 20.98148536682129\n",
      "[step: 894] loss: 20.98105239868164\n",
      "[step: 895] loss: 20.980623245239258\n",
      "[step: 896] loss: 20.980195999145508\n",
      "[step: 897] loss: 20.979768753051758\n",
      "[step: 898] loss: 20.97933578491211\n",
      "[step: 899] loss: 20.978912353515625\n",
      "[step: 900] loss: 20.978473663330078\n",
      "[step: 901] loss: 20.97804832458496\n",
      "[step: 902] loss: 20.977617263793945\n",
      "[step: 903] loss: 20.9771728515625\n",
      "[step: 904] loss: 20.976736068725586\n",
      "[step: 905] loss: 20.976306915283203\n",
      "[step: 906] loss: 20.975860595703125\n",
      "[step: 907] loss: 20.975419998168945\n",
      "[step: 908] loss: 20.97498893737793\n",
      "[step: 909] loss: 20.974550247192383\n",
      "[step: 910] loss: 20.974103927612305\n",
      "[step: 911] loss: 20.973661422729492\n",
      "[step: 912] loss: 20.973217010498047\n",
      "[step: 913] loss: 20.97276496887207\n",
      "[step: 914] loss: 20.97231674194336\n",
      "[step: 915] loss: 20.971872329711914\n",
      "[step: 916] loss: 20.971424102783203\n",
      "[step: 917] loss: 20.970977783203125\n",
      "[step: 918] loss: 20.97052001953125\n",
      "[step: 919] loss: 20.97005844116211\n",
      "[step: 920] loss: 20.969606399536133\n",
      "[step: 921] loss: 20.96915626525879\n",
      "[step: 922] loss: 20.96870231628418\n",
      "[step: 923] loss: 20.96824073791504\n",
      "[step: 924] loss: 20.967788696289062\n",
      "[step: 925] loss: 20.967323303222656\n",
      "[step: 926] loss: 20.966869354248047\n",
      "[step: 927] loss: 20.966394424438477\n",
      "[step: 928] loss: 20.965940475463867\n",
      "[step: 929] loss: 20.965471267700195\n",
      "[step: 930] loss: 20.965011596679688\n",
      "[step: 931] loss: 20.964542388916016\n",
      "[step: 932] loss: 20.964067459106445\n",
      "[step: 933] loss: 20.96360206604004\n",
      "[step: 934] loss: 20.963130950927734\n",
      "[step: 935] loss: 20.962650299072266\n",
      "[step: 936] loss: 20.962177276611328\n",
      "[step: 937] loss: 20.961706161499023\n",
      "[step: 938] loss: 20.961225509643555\n",
      "[step: 939] loss: 20.960756301879883\n",
      "[step: 940] loss: 20.96026611328125\n",
      "[step: 941] loss: 20.959787368774414\n",
      "[step: 942] loss: 20.95930290222168\n",
      "[step: 943] loss: 20.958818435668945\n",
      "[step: 944] loss: 20.958337783813477\n",
      "[step: 945] loss: 20.95785903930664\n",
      "[step: 946] loss: 20.957365036010742\n",
      "[step: 947] loss: 20.956878662109375\n",
      "[step: 948] loss: 20.95638656616211\n",
      "[step: 949] loss: 20.955888748168945\n",
      "[step: 950] loss: 20.955392837524414\n",
      "[step: 951] loss: 20.95490264892578\n",
      "[step: 952] loss: 20.95440673828125\n",
      "[step: 953] loss: 20.953907012939453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 954] loss: 20.953407287597656\n",
      "[step: 955] loss: 20.95290756225586\n",
      "[step: 956] loss: 20.95241355895996\n",
      "[step: 957] loss: 20.951902389526367\n",
      "[step: 958] loss: 20.95138931274414\n",
      "[step: 959] loss: 20.950895309448242\n",
      "[step: 960] loss: 20.95038604736328\n",
      "[step: 961] loss: 20.94986915588379\n",
      "[step: 962] loss: 20.949365615844727\n",
      "[step: 963] loss: 20.9488468170166\n",
      "[step: 964] loss: 20.948333740234375\n",
      "[step: 965] loss: 20.94782066345215\n",
      "[step: 966] loss: 20.947301864624023\n",
      "[step: 967] loss: 20.94678497314453\n",
      "[step: 968] loss: 20.946266174316406\n",
      "[step: 969] loss: 20.94573974609375\n",
      "[step: 970] loss: 20.945226669311523\n",
      "[step: 971] loss: 20.944698333740234\n",
      "[step: 972] loss: 20.944168090820312\n",
      "[step: 973] loss: 20.943641662597656\n",
      "[step: 974] loss: 20.943119049072266\n",
      "[step: 975] loss: 20.942588806152344\n",
      "[step: 976] loss: 20.942047119140625\n",
      "[step: 977] loss: 20.941516876220703\n",
      "[step: 978] loss: 20.940988540649414\n",
      "[step: 979] loss: 20.940452575683594\n",
      "[step: 980] loss: 20.939908981323242\n",
      "[step: 981] loss: 20.939367294311523\n",
      "[step: 982] loss: 20.938825607299805\n",
      "[step: 983] loss: 20.938278198242188\n",
      "[step: 984] loss: 20.93773651123047\n",
      "[step: 985] loss: 20.937192916870117\n",
      "[step: 986] loss: 20.9366397857666\n",
      "[step: 987] loss: 20.936098098754883\n",
      "[step: 988] loss: 20.935544967651367\n",
      "[step: 989] loss: 20.934988021850586\n",
      "[step: 990] loss: 20.93444061279297\n",
      "[step: 991] loss: 20.933887481689453\n",
      "[step: 992] loss: 20.933324813842773\n",
      "[step: 993] loss: 20.932769775390625\n",
      "[step: 994] loss: 20.932209014892578\n",
      "[step: 995] loss: 20.931638717651367\n",
      "[step: 996] loss: 20.93108558654785\n",
      "[step: 997] loss: 20.930519104003906\n",
      "[step: 998] loss: 20.929950714111328\n",
      "[step: 999] loss: 20.929378509521484\n",
      "[step: 1000] loss: 20.928813934326172\n",
      "RMSE: 2.360671105271054\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBI//D4cH0yUClIRcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe1df2bb908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "def predict(file):\n",
    "    # train Parameters\n",
    "    seq_length = 6\n",
    "    data_dim = 1\n",
    "    hidden_dim = 12 # 내 맘대로 정해도 됨\n",
    "    output_dim = 1\n",
    "    learning_rate = 0.01\n",
    "    iterations = 1000\n",
    "    layer_num=1\n",
    "\n",
    "    # train Data: Open, High, Low, Volume, Close\n",
    "    origin_xy = np.loadtxt('train.csv', delimiter=',', skiprows=1, usecols=range(1,8))\n",
    "    xy = MinMaxScaler(origin_xy)\n",
    "    x = xy[:,:-1]\n",
    "    y = xy[:,-1]\n",
    "    x = x.reshape(-1,seq_length,data_dim)\n",
    "    y = y.reshape(-1,data_dim) \n",
    "\n",
    "    # train/validation split\n",
    "    train_size = int(len(y) * 0.7)\n",
    "    test_size = len(y) - train_size\n",
    "    trainX, validX = np.array(x[0:train_size]), np.array(x[train_size:])\n",
    "    trainY, validY = np.array(y[0:train_size]), np.array(y[train_size:])\n",
    "\n",
    "    #test data\n",
    "    test_x=np.loadtxt(file, delimiter=',', skiprows=1, usecols=range(1,7))\n",
    "    test_x = test_x.reshape(-1,seq_length, data_dim)\n",
    "    \n",
    "    # input place holders\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # build a LSTM network\n",
    "    cells=[]\n",
    "    for _ in range(layer_num):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "        #cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.7)\n",
    "        cells.append(cell)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "    # cost/loss\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # RMSE\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "    sess=tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i+1, step_loss))\n",
    "\n",
    "    # validation step\n",
    "    valid_predict = sess.run(Y_pred, feed_dict={X: validX})\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validY, predictions: valid_predict})\n",
    "    revised_rmse = rmse_val*(origin_xy.max()-origin_xy.min()+1e-7) + origin_xy.min()\n",
    "    print(\"RMSE: {}\".format(revised_rmse))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(validY)\n",
    "    plt.plot(valid_predict)\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Temperature\")\n",
    "    plt.show()\n",
    "    \n",
    "    #test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: test_x})\n",
    "    prediction_list = []\n",
    "    for i in test_predict:\n",
    "        revised_pred = i[0]*(origin_xy.max()-origin_xy.min()+1e-7) + origin_xy.min()\n",
    "        prediction_list.append(revised_pred)\n",
    "    return prediction_list\n",
    "\n",
    "def write_result(predictions):\n",
    "    # You don't need to modify this function.\n",
    "    with open('result.csv', 'w') as f:\n",
    "        f.write('Value\\n')\n",
    "        for l in predictions:\n",
    "            f.write('{}\\n'.format(l))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # You don't need to modify this function.\n",
    "    predictions = predict('test.csv')\n",
    "    write_result(predictions)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # You don't need to modify this part.\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
