{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to add any functions, import statements, and variables.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1] loss: 495.4986267089844\n",
      "[step: 2] loss: 42.58356857299805\n",
      "[step: 3] loss: 719.3489379882812\n",
      "[step: 4] loss: 47.43688201904297\n",
      "[step: 5] loss: 251.87680053710938\n",
      "[step: 6] loss: 253.50457763671875\n",
      "[step: 7] loss: 176.7390899658203\n",
      "[step: 8] loss: 111.55397033691406\n",
      "[step: 9] loss: 71.56097412109375\n",
      "[step: 10] loss: 53.49079513549805\n",
      "[step: 11] loss: 51.83563232421875\n",
      "[step: 12] loss: 59.824462890625\n",
      "[step: 13] loss: 69.6226577758789\n",
      "[step: 14] loss: 75.04723358154297\n",
      "[step: 15] loss: 74.1891860961914\n",
      "[step: 16] loss: 68.87608337402344\n",
      "[step: 17] loss: 62.224334716796875\n",
      "[step: 18] loss: 56.68928146362305\n",
      "[step: 19] loss: 53.43173599243164\n",
      "[step: 20] loss: 52.52058029174805\n",
      "[step: 21] loss: 53.368446350097656\n",
      "[step: 22] loss: 55.119911193847656\n",
      "[step: 23] loss: 56.93553924560547\n",
      "[step: 24] loss: 58.180686950683594\n",
      "[step: 25] loss: 58.52450180053711\n",
      "[step: 26] loss: 57.951324462890625\n",
      "[step: 27] loss: 56.695518493652344\n",
      "[step: 28] loss: 55.12909698486328\n",
      "[step: 29] loss: 53.63892364501953\n",
      "[step: 30] loss: 52.52571487426758\n",
      "[step: 31] loss: 51.94300842285156\n",
      "[step: 32] loss: 51.88235855102539\n",
      "[step: 33] loss: 52.199771881103516\n",
      "[step: 34] loss: 52.67280197143555\n",
      "[step: 35] loss: 53.07120132446289\n",
      "[step: 36] loss: 53.22146987915039\n",
      "[step: 37] loss: 53.04779815673828\n",
      "[step: 38] loss: 52.579532623291016\n",
      "[step: 39] loss: 51.927162170410156\n",
      "[step: 40] loss: 51.23720932006836\n",
      "[step: 41] loss: 50.643348693847656\n",
      "[step: 42] loss: 50.22840881347656\n",
      "[step: 43] loss: 50.006473541259766\n",
      "[step: 44] loss: 49.92755889892578\n",
      "[step: 45] loss: 49.90059280395508\n",
      "[step: 46] loss: 49.82603073120117\n",
      "[step: 47] loss: 49.62799072265625\n",
      "[step: 48] loss: 49.275997161865234\n",
      "[step: 49] loss: 48.78956604003906\n",
      "[step: 50] loss: 48.225894927978516\n",
      "[step: 51] loss: 47.65427017211914\n",
      "[step: 52] loss: 47.12843704223633\n",
      "[step: 53] loss: 46.665775299072266\n",
      "[step: 54] loss: 46.24170684814453\n",
      "[step: 55] loss: 45.80048751831055\n",
      "[step: 56] loss: 45.278499603271484\n",
      "[step: 57] loss: 44.630226135253906\n",
      "[step: 58] loss: 43.84701156616211\n",
      "[step: 59] loss: 42.95960998535156\n",
      "[step: 60] loss: 42.022281646728516\n",
      "[step: 61] loss: 41.08230209350586\n",
      "[step: 62] loss: 40.146705627441406\n",
      "[step: 63] loss: 39.165157318115234\n",
      "[step: 64] loss: 38.0495491027832\n",
      "[step: 65] loss: 36.7369499206543\n",
      "[step: 66] loss: 35.2587890625\n",
      "[step: 67] loss: 33.742271423339844\n",
      "[step: 68] loss: 32.311222076416016\n",
      "[step: 69] loss: 30.98594093322754\n",
      "[step: 70] loss: 29.750730514526367\n",
      "[step: 71] loss: 28.774274826049805\n",
      "[step: 72] loss: 28.378170013427734\n",
      "[step: 73] loss: 28.499895095825195\n",
      "[step: 74] loss: 28.8228702545166\n",
      "[step: 75] loss: 29.508832931518555\n",
      "[step: 76] loss: 30.166133880615234\n",
      "[step: 77] loss: 30.22664451599121\n",
      "[step: 78] loss: 29.99323081970215\n",
      "[step: 79] loss: 29.389633178710938\n",
      "[step: 80] loss: 28.579822540283203\n",
      "[step: 81] loss: 28.000417709350586\n",
      "[step: 82] loss: 27.57487678527832\n",
      "[step: 83] loss: 27.339101791381836\n",
      "[step: 84] loss: 27.344533920288086\n",
      "[step: 85] loss: 27.42124366760254\n",
      "[step: 86] loss: 27.493188858032227\n",
      "[step: 87] loss: 27.591297149658203\n",
      "[step: 88] loss: 27.659456253051758\n",
      "[step: 89] loss: 27.635738372802734\n",
      "[step: 90] loss: 27.552640914916992\n",
      "[step: 91] loss: 27.441158294677734\n",
      "[step: 92] loss: 27.274755477905273\n",
      "[step: 93] loss: 27.066768646240234\n",
      "[step: 94] loss: 26.875211715698242\n",
      "[step: 95] loss: 26.70381736755371\n",
      "[step: 96] loss: 26.545591354370117\n",
      "[step: 97] loss: 26.44257354736328\n",
      "[step: 98] loss: 26.390785217285156\n",
      "[step: 99] loss: 26.353099822998047\n",
      "[step: 100] loss: 26.341447830200195\n",
      "[step: 101] loss: 26.326284408569336\n",
      "[step: 102] loss: 26.27383804321289\n",
      "[step: 103] loss: 26.2039794921875\n",
      "[step: 104] loss: 26.101551055908203\n",
      "[step: 105] loss: 25.97775650024414\n",
      "[step: 106] loss: 25.862110137939453\n",
      "[step: 107] loss: 25.747861862182617\n",
      "[step: 108] loss: 25.65163803100586\n",
      "[step: 109] loss: 25.574913024902344\n",
      "[step: 110] loss: 25.50194549560547\n",
      "[step: 111] loss: 25.437658309936523\n",
      "[step: 112] loss: 25.37029266357422\n",
      "[step: 113] loss: 25.291200637817383\n",
      "[step: 114] loss: 25.2054443359375\n",
      "[step: 115] loss: 25.105133056640625\n",
      "[step: 116] loss: 24.994728088378906\n",
      "[step: 117] loss: 24.880462646484375\n",
      "[step: 118] loss: 24.760007858276367\n",
      "[step: 119] loss: 24.64215660095215\n",
      "[step: 120] loss: 24.524723052978516\n",
      "[step: 121] loss: 24.407468795776367\n",
      "[step: 122] loss: 24.290950775146484\n",
      "[step: 123] loss: 24.168235778808594\n",
      "[step: 124] loss: 24.040977478027344\n",
      "[step: 125] loss: 23.902944564819336\n",
      "[step: 126] loss: 23.75745964050293\n",
      "[step: 127] loss: 23.603878021240234\n",
      "[step: 128] loss: 23.445890426635742\n",
      "[step: 129] loss: 23.28429412841797\n",
      "[step: 130] loss: 23.120710372924805\n",
      "[step: 131] loss: 22.956193923950195\n",
      "[step: 132] loss: 22.790428161621094\n",
      "[step: 133] loss: 22.622541427612305\n",
      "[step: 134] loss: 22.452665328979492\n",
      "[step: 135] loss: 22.281919479370117\n",
      "[step: 136] loss: 22.113204956054688\n",
      "[step: 137] loss: 21.948259353637695\n",
      "[step: 138] loss: 21.79327392578125\n",
      "[step: 139] loss: 21.651641845703125\n",
      "[step: 140] loss: 21.528236389160156\n",
      "[step: 141] loss: 21.427026748657227\n",
      "[step: 142] loss: 21.349512100219727\n",
      "[step: 143] loss: 21.2971134185791\n",
      "[step: 144] loss: 21.269634246826172\n",
      "[step: 145] loss: 21.264148712158203\n",
      "[step: 146] loss: 21.276145935058594\n",
      "[step: 147] loss: 21.299325942993164\n",
      "[step: 148] loss: 21.3264102935791\n",
      "[step: 149] loss: 21.3496150970459\n",
      "[step: 150] loss: 21.36256980895996\n",
      "[step: 151] loss: 21.36155891418457\n",
      "[step: 152] loss: 21.346031188964844\n",
      "[step: 153] loss: 21.318241119384766\n",
      "[step: 154] loss: 21.28223991394043\n",
      "[step: 155] loss: 21.242353439331055\n",
      "[step: 156] loss: 21.202678680419922\n",
      "[step: 157] loss: 21.165889739990234\n",
      "[step: 158] loss: 21.134132385253906\n",
      "[step: 159] loss: 21.10810661315918\n",
      "[step: 160] loss: 21.088300704956055\n",
      "[step: 161] loss: 21.074169158935547\n",
      "[step: 162] loss: 21.064565658569336\n",
      "[step: 163] loss: 21.05808448791504\n",
      "[step: 164] loss: 21.053300857543945\n",
      "[step: 165] loss: 21.049480438232422\n",
      "[step: 166] loss: 21.0459041595459\n",
      "[step: 167] loss: 21.041828155517578\n",
      "[step: 168] loss: 21.03678321838379\n",
      "[step: 169] loss: 21.030717849731445\n",
      "[step: 170] loss: 21.02379035949707\n",
      "[step: 171] loss: 21.016036987304688\n",
      "[step: 172] loss: 21.007484436035156\n",
      "[step: 173] loss: 20.998374938964844\n",
      "[step: 174] loss: 20.98911476135254\n",
      "[step: 175] loss: 20.980012893676758\n",
      "[step: 176] loss: 20.971263885498047\n",
      "[step: 177] loss: 20.963111877441406\n",
      "[step: 178] loss: 20.955760955810547\n",
      "[step: 179] loss: 20.94932746887207\n",
      "[step: 180] loss: 20.94377899169922\n",
      "[step: 181] loss: 20.93901252746582\n",
      "[step: 182] loss: 20.93488311767578\n",
      "[step: 183] loss: 20.931293487548828\n",
      "[step: 184] loss: 20.9280948638916\n",
      "[step: 185] loss: 20.925134658813477\n",
      "[step: 186] loss: 20.92226791381836\n",
      "[step: 187] loss: 20.91936492919922\n",
      "[step: 188] loss: 20.916351318359375\n",
      "[step: 189] loss: 20.913192749023438\n",
      "[step: 190] loss: 20.909908294677734\n",
      "[step: 191] loss: 20.906553268432617\n",
      "[step: 192] loss: 20.903175354003906\n",
      "[step: 193] loss: 20.899818420410156\n",
      "[step: 194] loss: 20.896556854248047\n",
      "[step: 195] loss: 20.893442153930664\n",
      "[step: 196] loss: 20.890499114990234\n",
      "[step: 197] loss: 20.887739181518555\n",
      "[step: 198] loss: 20.885177612304688\n",
      "[step: 199] loss: 20.882783889770508\n",
      "[step: 200] loss: 20.880538940429688\n",
      "[step: 201] loss: 20.87843132019043\n",
      "[step: 202] loss: 20.87642478942871\n",
      "[step: 203] loss: 20.874479293823242\n",
      "[step: 204] loss: 20.87258529663086\n",
      "[step: 205] loss: 20.87070655822754\n",
      "[step: 206] loss: 20.86883544921875\n",
      "[step: 207] loss: 20.866975784301758\n",
      "[step: 208] loss: 20.865116119384766\n",
      "[step: 209] loss: 20.8632755279541\n",
      "[step: 210] loss: 20.8614444732666\n",
      "[step: 211] loss: 20.859642028808594\n",
      "[step: 212] loss: 20.85787010192871\n",
      "[step: 213] loss: 20.856151580810547\n",
      "[step: 214] loss: 20.854463577270508\n",
      "[step: 215] loss: 20.852825164794922\n",
      "[step: 216] loss: 20.851245880126953\n",
      "[step: 217] loss: 20.849700927734375\n",
      "[step: 218] loss: 20.848194122314453\n",
      "[step: 219] loss: 20.846721649169922\n",
      "[step: 220] loss: 20.845285415649414\n",
      "[step: 221] loss: 20.8438720703125\n",
      "[step: 222] loss: 20.842472076416016\n",
      "[step: 223] loss: 20.84109878540039\n",
      "[step: 224] loss: 20.839725494384766\n",
      "[step: 225] loss: 20.8383846282959\n",
      "[step: 226] loss: 20.83704376220703\n",
      "[step: 227] loss: 20.835729598999023\n",
      "[step: 228] loss: 20.83441925048828\n",
      "[step: 229] loss: 20.83313751220703\n",
      "[step: 230] loss: 20.83187484741211\n",
      "[step: 231] loss: 20.830638885498047\n",
      "[step: 232] loss: 20.829402923583984\n",
      "[step: 233] loss: 20.828195571899414\n",
      "[step: 234] loss: 20.827009201049805\n",
      "[step: 235] loss: 20.825830459594727\n",
      "[step: 236] loss: 20.824684143066406\n",
      "[step: 237] loss: 20.823537826538086\n",
      "[step: 238] loss: 20.82240867614746\n",
      "[step: 239] loss: 20.821287155151367\n",
      "[step: 240] loss: 20.82018280029297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 241] loss: 20.81908416748047\n",
      "[step: 242] loss: 20.817989349365234\n",
      "[step: 243] loss: 20.816919326782227\n",
      "[step: 244] loss: 20.815847396850586\n",
      "[step: 245] loss: 20.814783096313477\n",
      "[step: 246] loss: 20.813735961914062\n",
      "[step: 247] loss: 20.812698364257812\n",
      "[step: 248] loss: 20.811676025390625\n",
      "[step: 249] loss: 20.810653686523438\n",
      "[step: 250] loss: 20.80963134765625\n",
      "[step: 251] loss: 20.808629989624023\n",
      "[step: 252] loss: 20.80763816833496\n",
      "[step: 253] loss: 20.806657791137695\n",
      "[step: 254] loss: 20.805665969848633\n",
      "[step: 255] loss: 20.8046932220459\n",
      "[step: 256] loss: 20.803722381591797\n",
      "[step: 257] loss: 20.802753448486328\n",
      "[step: 258] loss: 20.80179214477539\n",
      "[step: 259] loss: 20.800846099853516\n",
      "[step: 260] loss: 20.79990577697754\n",
      "[step: 261] loss: 20.798959732055664\n",
      "[step: 262] loss: 20.798015594482422\n",
      "[step: 263] loss: 20.797086715698242\n",
      "[step: 264] loss: 20.79615592956543\n",
      "[step: 265] loss: 20.795223236083984\n",
      "[step: 266] loss: 20.7943058013916\n",
      "[step: 267] loss: 20.793399810791016\n",
      "[step: 268] loss: 20.79248046875\n",
      "[step: 269] loss: 20.79157829284668\n",
      "[step: 270] loss: 20.79067039489746\n",
      "[step: 271] loss: 20.789777755737305\n",
      "[step: 272] loss: 20.78887367248535\n",
      "[step: 273] loss: 20.788002014160156\n",
      "[step: 274] loss: 20.787134170532227\n",
      "[step: 275] loss: 20.786296844482422\n",
      "[step: 276] loss: 20.7855281829834\n",
      "[step: 277] loss: 20.784893035888672\n",
      "[step: 278] loss: 20.784536361694336\n",
      "[step: 279] loss: 20.784814834594727\n",
      "[step: 280] loss: 20.786500930786133\n",
      "[step: 281] loss: 20.791255950927734\n",
      "[step: 282] loss: 20.803369522094727\n",
      "[step: 283] loss: 20.830183029174805\n",
      "[step: 284] loss: 20.891069412231445\n",
      "[step: 285] loss: 20.99495506286621\n",
      "[step: 286] loss: 21.167551040649414\n",
      "[step: 287] loss: 21.255680084228516\n",
      "[step: 288] loss: 21.21440887451172\n",
      "[step: 289] loss: 20.942401885986328\n",
      "[step: 290] loss: 20.77855682373047\n",
      "[step: 291] loss: 20.8682861328125\n",
      "[step: 292] loss: 20.997419357299805\n",
      "[step: 293] loss: 20.94794273376465\n",
      "[step: 294] loss: 20.795299530029297\n",
      "[step: 295] loss: 20.80129623413086\n",
      "[step: 296] loss: 20.906450271606445\n",
      "[step: 297] loss: 20.88473892211914\n",
      "[step: 298] loss: 20.787586212158203\n",
      "[step: 299] loss: 20.786115646362305\n",
      "[step: 300] loss: 20.850996017456055\n",
      "[step: 301] loss: 20.838808059692383\n",
      "[step: 302] loss: 20.773710250854492\n",
      "[step: 303] loss: 20.782411575317383\n",
      "[step: 304] loss: 20.82491111755371\n",
      "[step: 305] loss: 20.802125930786133\n",
      "[step: 306] loss: 20.764148712158203\n",
      "[step: 307] loss: 20.779735565185547\n",
      "[step: 308] loss: 20.80181884765625\n",
      "[step: 309] loss: 20.78163719177246\n",
      "[step: 310] loss: 20.759803771972656\n",
      "[step: 311] loss: 20.77390480041504\n",
      "[step: 312] loss: 20.786779403686523\n",
      "[step: 313] loss: 20.7701473236084\n",
      "[step: 314] loss: 20.75696563720703\n",
      "[step: 315] loss: 20.767047882080078\n",
      "[step: 316] loss: 20.77377700805664\n",
      "[step: 317] loss: 20.76241111755371\n",
      "[step: 318] loss: 20.7540225982666\n",
      "[step: 319] loss: 20.760717391967773\n",
      "[step: 320] loss: 20.764856338500977\n",
      "[step: 321] loss: 20.756690979003906\n",
      "[step: 322] loss: 20.7509765625\n",
      "[step: 323] loss: 20.755130767822266\n",
      "[step: 324] loss: 20.75763511657715\n",
      "[step: 325] loss: 20.752363204956055\n",
      "[step: 326] loss: 20.74793243408203\n",
      "[step: 327] loss: 20.7499942779541\n",
      "[step: 328] loss: 20.751977920532227\n",
      "[step: 329] loss: 20.74868392944336\n",
      "[step: 330] loss: 20.744935989379883\n",
      "[step: 331] loss: 20.7453556060791\n",
      "[step: 332] loss: 20.746824264526367\n",
      "[step: 333] loss: 20.74519157409668\n",
      "[step: 334] loss: 20.74212074279785\n",
      "[step: 335] loss: 20.741283416748047\n",
      "[step: 336] loss: 20.742122650146484\n",
      "[step: 337] loss: 20.741605758666992\n",
      "[step: 338] loss: 20.73941421508789\n",
      "[step: 339] loss: 20.73777198791504\n",
      "[step: 340] loss: 20.73769187927246\n",
      "[step: 341] loss: 20.73773193359375\n",
      "[step: 342] loss: 20.73659324645996\n",
      "[step: 343] loss: 20.7348690032959\n",
      "[step: 344] loss: 20.733816146850586\n",
      "[step: 345] loss: 20.733545303344727\n",
      "[step: 346] loss: 20.73316192626953\n",
      "[step: 347] loss: 20.732097625732422\n",
      "[step: 348] loss: 20.73072052001953\n",
      "[step: 349] loss: 20.729732513427734\n",
      "[step: 350] loss: 20.729204177856445\n",
      "[step: 351] loss: 20.728681564331055\n",
      "[step: 352] loss: 20.727815628051758\n",
      "[step: 353] loss: 20.7266788482666\n",
      "[step: 354] loss: 20.725629806518555\n",
      "[step: 355] loss: 20.724853515625\n",
      "[step: 356] loss: 20.72420883178711\n",
      "[step: 357] loss: 20.723506927490234\n",
      "[step: 358] loss: 20.722599029541016\n",
      "[step: 359] loss: 20.72159194946289\n",
      "[step: 360] loss: 20.720640182495117\n",
      "[step: 361] loss: 20.719785690307617\n",
      "[step: 362] loss: 20.719043731689453\n",
      "[step: 363] loss: 20.71829605102539\n",
      "[step: 364] loss: 20.71747398376465\n",
      "[step: 365] loss: 20.71656036376953\n",
      "[step: 366] loss: 20.71562385559082\n",
      "[step: 367] loss: 20.714704513549805\n",
      "[step: 368] loss: 20.713838577270508\n",
      "[step: 369] loss: 20.713008880615234\n",
      "[step: 370] loss: 20.712186813354492\n",
      "[step: 371] loss: 20.71137237548828\n",
      "[step: 372] loss: 20.71052360534668\n",
      "[step: 373] loss: 20.70966148376465\n",
      "[step: 374] loss: 20.70876121520996\n",
      "[step: 375] loss: 20.707870483398438\n",
      "[step: 376] loss: 20.706968307495117\n",
      "[step: 377] loss: 20.70607566833496\n",
      "[step: 378] loss: 20.705184936523438\n",
      "[step: 379] loss: 20.704303741455078\n",
      "[step: 380] loss: 20.703428268432617\n",
      "[step: 381] loss: 20.702533721923828\n",
      "[step: 382] loss: 20.7016658782959\n",
      "[step: 383] loss: 20.700796127319336\n",
      "[step: 384] loss: 20.699926376342773\n",
      "[step: 385] loss: 20.699052810668945\n",
      "[step: 386] loss: 20.698196411132812\n",
      "[step: 387] loss: 20.69736099243164\n",
      "[step: 388] loss: 20.696548461914062\n",
      "[step: 389] loss: 20.69580841064453\n",
      "[step: 390] loss: 20.6951847076416\n",
      "[step: 391] loss: 20.69481658935547\n",
      "[step: 392] loss: 20.6949462890625\n",
      "[step: 393] loss: 20.696155548095703\n",
      "[step: 394] loss: 20.699613571166992\n",
      "[step: 395] loss: 20.708227157592773\n",
      "[step: 396] loss: 20.727771759033203\n",
      "[step: 397] loss: 20.77372169494629\n",
      "[step: 398] loss: 20.872255325317383\n",
      "[step: 399] loss: 21.096303939819336\n",
      "[step: 400] loss: 21.47857093811035\n",
      "[step: 401] loss: 22.097244262695312\n",
      "[step: 402] loss: 22.273759841918945\n",
      "[step: 403] loss: 21.744882583618164\n",
      "[step: 404] loss: 20.81437873840332\n",
      "[step: 405] loss: 20.939708709716797\n",
      "[step: 406] loss: 21.475635528564453\n",
      "[step: 407] loss: 21.035751342773438\n",
      "[step: 408] loss: 20.72492790222168\n",
      "[step: 409] loss: 21.133827209472656\n",
      "[step: 410] loss: 21.000734329223633\n",
      "[step: 411] loss: 20.707406997680664\n",
      "[step: 412] loss: 20.971866607666016\n",
      "[step: 413] loss: 20.914066314697266\n",
      "[step: 414] loss: 20.701826095581055\n",
      "[step: 415] loss: 20.88580322265625\n",
      "[step: 416] loss: 20.81721305847168\n",
      "[step: 417] loss: 20.69977378845215\n",
      "[step: 418] loss: 20.83909797668457\n",
      "[step: 419] loss: 20.746644973754883\n",
      "[step: 420] loss: 20.70819854736328\n",
      "[step: 421] loss: 20.798603057861328\n",
      "[step: 422] loss: 20.703964233398438\n",
      "[step: 423] loss: 20.713050842285156\n",
      "[step: 424] loss: 20.763887405395508\n",
      "[step: 425] loss: 20.687644958496094\n",
      "[step: 426] loss: 20.714059829711914\n",
      "[step: 427] loss: 20.738075256347656\n",
      "[step: 428] loss: 20.680641174316406\n",
      "[step: 429] loss: 20.709144592285156\n",
      "[step: 430] loss: 20.716049194335938\n",
      "[step: 431] loss: 20.676448822021484\n",
      "[step: 432] loss: 20.703718185424805\n",
      "[step: 433] loss: 20.69929313659668\n",
      "[step: 434] loss: 20.674034118652344\n",
      "[step: 435] loss: 20.69660186767578\n",
      "[step: 436] loss: 20.685861587524414\n",
      "[step: 437] loss: 20.671640396118164\n",
      "[step: 438] loss: 20.688955307006836\n",
      "[step: 439] loss: 20.67700958251953\n",
      "[step: 440] loss: 20.670034408569336\n",
      "[step: 441] loss: 20.681995391845703\n",
      "[step: 442] loss: 20.670642852783203\n",
      "[step: 443] loss: 20.667816162109375\n",
      "[step: 444] loss: 20.675498962402344\n",
      "[step: 445] loss: 20.665910720825195\n",
      "[step: 446] loss: 20.66557502746582\n",
      "[step: 447] loss: 20.670061111450195\n",
      "[step: 448] loss: 20.66222381591797\n",
      "[step: 449] loss: 20.662691116333008\n",
      "[step: 450] loss: 20.66510581970215\n",
      "[step: 451] loss: 20.658985137939453\n",
      "[step: 452] loss: 20.659494400024414\n",
      "[step: 453] loss: 20.660947799682617\n",
      "[step: 454] loss: 20.65620994567871\n",
      "[step: 455] loss: 20.656253814697266\n",
      "[step: 456] loss: 20.65721321105957\n",
      "[step: 457] loss: 20.653541564941406\n",
      "[step: 458] loss: 20.653060913085938\n",
      "[step: 459] loss: 20.653831481933594\n",
      "[step: 460] loss: 20.65106773376465\n",
      "[step: 461] loss: 20.650089263916016\n",
      "[step: 462] loss: 20.65062141418457\n",
      "[step: 463] loss: 20.648582458496094\n",
      "[step: 464] loss: 20.647205352783203\n",
      "[step: 465] loss: 20.647483825683594\n",
      "[step: 466] loss: 20.64613151550293\n",
      "[step: 467] loss: 20.644533157348633\n",
      "[step: 468] loss: 20.644437789916992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 469] loss: 20.64361000061035\n",
      "[step: 470] loss: 20.642026901245117\n",
      "[step: 471] loss: 20.641489028930664\n",
      "[step: 472] loss: 20.640995025634766\n",
      "[step: 473] loss: 20.639650344848633\n",
      "[step: 474] loss: 20.63874053955078\n",
      "[step: 475] loss: 20.638301849365234\n",
      "[step: 476] loss: 20.637277603149414\n",
      "[step: 477] loss: 20.63615608215332\n",
      "[step: 478] loss: 20.635570526123047\n",
      "[step: 479] loss: 20.63483238220215\n",
      "[step: 480] loss: 20.633747100830078\n",
      "[step: 481] loss: 20.632911682128906\n",
      "[step: 482] loss: 20.63228416442871\n",
      "[step: 483] loss: 20.63137435913086\n",
      "[step: 484] loss: 20.630393981933594\n",
      "[step: 485] loss: 20.62967872619629\n",
      "[step: 486] loss: 20.62894058227539\n",
      "[step: 487] loss: 20.628013610839844\n",
      "[step: 488] loss: 20.627145767211914\n",
      "[step: 489] loss: 20.626432418823242\n",
      "[step: 490] loss: 20.625642776489258\n",
      "[step: 491] loss: 20.62474250793457\n",
      "[step: 492] loss: 20.623931884765625\n",
      "[step: 493] loss: 20.62318992614746\n",
      "[step: 494] loss: 20.622394561767578\n",
      "[step: 495] loss: 20.621538162231445\n",
      "[step: 496] loss: 20.620737075805664\n",
      "[step: 497] loss: 20.61998748779297\n",
      "[step: 498] loss: 20.619197845458984\n",
      "[step: 499] loss: 20.618370056152344\n",
      "[step: 500] loss: 20.617578506469727\n",
      "[step: 501] loss: 20.616817474365234\n",
      "[step: 502] loss: 20.61603355407715\n",
      "[step: 503] loss: 20.6152286529541\n",
      "[step: 504] loss: 20.614442825317383\n",
      "[step: 505] loss: 20.61368751525879\n",
      "[step: 506] loss: 20.612918853759766\n",
      "[step: 507] loss: 20.612133026123047\n",
      "[step: 508] loss: 20.611352920532227\n",
      "[step: 509] loss: 20.610584259033203\n",
      "[step: 510] loss: 20.60982894897461\n",
      "[step: 511] loss: 20.609066009521484\n",
      "[step: 512] loss: 20.608291625976562\n",
      "[step: 513] loss: 20.6075382232666\n",
      "[step: 514] loss: 20.606782913208008\n",
      "[step: 515] loss: 20.606040954589844\n",
      "[step: 516] loss: 20.60527992248535\n",
      "[step: 517] loss: 20.604524612426758\n",
      "[step: 518] loss: 20.60378074645996\n",
      "[step: 519] loss: 20.603036880493164\n",
      "[step: 520] loss: 20.602294921875\n",
      "[step: 521] loss: 20.6015625\n",
      "[step: 522] loss: 20.600818634033203\n",
      "[step: 523] loss: 20.600078582763672\n",
      "[step: 524] loss: 20.599340438842773\n",
      "[step: 525] loss: 20.59861946105957\n",
      "[step: 526] loss: 20.597883224487305\n",
      "[step: 527] loss: 20.59715461730957\n",
      "[step: 528] loss: 20.596435546875\n",
      "[step: 529] loss: 20.595714569091797\n",
      "[step: 530] loss: 20.594993591308594\n",
      "[step: 531] loss: 20.594280242919922\n",
      "[step: 532] loss: 20.593564987182617\n",
      "[step: 533] loss: 20.592851638793945\n",
      "[step: 534] loss: 20.592145919799805\n",
      "[step: 535] loss: 20.59144401550293\n",
      "[step: 536] loss: 20.590734481811523\n",
      "[step: 537] loss: 20.59003257751465\n",
      "[step: 538] loss: 20.589326858520508\n",
      "[step: 539] loss: 20.588638305664062\n",
      "[step: 540] loss: 20.587940216064453\n",
      "[step: 541] loss: 20.587244033813477\n",
      "[step: 542] loss: 20.586563110351562\n",
      "[step: 543] loss: 20.585859298706055\n",
      "[step: 544] loss: 20.585180282592773\n",
      "[step: 545] loss: 20.58449935913086\n",
      "[step: 546] loss: 20.583816528320312\n",
      "[step: 547] loss: 20.583145141601562\n",
      "[step: 548] loss: 20.582462310791016\n",
      "[step: 549] loss: 20.581787109375\n",
      "[step: 550] loss: 20.581106185913086\n",
      "[step: 551] loss: 20.580440521240234\n",
      "[step: 552] loss: 20.57977867126465\n",
      "[step: 553] loss: 20.579103469848633\n",
      "[step: 554] loss: 20.57843589782715\n",
      "[step: 555] loss: 20.57777214050293\n",
      "[step: 556] loss: 20.577125549316406\n",
      "[step: 557] loss: 20.576461791992188\n",
      "[step: 558] loss: 20.575809478759766\n",
      "[step: 559] loss: 20.575157165527344\n",
      "[step: 560] loss: 20.57450294494629\n",
      "[step: 561] loss: 20.573854446411133\n",
      "[step: 562] loss: 20.573209762573242\n",
      "[step: 563] loss: 20.572561264038086\n",
      "[step: 564] loss: 20.57192039489746\n",
      "[step: 565] loss: 20.571277618408203\n",
      "[step: 566] loss: 20.570648193359375\n",
      "[step: 567] loss: 20.57000732421875\n",
      "[step: 568] loss: 20.569368362426758\n",
      "[step: 569] loss: 20.56873893737793\n",
      "[step: 570] loss: 20.56810760498047\n",
      "[step: 571] loss: 20.5674991607666\n",
      "[step: 572] loss: 20.566892623901367\n",
      "[step: 573] loss: 20.566301345825195\n",
      "[step: 574] loss: 20.565763473510742\n",
      "[step: 575] loss: 20.565326690673828\n",
      "[step: 576] loss: 20.565101623535156\n",
      "[step: 577] loss: 20.565366744995117\n",
      "[step: 578] loss: 20.566709518432617\n",
      "[step: 579] loss: 20.570615768432617\n",
      "[step: 580] loss: 20.580495834350586\n",
      "[step: 581] loss: 20.604856491088867\n",
      "[step: 582] loss: 20.663536071777344\n",
      "[step: 583] loss: 20.805639266967773\n",
      "[step: 584] loss: 21.13077163696289\n",
      "[step: 585] loss: 21.827394485473633\n",
      "[step: 586] loss: 22.92165756225586\n",
      "[step: 587] loss: 23.70857810974121\n",
      "[step: 588] loss: 22.62144660949707\n",
      "[step: 589] loss: 20.73929214477539\n",
      "[step: 590] loss: 21.405378341674805\n",
      "[step: 591] loss: 21.924365997314453\n",
      "[step: 592] loss: 20.688697814941406\n",
      "[step: 593] loss: 21.32450294494629\n",
      "[step: 594] loss: 21.30453109741211\n",
      "[step: 595] loss: 20.683656692504883\n",
      "[step: 596] loss: 21.35689353942871\n",
      "[step: 597] loss: 20.789690017700195\n",
      "[step: 598] loss: 20.909460067749023\n",
      "[step: 599] loss: 20.996427536010742\n",
      "[step: 600] loss: 20.636173248291016\n",
      "[step: 601] loss: 20.985506057739258\n",
      "[step: 602] loss: 20.616901397705078\n",
      "[step: 603] loss: 20.838144302368164\n",
      "[step: 604] loss: 20.697681427001953\n",
      "[step: 605] loss: 20.667680740356445\n",
      "[step: 606] loss: 20.770492553710938\n",
      "[step: 607] loss: 20.579221725463867\n",
      "[step: 608] loss: 20.76321029663086\n",
      "[step: 609] loss: 20.604598999023438\n",
      "[step: 610] loss: 20.675256729125977\n",
      "[step: 611] loss: 20.66541290283203\n",
      "[step: 612] loss: 20.597288131713867\n",
      "[step: 613] loss: 20.684783935546875\n",
      "[step: 614] loss: 20.578641891479492\n",
      "[step: 615] loss: 20.65481948852539\n",
      "[step: 616] loss: 20.59359359741211\n",
      "[step: 617] loss: 20.60663414001465\n",
      "[step: 618] loss: 20.610681533813477\n",
      "[step: 619] loss: 20.57305145263672\n",
      "[step: 620] loss: 20.614120483398438\n",
      "[step: 621] loss: 20.563940048217773\n",
      "[step: 622] loss: 20.600645065307617\n",
      "[step: 623] loss: 20.569608688354492\n",
      "[step: 624] loss: 20.580596923828125\n",
      "[step: 625] loss: 20.578100204467773\n",
      "[step: 626] loss: 20.565876007080078\n",
      "[step: 627] loss: 20.581356048583984\n",
      "[step: 628] loss: 20.558713912963867\n",
      "[step: 629] loss: 20.576398849487305\n",
      "[step: 630] loss: 20.55752182006836\n",
      "[step: 631] loss: 20.566438674926758\n",
      "[step: 632] loss: 20.56001091003418\n",
      "[step: 633] loss: 20.556547164916992\n",
      "[step: 634] loss: 20.56148338317871\n",
      "[step: 635] loss: 20.550390243530273\n",
      "[step: 636] loss: 20.55876922607422\n",
      "[step: 637] loss: 20.549386978149414\n",
      "[step: 638] loss: 20.553203582763672\n",
      "[step: 639] loss: 20.551145553588867\n",
      "[step: 640] loss: 20.54794692993164\n",
      "[step: 641] loss: 20.55145835876465\n",
      "[step: 642] loss: 20.545204162597656\n",
      "[step: 643] loss: 20.548776626586914\n",
      "[step: 644] loss: 20.545074462890625\n",
      "[step: 645] loss: 20.54473876953125\n",
      "[step: 646] loss: 20.54522132873535\n",
      "[step: 647] loss: 20.541669845581055\n",
      "[step: 648] loss: 20.54362678527832\n",
      "[step: 649] loss: 20.540504455566406\n",
      "[step: 650] loss: 20.54079818725586\n",
      "[step: 651] loss: 20.540212631225586\n",
      "[step: 652] loss: 20.538345336914062\n",
      "[step: 653] loss: 20.53923797607422\n",
      "[step: 654] loss: 20.537025451660156\n",
      "[step: 655] loss: 20.537288665771484\n",
      "[step: 656] loss: 20.53640365600586\n",
      "[step: 657] loss: 20.53527069091797\n",
      "[step: 658] loss: 20.53547477722168\n",
      "[step: 659] loss: 20.533876419067383\n",
      "[step: 660] loss: 20.533838272094727\n",
      "[step: 661] loss: 20.53295135498047\n",
      "[step: 662] loss: 20.532052993774414\n",
      "[step: 663] loss: 20.531911849975586\n",
      "[step: 664] loss: 20.530723571777344\n",
      "[step: 665] loss: 20.530471801757812\n",
      "[step: 666] loss: 20.529741287231445\n",
      "[step: 667] loss: 20.52895736694336\n",
      "[step: 668] loss: 20.52866554260254\n",
      "[step: 669] loss: 20.527746200561523\n",
      "[step: 670] loss: 20.527332305908203\n",
      "[step: 671] loss: 20.526729583740234\n",
      "[step: 672] loss: 20.52597999572754\n",
      "[step: 673] loss: 20.52559471130371\n",
      "[step: 674] loss: 20.52483367919922\n",
      "[step: 675] loss: 20.524293899536133\n",
      "[step: 676] loss: 20.52377700805664\n",
      "[step: 677] loss: 20.523067474365234\n",
      "[step: 678] loss: 20.522613525390625\n",
      "[step: 679] loss: 20.52198028564453\n",
      "[step: 680] loss: 20.521398544311523\n",
      "[step: 681] loss: 20.520906448364258\n",
      "[step: 682] loss: 20.520259857177734\n",
      "[step: 683] loss: 20.51976203918457\n",
      "[step: 684] loss: 20.519201278686523\n",
      "[step: 685] loss: 20.518613815307617\n",
      "[step: 686] loss: 20.51812744140625\n",
      "[step: 687] loss: 20.517532348632812\n",
      "[step: 688] loss: 20.51700210571289\n",
      "[step: 689] loss: 20.516477584838867\n",
      "[step: 690] loss: 20.515914916992188\n",
      "[step: 691] loss: 20.51540756225586\n",
      "[step: 692] loss: 20.514875411987305\n",
      "[step: 693] loss: 20.514324188232422\n",
      "[step: 694] loss: 20.513824462890625\n",
      "[step: 695] loss: 20.513280868530273\n",
      "[step: 696] loss: 20.512773513793945\n",
      "[step: 697] loss: 20.51226806640625\n",
      "[step: 698] loss: 20.51174545288086\n",
      "[step: 699] loss: 20.5112361907959\n",
      "[step: 700] loss: 20.51073455810547\n",
      "[step: 701] loss: 20.510215759277344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 702] loss: 20.50973129272461\n",
      "[step: 703] loss: 20.50922966003418\n",
      "[step: 704] loss: 20.50872230529785\n",
      "[step: 705] loss: 20.50823211669922\n",
      "[step: 706] loss: 20.50773811340332\n",
      "[step: 707] loss: 20.507244110107422\n",
      "[step: 708] loss: 20.50676727294922\n",
      "[step: 709] loss: 20.506271362304688\n",
      "[step: 710] loss: 20.505788803100586\n",
      "[step: 711] loss: 20.50531005859375\n",
      "[step: 712] loss: 20.504837036132812\n",
      "[step: 713] loss: 20.504356384277344\n",
      "[step: 714] loss: 20.503881454467773\n",
      "[step: 715] loss: 20.50340461730957\n",
      "[step: 716] loss: 20.50293731689453\n",
      "[step: 717] loss: 20.50246810913086\n",
      "[step: 718] loss: 20.50200843811035\n",
      "[step: 719] loss: 20.501548767089844\n",
      "[step: 720] loss: 20.501081466674805\n",
      "[step: 721] loss: 20.500621795654297\n",
      "[step: 722] loss: 20.500167846679688\n",
      "[step: 723] loss: 20.49972152709961\n",
      "[step: 724] loss: 20.4992618560791\n",
      "[step: 725] loss: 20.498817443847656\n",
      "[step: 726] loss: 20.49837303161621\n",
      "[step: 727] loss: 20.4979248046875\n",
      "[step: 728] loss: 20.497478485107422\n",
      "[step: 729] loss: 20.497039794921875\n",
      "[step: 730] loss: 20.496604919433594\n",
      "[step: 731] loss: 20.49616241455078\n",
      "[step: 732] loss: 20.4957275390625\n",
      "[step: 733] loss: 20.495296478271484\n",
      "[step: 734] loss: 20.49486541748047\n",
      "[step: 735] loss: 20.494430541992188\n",
      "[step: 736] loss: 20.494003295898438\n",
      "[step: 737] loss: 20.493568420410156\n",
      "[step: 738] loss: 20.49314308166504\n",
      "[step: 739] loss: 20.49272346496582\n",
      "[step: 740] loss: 20.4923152923584\n",
      "[step: 741] loss: 20.49188804626465\n",
      "[step: 742] loss: 20.491470336914062\n",
      "[step: 743] loss: 20.491058349609375\n",
      "[step: 744] loss: 20.490650177001953\n",
      "[step: 745] loss: 20.490234375\n",
      "[step: 746] loss: 20.48981285095215\n",
      "[step: 747] loss: 20.489398956298828\n",
      "[step: 748] loss: 20.489004135131836\n",
      "[step: 749] loss: 20.488597869873047\n",
      "[step: 750] loss: 20.48819351196289\n",
      "[step: 751] loss: 20.487794876098633\n",
      "[step: 752] loss: 20.487390518188477\n",
      "[step: 753] loss: 20.48699951171875\n",
      "[step: 754] loss: 20.486600875854492\n",
      "[step: 755] loss: 20.486202239990234\n",
      "[step: 756] loss: 20.485815048217773\n",
      "[step: 757] loss: 20.48542022705078\n",
      "[step: 758] loss: 20.485027313232422\n",
      "[step: 759] loss: 20.48463249206543\n",
      "[step: 760] loss: 20.4842472076416\n",
      "[step: 761] loss: 20.483869552612305\n",
      "[step: 762] loss: 20.483470916748047\n",
      "[step: 763] loss: 20.483095169067383\n",
      "[step: 764] loss: 20.48270606994629\n",
      "[step: 765] loss: 20.482332229614258\n",
      "[step: 766] loss: 20.48195457458496\n",
      "[step: 767] loss: 20.4815673828125\n",
      "[step: 768] loss: 20.481189727783203\n",
      "[step: 769] loss: 20.480810165405273\n",
      "[step: 770] loss: 20.480440139770508\n",
      "[step: 771] loss: 20.480073928833008\n",
      "[step: 772] loss: 20.479694366455078\n",
      "[step: 773] loss: 20.479328155517578\n",
      "[step: 774] loss: 20.478960037231445\n",
      "[step: 775] loss: 20.47859001159668\n",
      "[step: 776] loss: 20.478219985961914\n",
      "[step: 777] loss: 20.477861404418945\n",
      "[step: 778] loss: 20.47749900817871\n",
      "[step: 779] loss: 20.477128982543945\n",
      "[step: 780] loss: 20.476768493652344\n",
      "[step: 781] loss: 20.476408004760742\n",
      "[step: 782] loss: 20.476051330566406\n",
      "[step: 783] loss: 20.475692749023438\n",
      "[step: 784] loss: 20.475326538085938\n",
      "[step: 785] loss: 20.474973678588867\n",
      "[step: 786] loss: 20.47462272644043\n",
      "[step: 787] loss: 20.47426414489746\n",
      "[step: 788] loss: 20.473913192749023\n",
      "[step: 789] loss: 20.47355079650879\n",
      "[step: 790] loss: 20.473203659057617\n",
      "[step: 791] loss: 20.472848892211914\n",
      "[step: 792] loss: 20.472501754760742\n",
      "[step: 793] loss: 20.47215461730957\n",
      "[step: 794] loss: 20.471803665161133\n",
      "[step: 795] loss: 20.471464157104492\n",
      "[step: 796] loss: 20.471118927001953\n",
      "[step: 797] loss: 20.47076416015625\n",
      "[step: 798] loss: 20.470430374145508\n",
      "[step: 799] loss: 20.470081329345703\n",
      "[step: 800] loss: 20.46973991394043\n",
      "[step: 801] loss: 20.469404220581055\n",
      "[step: 802] loss: 20.469053268432617\n",
      "[step: 803] loss: 20.46872329711914\n",
      "[step: 804] loss: 20.468385696411133\n",
      "[step: 805] loss: 20.468032836914062\n",
      "[step: 806] loss: 20.467700958251953\n",
      "[step: 807] loss: 20.467361450195312\n",
      "[step: 808] loss: 20.467021942138672\n",
      "[step: 809] loss: 20.466691970825195\n",
      "[step: 810] loss: 20.466360092163086\n",
      "[step: 811] loss: 20.46602439880371\n",
      "[step: 812] loss: 20.46569061279297\n",
      "[step: 813] loss: 20.465360641479492\n",
      "[step: 814] loss: 20.46502113342285\n",
      "[step: 815] loss: 20.464693069458008\n",
      "[step: 816] loss: 20.464366912841797\n",
      "[step: 817] loss: 20.464033126831055\n",
      "[step: 818] loss: 20.463703155517578\n",
      "[step: 819] loss: 20.46337127685547\n",
      "[step: 820] loss: 20.463045120239258\n",
      "[step: 821] loss: 20.462724685668945\n",
      "[step: 822] loss: 20.462392807006836\n",
      "[step: 823] loss: 20.462060928344727\n",
      "[step: 824] loss: 20.461740493774414\n",
      "[step: 825] loss: 20.461406707763672\n",
      "[step: 826] loss: 20.461084365844727\n",
      "[step: 827] loss: 20.460758209228516\n",
      "[step: 828] loss: 20.460437774658203\n",
      "[step: 829] loss: 20.460115432739258\n",
      "[step: 830] loss: 20.459789276123047\n",
      "[step: 831] loss: 20.45946502685547\n",
      "[step: 832] loss: 20.45914649963379\n",
      "[step: 833] loss: 20.458829879760742\n",
      "[step: 834] loss: 20.45850944519043\n",
      "[step: 835] loss: 20.458187103271484\n",
      "[step: 836] loss: 20.457857131958008\n",
      "[step: 837] loss: 20.457538604736328\n",
      "[step: 838] loss: 20.45722198486328\n",
      "[step: 839] loss: 20.456905364990234\n",
      "[step: 840] loss: 20.456588745117188\n",
      "[step: 841] loss: 20.45626449584961\n",
      "[step: 842] loss: 20.455947875976562\n",
      "[step: 843] loss: 20.455629348754883\n",
      "[step: 844] loss: 20.455310821533203\n",
      "[step: 845] loss: 20.454992294311523\n",
      "[step: 846] loss: 20.45467185974121\n",
      "[step: 847] loss: 20.454355239868164\n",
      "[step: 848] loss: 20.454036712646484\n",
      "[step: 849] loss: 20.45372200012207\n",
      "[step: 850] loss: 20.453399658203125\n",
      "[step: 851] loss: 20.45309066772461\n",
      "[step: 852] loss: 20.452775955200195\n",
      "[step: 853] loss: 20.45245933532715\n",
      "[step: 854] loss: 20.452136993408203\n",
      "[step: 855] loss: 20.45182228088379\n",
      "[step: 856] loss: 20.451507568359375\n",
      "[step: 857] loss: 20.451196670532227\n",
      "[step: 858] loss: 20.450878143310547\n",
      "[step: 859] loss: 20.450572967529297\n",
      "[step: 860] loss: 20.45025634765625\n",
      "[step: 861] loss: 20.449934005737305\n",
      "[step: 862] loss: 20.449621200561523\n",
      "[step: 863] loss: 20.44930648803711\n",
      "[step: 864] loss: 20.448993682861328\n",
      "[step: 865] loss: 20.448678970336914\n",
      "[step: 866] loss: 20.448362350463867\n",
      "[step: 867] loss: 20.44805335998535\n",
      "[step: 868] loss: 20.447738647460938\n",
      "[step: 869] loss: 20.447416305541992\n",
      "[step: 870] loss: 20.447107315063477\n",
      "[step: 871] loss: 20.44678497314453\n",
      "[step: 872] loss: 20.446483612060547\n",
      "[step: 873] loss: 20.44617462158203\n",
      "[step: 874] loss: 20.44584846496582\n",
      "[step: 875] loss: 20.44552993774414\n",
      "[step: 876] loss: 20.445220947265625\n",
      "[step: 877] loss: 20.444904327392578\n",
      "[step: 878] loss: 20.444599151611328\n",
      "[step: 879] loss: 20.44428253173828\n",
      "[step: 880] loss: 20.443965911865234\n",
      "[step: 881] loss: 20.443645477294922\n",
      "[step: 882] loss: 20.443336486816406\n",
      "[step: 883] loss: 20.443021774291992\n",
      "[step: 884] loss: 20.44270133972168\n",
      "[step: 885] loss: 20.4423885345459\n",
      "[step: 886] loss: 20.442075729370117\n",
      "[step: 887] loss: 20.44175910949707\n",
      "[step: 888] loss: 20.441444396972656\n",
      "[step: 889] loss: 20.441131591796875\n",
      "[step: 890] loss: 20.44081687927246\n",
      "[step: 891] loss: 20.440494537353516\n",
      "[step: 892] loss: 20.440181732177734\n",
      "[step: 893] loss: 20.43986701965332\n",
      "[step: 894] loss: 20.439544677734375\n",
      "[step: 895] loss: 20.439220428466797\n",
      "[step: 896] loss: 20.438913345336914\n",
      "[step: 897] loss: 20.438587188720703\n",
      "[step: 898] loss: 20.438274383544922\n",
      "[step: 899] loss: 20.437959671020508\n",
      "[step: 900] loss: 20.43764305114746\n",
      "[step: 901] loss: 20.43731689453125\n",
      "[step: 902] loss: 20.437000274658203\n",
      "[step: 903] loss: 20.436677932739258\n",
      "[step: 904] loss: 20.436363220214844\n",
      "[step: 905] loss: 20.43603515625\n",
      "[step: 906] loss: 20.43572425842285\n",
      "[step: 907] loss: 20.435407638549805\n",
      "[step: 908] loss: 20.435081481933594\n",
      "[step: 909] loss: 20.43475914001465\n",
      "[step: 910] loss: 20.434438705444336\n",
      "[step: 911] loss: 20.434118270874023\n",
      "[step: 912] loss: 20.433792114257812\n",
      "[step: 913] loss: 20.4334716796875\n",
      "[step: 914] loss: 20.433135986328125\n",
      "[step: 915] loss: 20.432823181152344\n",
      "[step: 916] loss: 20.43250274658203\n",
      "[step: 917] loss: 20.43217658996582\n",
      "[step: 918] loss: 20.43184471130371\n",
      "[step: 919] loss: 20.43152618408203\n",
      "[step: 920] loss: 20.431194305419922\n",
      "[step: 921] loss: 20.430870056152344\n",
      "[step: 922] loss: 20.430538177490234\n",
      "[step: 923] loss: 20.430217742919922\n",
      "[step: 924] loss: 20.429895401000977\n",
      "[step: 925] loss: 20.429563522338867\n",
      "[step: 926] loss: 20.42923355102539\n",
      "[step: 927] loss: 20.42889404296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 928] loss: 20.428571701049805\n",
      "[step: 929] loss: 20.428239822387695\n",
      "[step: 930] loss: 20.427913665771484\n",
      "[step: 931] loss: 20.42758560180664\n",
      "[step: 932] loss: 20.427249908447266\n",
      "[step: 933] loss: 20.426923751831055\n",
      "[step: 934] loss: 20.42658233642578\n",
      "[step: 935] loss: 20.42624855041504\n",
      "[step: 936] loss: 20.425920486450195\n",
      "[step: 937] loss: 20.425579071044922\n",
      "[step: 938] loss: 20.425241470336914\n",
      "[step: 939] loss: 20.424911499023438\n",
      "[step: 940] loss: 20.42457389831543\n",
      "[step: 941] loss: 20.424240112304688\n",
      "[step: 942] loss: 20.42390251159668\n",
      "[step: 943] loss: 20.42356300354004\n",
      "[step: 944] loss: 20.42322540283203\n",
      "[step: 945] loss: 20.422883987426758\n",
      "[step: 946] loss: 20.422544479370117\n",
      "[step: 947] loss: 20.422197341918945\n",
      "[step: 948] loss: 20.421857833862305\n",
      "[step: 949] loss: 20.421506881713867\n",
      "[step: 950] loss: 20.421165466308594\n",
      "[step: 951] loss: 20.420833587646484\n",
      "[step: 952] loss: 20.420486450195312\n",
      "[step: 953] loss: 20.420135498046875\n",
      "[step: 954] loss: 20.419795989990234\n",
      "[step: 955] loss: 20.419448852539062\n",
      "[step: 956] loss: 20.41909408569336\n",
      "[step: 957] loss: 20.418752670288086\n",
      "[step: 958] loss: 20.418399810791016\n",
      "[step: 959] loss: 20.41805648803711\n",
      "[step: 960] loss: 20.41770362854004\n",
      "[step: 961] loss: 20.417356491088867\n",
      "[step: 962] loss: 20.417003631591797\n",
      "[step: 963] loss: 20.416645050048828\n",
      "[step: 964] loss: 20.416297912597656\n",
      "[step: 965] loss: 20.415943145751953\n",
      "[step: 966] loss: 20.41558837890625\n",
      "[step: 967] loss: 20.41522979736328\n",
      "[step: 968] loss: 20.414875030517578\n",
      "[step: 969] loss: 20.414525985717773\n",
      "[step: 970] loss: 20.414155960083008\n",
      "[step: 971] loss: 20.413806915283203\n",
      "[step: 972] loss: 20.41344451904297\n",
      "[step: 973] loss: 20.4130802154541\n",
      "[step: 974] loss: 20.4127254486084\n",
      "[step: 975] loss: 20.412363052368164\n",
      "[step: 976] loss: 20.411996841430664\n",
      "[step: 977] loss: 20.411640167236328\n",
      "[step: 978] loss: 20.41128158569336\n",
      "[step: 979] loss: 20.410932540893555\n",
      "[step: 980] loss: 20.41057014465332\n",
      "[step: 981] loss: 20.410234451293945\n",
      "[step: 982] loss: 20.409929275512695\n",
      "[step: 983] loss: 20.409687042236328\n",
      "[step: 984] loss: 20.409584045410156\n",
      "[step: 985] loss: 20.409751892089844\n",
      "[step: 986] loss: 20.410541534423828\n",
      "[step: 987] loss: 20.412683486938477\n",
      "[step: 988] loss: 20.417768478393555\n",
      "[step: 989] loss: 20.429607391357422\n",
      "[step: 990] loss: 20.455928802490234\n",
      "[step: 991] loss: 20.517139434814453\n",
      "[step: 992] loss: 20.647207260131836\n",
      "[step: 993] loss: 20.945419311523438\n",
      "[step: 994] loss: 21.460235595703125\n",
      "[step: 995] loss: 22.396743774414062\n",
      "[step: 996] loss: 22.742467880249023\n",
      "[step: 997] loss: 22.260772705078125\n",
      "[step: 998] loss: 20.677303314208984\n",
      "[step: 999] loss: 20.893476486206055\n",
      "[step: 1000] loss: 21.689653396606445\n",
      "RMSE: 2.3818035581373747\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fde77a8f470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "def predict(file):\n",
    "    # train Parameters\n",
    "    seq_length = 6\n",
    "    data_dim = 1\n",
    "    hidden_dim = 12 #    \n",
    "    output_dim = 1\n",
    "    learning_rate = 0.05\n",
    "    iterations = 1000\n",
    "    layer_num=2\n",
    "\n",
    "    # train Data: Open, High, Low, Volume, Close\n",
    "    origin_xy = np.loadtxt('train.csv', delimiter=',', skiprows=1, usecols=range(1,8))\n",
    "    xy = MinMaxScaler(origin_xy)\n",
    "    x = xy[:,:-1]\n",
    "    y = xy[:,-1]\n",
    "    x = x.reshape(-1,seq_length,data_dim)\n",
    "    y = y.reshape(-1,data_dim) \n",
    "\n",
    "    # train/validation split\n",
    "    train_size = int(len(y) * 0.7)\n",
    "    test_size = len(y) - train_size\n",
    "    trainX, validX = np.array(x[0:train_size]), np.array(x[train_size:])\n",
    "    trainY, validY = np.array(y[0:train_size]), np.array(y[train_size:])\n",
    "\n",
    "    #test data\n",
    "    test_x=np.loadtxt(file, delimiter=',', skiprows=1, usecols=range(1,7))\n",
    "    test_x = test_x.reshape(-1,seq_length, data_dim)\n",
    "    \n",
    "    # input place holders\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # build a LSTM network\n",
    "    cells=[]\n",
    "    for _ in range(layer_num):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "        #cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.7)\n",
    "        cells.append(cell)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "    # cost/loss\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # RMSE\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "    sess=tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i+1, step_loss))\n",
    "\n",
    "    # validation step\n",
    "    valid_predict = sess.run(Y_pred, feed_dict={X: validX})\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validY, predictions: valid_predict})\n",
    "    revised_rmse = rmse_val*(origin_xy.max()-origin_xy.min()+1e-7) + origin_xy.min()\n",
    "    print(\"RMSE: {}\".format(revised_rmse))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(validY)\n",
    "    plt.plot(valid_predict)\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Temperature\")\n",
    "    plt.show()\n",
    "    \n",
    "    #test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: test_x})\n",
    "    prediction_list = []\n",
    "    for i in test_predict:\n",
    "        revised_pred = i[0]*(origin_xy.max()-origin_xy.min()+1e-7) + origin_xy.min()\n",
    "        prediction_list.append(revised_pred)\n",
    "    return prediction_list\n",
    "\n",
    "def write_result(predictions):\n",
    "    # You don't need to modify this function.\n",
    "    with open('result.csv', 'w') as f:\n",
    "        f.write('Value\\n')\n",
    "        for l in predictions:\n",
    "            f.write('{}\\n'.format(l))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # You don't need to modify this function.\n",
    "    predictions = predict('test.csv')\n",
    "    write_result(predictions)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # You don't need to modify this part.\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
