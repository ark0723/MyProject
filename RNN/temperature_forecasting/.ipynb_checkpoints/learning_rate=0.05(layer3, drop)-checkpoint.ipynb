{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to add any functions, import statements, and variables.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1] loss: 432.4604797363281\n",
      "[step: 2] loss: 507.198486328125\n",
      "[step: 3] loss: 195.63681030273438\n",
      "[step: 4] loss: 224.2957763671875\n",
      "[step: 5] loss: 137.4382781982422\n",
      "[step: 6] loss: 72.07099914550781\n",
      "[step: 7] loss: 98.22661590576172\n",
      "[step: 8] loss: 112.47259521484375\n",
      "[step: 9] loss: 80.27091979980469\n",
      "[step: 10] loss: 62.49224853515625\n",
      "[step: 11] loss: 71.85052490234375\n",
      "[step: 12] loss: 81.52843475341797\n",
      "[step: 13] loss: 78.30508422851562\n",
      "[step: 14] loss: 69.92283630371094\n",
      "[step: 15] loss: 60.49073028564453\n",
      "[step: 16] loss: 58.68761444091797\n",
      "[step: 17] loss: 60.00080108642578\n",
      "[step: 18] loss: 61.590309143066406\n",
      "[step: 19] loss: 63.89303207397461\n",
      "[step: 20] loss: 63.70114517211914\n",
      "[step: 21] loss: 62.92230987548828\n",
      "[step: 22] loss: 61.09193420410156\n",
      "[step: 23] loss: 60.218467712402344\n",
      "[step: 24] loss: 57.76605224609375\n",
      "[step: 25] loss: 57.24220275878906\n",
      "[step: 26] loss: 57.36215591430664\n",
      "[step: 27] loss: 57.72726821899414\n",
      "[step: 28] loss: 58.067474365234375\n",
      "[step: 29] loss: 58.88174057006836\n",
      "[step: 30] loss: 58.4165153503418\n",
      "[step: 31] loss: 58.28258514404297\n",
      "[step: 32] loss: 56.96497344970703\n",
      "[step: 33] loss: 57.79704666137695\n",
      "[step: 34] loss: 56.671146392822266\n",
      "[step: 35] loss: 56.33600616455078\n",
      "[step: 36] loss: 56.15873718261719\n",
      "[step: 37] loss: 56.046112060546875\n",
      "[step: 38] loss: 56.09756088256836\n",
      "[step: 39] loss: 55.68357849121094\n",
      "[step: 40] loss: 56.06873321533203\n",
      "[step: 41] loss: 56.2701301574707\n",
      "[step: 42] loss: 55.820457458496094\n",
      "[step: 43] loss: 54.98788070678711\n",
      "[step: 44] loss: 54.49399185180664\n",
      "[step: 45] loss: 55.068153381347656\n",
      "[step: 46] loss: 53.713375091552734\n",
      "[step: 47] loss: 53.61825180053711\n",
      "[step: 48] loss: 53.31926727294922\n",
      "[step: 49] loss: 53.03261184692383\n",
      "[step: 50] loss: 53.009586334228516\n",
      "[step: 51] loss: 51.63102722167969\n",
      "[step: 52] loss: 50.262516021728516\n",
      "[step: 53] loss: 49.40029525756836\n",
      "[step: 54] loss: 48.53166580200195\n",
      "[step: 55] loss: 47.73619842529297\n",
      "[step: 56] loss: 45.06511306762695\n",
      "[step: 57] loss: 44.17151641845703\n",
      "[step: 58] loss: 41.78413772583008\n",
      "[step: 59] loss: 39.9680290222168\n",
      "[step: 60] loss: 38.201263427734375\n",
      "[step: 61] loss: 37.58832550048828\n",
      "[step: 62] loss: 38.06410217285156\n",
      "[step: 63] loss: 38.13174819946289\n",
      "[step: 64] loss: 38.45372009277344\n",
      "[step: 65] loss: 37.807167053222656\n",
      "[step: 66] loss: 37.460147857666016\n",
      "[step: 67] loss: 34.06575393676758\n",
      "[step: 68] loss: 34.786537170410156\n",
      "[step: 69] loss: 34.07564926147461\n",
      "[step: 70] loss: 34.49717712402344\n",
      "[step: 71] loss: 34.06959533691406\n",
      "[step: 72] loss: 33.5207633972168\n",
      "[step: 73] loss: 33.40370178222656\n",
      "[step: 74] loss: 33.633113861083984\n",
      "[step: 75] loss: 33.25503158569336\n",
      "[step: 76] loss: 32.66087341308594\n",
      "[step: 77] loss: 32.8980827331543\n",
      "[step: 78] loss: 32.44744873046875\n",
      "[step: 79] loss: 32.193458557128906\n",
      "[step: 80] loss: 32.31464767456055\n",
      "[step: 81] loss: 32.20588684082031\n",
      "[step: 82] loss: 32.92237091064453\n",
      "[step: 83] loss: 32.92852783203125\n",
      "[step: 84] loss: 31.681623458862305\n",
      "[step: 85] loss: 32.19029235839844\n",
      "[step: 86] loss: 32.141963958740234\n",
      "[step: 87] loss: 31.504962921142578\n",
      "[step: 88] loss: 31.91454315185547\n",
      "[step: 89] loss: 32.18217086791992\n",
      "[step: 90] loss: 31.9838809967041\n",
      "[step: 91] loss: 31.658029556274414\n",
      "[step: 92] loss: 32.096981048583984\n",
      "[step: 93] loss: 31.60171890258789\n",
      "[step: 94] loss: 31.61028289794922\n",
      "[step: 95] loss: 31.87854766845703\n",
      "[step: 96] loss: 31.413127899169922\n",
      "[step: 97] loss: 31.881147384643555\n",
      "[step: 98] loss: 31.215179443359375\n",
      "[step: 99] loss: 31.648454666137695\n",
      "[step: 100] loss: 31.61355972290039\n",
      "[step: 101] loss: 31.559677124023438\n",
      "[step: 102] loss: 31.273460388183594\n",
      "[step: 103] loss: 32.080177307128906\n",
      "[step: 104] loss: 31.18290138244629\n",
      "[step: 105] loss: 30.89992332458496\n",
      "[step: 106] loss: 31.711400985717773\n",
      "[step: 107] loss: 31.504352569580078\n",
      "[step: 108] loss: 31.318378448486328\n",
      "[step: 109] loss: 31.172510147094727\n",
      "[step: 110] loss: 31.261445999145508\n",
      "[step: 111] loss: 31.339401245117188\n",
      "[step: 112] loss: 31.461477279663086\n",
      "[step: 113] loss: 31.048036575317383\n",
      "[step: 114] loss: 31.154766082763672\n",
      "[step: 115] loss: 30.841115951538086\n",
      "[step: 116] loss: 30.43405532836914\n",
      "[step: 117] loss: 30.99964714050293\n",
      "[step: 118] loss: 30.90718650817871\n",
      "[step: 119] loss: 30.918996810913086\n",
      "[step: 120] loss: 30.820287704467773\n",
      "[step: 121] loss: 30.857879638671875\n",
      "[step: 122] loss: 30.3754825592041\n",
      "[step: 123] loss: 30.334579467773438\n",
      "[step: 124] loss: 30.52269744873047\n",
      "[step: 125] loss: 30.769550323486328\n",
      "[step: 126] loss: 30.450191497802734\n",
      "[step: 127] loss: 30.48777198791504\n",
      "[step: 128] loss: 30.49342155456543\n",
      "[step: 129] loss: 30.726621627807617\n",
      "[step: 130] loss: 30.463407516479492\n",
      "[step: 131] loss: 30.327760696411133\n",
      "[step: 132] loss: 30.607248306274414\n",
      "[step: 133] loss: 30.80780601501465\n",
      "[step: 134] loss: 30.30609893798828\n",
      "[step: 135] loss: 30.582672119140625\n",
      "[step: 136] loss: 30.621829986572266\n",
      "[step: 137] loss: 30.553932189941406\n",
      "[step: 138] loss: 29.99803924560547\n",
      "[step: 139] loss: 29.816688537597656\n",
      "[step: 140] loss: 30.695011138916016\n",
      "[step: 141] loss: 30.165063858032227\n",
      "[step: 142] loss: 30.784143447875977\n",
      "[step: 143] loss: 29.79789924621582\n",
      "[step: 144] loss: 30.365480422973633\n",
      "[step: 145] loss: 31.058155059814453\n",
      "[step: 146] loss: 29.67392349243164\n",
      "[step: 147] loss: 30.01300811767578\n",
      "[step: 148] loss: 30.54981803894043\n",
      "[step: 149] loss: 30.271718978881836\n",
      "[step: 150] loss: 29.5929012298584\n",
      "[step: 151] loss: 29.60786247253418\n",
      "[step: 152] loss: 30.710159301757812\n",
      "[step: 153] loss: 29.5225772857666\n",
      "[step: 154] loss: 30.075313568115234\n",
      "[step: 155] loss: 29.82740020751953\n",
      "[step: 156] loss: 30.524877548217773\n",
      "[step: 157] loss: 29.400157928466797\n",
      "[step: 158] loss: 29.720932006835938\n",
      "[step: 159] loss: 29.859947204589844\n",
      "[step: 160] loss: 30.08877944946289\n",
      "[step: 161] loss: 29.755260467529297\n",
      "[step: 162] loss: 29.31836700439453\n",
      "[step: 163] loss: 30.096426010131836\n",
      "[step: 164] loss: 30.08430290222168\n",
      "[step: 165] loss: 30.183801651000977\n",
      "[step: 166] loss: 29.617618560791016\n",
      "[step: 167] loss: 29.515535354614258\n",
      "[step: 168] loss: 29.93775177001953\n",
      "[step: 169] loss: 29.274145126342773\n",
      "[step: 170] loss: 29.297048568725586\n",
      "[step: 171] loss: 28.92034339904785\n",
      "[step: 172] loss: 29.974157333374023\n",
      "[step: 173] loss: 30.168729782104492\n",
      "[step: 174] loss: 29.522911071777344\n",
      "[step: 175] loss: 29.051328659057617\n",
      "[step: 176] loss: 29.220613479614258\n",
      "[step: 177] loss: 28.741756439208984\n",
      "[step: 178] loss: 29.102163314819336\n",
      "[step: 179] loss: 28.94454574584961\n",
      "[step: 180] loss: 29.510822296142578\n",
      "[step: 181] loss: 29.122764587402344\n",
      "[step: 182] loss: 28.813709259033203\n",
      "[step: 183] loss: 28.99504852294922\n",
      "[step: 184] loss: 29.331899642944336\n",
      "[step: 185] loss: 28.73459815979004\n",
      "[step: 186] loss: 28.99088478088379\n",
      "[step: 187] loss: 28.859766006469727\n",
      "[step: 188] loss: 29.040502548217773\n",
      "[step: 189] loss: 28.52579116821289\n",
      "[step: 190] loss: 28.42292594909668\n",
      "[step: 191] loss: 29.13852882385254\n",
      "[step: 192] loss: 29.288726806640625\n",
      "[step: 193] loss: 29.24400520324707\n",
      "[step: 194] loss: 28.870603561401367\n",
      "[step: 195] loss: 28.90387725830078\n",
      "[step: 196] loss: 28.33530044555664\n",
      "[step: 197] loss: 28.976886749267578\n",
      "[step: 198] loss: 28.715227127075195\n",
      "[step: 199] loss: 28.893667221069336\n",
      "[step: 200] loss: 29.12324333190918\n",
      "[step: 201] loss: 28.525205612182617\n",
      "[step: 202] loss: 28.12840461730957\n",
      "[step: 203] loss: 28.5855655670166\n",
      "[step: 204] loss: 28.40814971923828\n",
      "[step: 205] loss: 28.210586547851562\n",
      "[step: 206] loss: 28.247888565063477\n",
      "[step: 207] loss: 27.870609283447266\n",
      "[step: 208] loss: 29.116275787353516\n",
      "[step: 209] loss: 28.44080924987793\n",
      "[step: 210] loss: 28.248390197753906\n",
      "[step: 211] loss: 28.247629165649414\n",
      "[step: 212] loss: 28.58049201965332\n",
      "[step: 213] loss: 28.178815841674805\n",
      "[step: 214] loss: 27.85915184020996\n",
      "[step: 215] loss: 28.159894943237305\n",
      "[step: 216] loss: 27.975521087646484\n",
      "[step: 217] loss: 27.790407180786133\n",
      "[step: 218] loss: 28.35338020324707\n",
      "[step: 219] loss: 28.509601593017578\n",
      "[step: 220] loss: 27.556711196899414\n",
      "[step: 221] loss: 28.566740036010742\n",
      "[step: 222] loss: 27.95822525024414\n",
      "[step: 223] loss: 28.09364128112793\n",
      "[step: 224] loss: 27.863615036010742\n",
      "[step: 225] loss: 28.51894187927246\n",
      "[step: 226] loss: 28.063447952270508\n",
      "[step: 227] loss: 28.22909164428711\n",
      "[step: 228] loss: 28.010251998901367\n",
      "[step: 229] loss: 28.01814842224121\n",
      "[step: 230] loss: 27.812891006469727\n",
      "[step: 231] loss: 27.348020553588867\n",
      "[step: 232] loss: 27.347652435302734\n",
      "[step: 233] loss: 28.11202049255371\n",
      "[step: 234] loss: 27.51375961303711\n",
      "[step: 235] loss: 27.993301391601562\n",
      "[step: 236] loss: 27.868606567382812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 237] loss: 27.764589309692383\n",
      "[step: 238] loss: 27.427387237548828\n",
      "[step: 239] loss: 27.00132179260254\n",
      "[step: 240] loss: 26.85268783569336\n",
      "[step: 241] loss: 27.28671646118164\n",
      "[step: 242] loss: 27.315515518188477\n",
      "[step: 243] loss: 27.441640853881836\n",
      "[step: 244] loss: 27.751516342163086\n",
      "[step: 245] loss: 27.809890747070312\n",
      "[step: 246] loss: 27.532398223876953\n",
      "[step: 247] loss: 26.901290893554688\n",
      "[step: 248] loss: 26.999431610107422\n",
      "[step: 249] loss: 27.953989028930664\n",
      "[step: 250] loss: 27.144176483154297\n",
      "[step: 251] loss: 27.041515350341797\n",
      "[step: 252] loss: 27.335790634155273\n",
      "[step: 253] loss: 26.80689239501953\n",
      "[step: 254] loss: 26.42093276977539\n",
      "[step: 255] loss: 26.783935546875\n",
      "[step: 256] loss: 26.93734359741211\n",
      "[step: 257] loss: 27.413930892944336\n",
      "[step: 258] loss: 26.94245719909668\n",
      "[step: 259] loss: 27.025365829467773\n",
      "[step: 260] loss: 27.354537963867188\n",
      "[step: 261] loss: 27.659568786621094\n",
      "[step: 262] loss: 27.094694137573242\n",
      "[step: 263] loss: 26.205219268798828\n",
      "[step: 264] loss: 26.868314743041992\n",
      "[step: 265] loss: 27.12168312072754\n",
      "[step: 266] loss: 25.95237922668457\n",
      "[step: 267] loss: 26.38782501220703\n",
      "[step: 268] loss: 25.697900772094727\n",
      "[step: 269] loss: 26.988067626953125\n",
      "[step: 270] loss: 26.770896911621094\n",
      "[step: 271] loss: 26.56699562072754\n",
      "[step: 272] loss: 26.42169952392578\n",
      "[step: 273] loss: 26.529253005981445\n",
      "[step: 274] loss: 26.95767593383789\n",
      "[step: 275] loss: 26.409379959106445\n",
      "[step: 276] loss: 25.921401977539062\n",
      "[step: 277] loss: 26.625953674316406\n",
      "[step: 278] loss: 26.67877197265625\n",
      "[step: 279] loss: 26.388961791992188\n",
      "[step: 280] loss: 25.40163803100586\n",
      "[step: 281] loss: 25.599109649658203\n",
      "[step: 282] loss: 26.468727111816406\n",
      "[step: 283] loss: 25.591205596923828\n",
      "[step: 284] loss: 26.17893409729004\n",
      "[step: 285] loss: 25.99797821044922\n",
      "[step: 286] loss: 26.2354793548584\n",
      "[step: 287] loss: 25.860300064086914\n",
      "[step: 288] loss: 26.556596755981445\n",
      "[step: 289] loss: 26.203413009643555\n",
      "[step: 290] loss: 25.765283584594727\n",
      "[step: 291] loss: 26.098358154296875\n",
      "[step: 292] loss: 25.305734634399414\n",
      "[step: 293] loss: 25.860013961791992\n",
      "[step: 294] loss: 25.938199996948242\n",
      "[step: 295] loss: 26.426708221435547\n",
      "[step: 296] loss: 26.23183822631836\n",
      "[step: 297] loss: 26.177947998046875\n",
      "[step: 298] loss: 26.598613739013672\n",
      "[step: 299] loss: 25.38225555419922\n",
      "[step: 300] loss: 25.262723922729492\n",
      "[step: 301] loss: 25.13848876953125\n",
      "[step: 302] loss: 25.72511863708496\n",
      "[step: 303] loss: 25.910634994506836\n",
      "[step: 304] loss: 25.48368263244629\n",
      "[step: 305] loss: 25.125070571899414\n",
      "[step: 306] loss: 26.004108428955078\n",
      "[step: 307] loss: 24.822607040405273\n",
      "[step: 308] loss: 25.451372146606445\n",
      "[step: 309] loss: 25.458322525024414\n",
      "[step: 310] loss: 25.13628387451172\n",
      "[step: 311] loss: 25.51025390625\n",
      "[step: 312] loss: 26.052114486694336\n",
      "[step: 313] loss: 24.903785705566406\n",
      "[step: 314] loss: 25.000268936157227\n",
      "[step: 315] loss: 25.558990478515625\n",
      "[step: 316] loss: 25.1549072265625\n",
      "[step: 317] loss: 25.34347152709961\n",
      "[step: 318] loss: 24.3975887298584\n",
      "[step: 319] loss: 25.040237426757812\n",
      "[step: 320] loss: 25.33700942993164\n",
      "[step: 321] loss: 25.359169006347656\n",
      "[step: 322] loss: 25.95025634765625\n",
      "[step: 323] loss: 24.376432418823242\n",
      "[step: 324] loss: 24.982376098632812\n",
      "[step: 325] loss: 25.517663955688477\n",
      "[step: 326] loss: 24.718015670776367\n",
      "[step: 327] loss: 24.51309585571289\n",
      "[step: 328] loss: 25.02140998840332\n",
      "[step: 329] loss: 24.471860885620117\n",
      "[step: 330] loss: 25.544221878051758\n",
      "[step: 331] loss: 25.17694854736328\n",
      "[step: 332] loss: 24.612245559692383\n",
      "[step: 333] loss: 25.347841262817383\n",
      "[step: 334] loss: 25.110488891601562\n",
      "[step: 335] loss: 24.702865600585938\n",
      "[step: 336] loss: 25.199541091918945\n",
      "[step: 337] loss: 25.475492477416992\n",
      "[step: 338] loss: 24.6727294921875\n",
      "[step: 339] loss: 24.997901916503906\n",
      "[step: 340] loss: 25.147628784179688\n",
      "[step: 341] loss: 24.901363372802734\n",
      "[step: 342] loss: 24.306589126586914\n",
      "[step: 343] loss: 24.97768211364746\n",
      "[step: 344] loss: 25.129478454589844\n",
      "[step: 345] loss: 24.661441802978516\n",
      "[step: 346] loss: 24.16640853881836\n",
      "[step: 347] loss: 24.679887771606445\n",
      "[step: 348] loss: 24.930498123168945\n",
      "[step: 349] loss: 25.322647094726562\n",
      "[step: 350] loss: 24.685678482055664\n",
      "[step: 351] loss: 25.14897346496582\n",
      "[step: 352] loss: 24.534849166870117\n",
      "[step: 353] loss: 24.157501220703125\n",
      "[step: 354] loss: 24.508468627929688\n",
      "[step: 355] loss: 25.071914672851562\n",
      "[step: 356] loss: 24.843421936035156\n",
      "[step: 357] loss: 24.587255477905273\n",
      "[step: 358] loss: 24.4092960357666\n",
      "[step: 359] loss: 24.68278694152832\n",
      "[step: 360] loss: 24.56035804748535\n",
      "[step: 361] loss: 24.366901397705078\n",
      "[step: 362] loss: 24.786357879638672\n",
      "[step: 363] loss: 25.03626823425293\n",
      "[step: 364] loss: 24.27676773071289\n",
      "[step: 365] loss: 24.6524600982666\n",
      "[step: 366] loss: 24.454008102416992\n",
      "[step: 367] loss: 24.136314392089844\n",
      "[step: 368] loss: 24.091440200805664\n",
      "[step: 369] loss: 24.58964729309082\n",
      "[step: 370] loss: 23.9105224609375\n",
      "[step: 371] loss: 24.50452423095703\n",
      "[step: 372] loss: 23.841922760009766\n",
      "[step: 373] loss: 24.232826232910156\n",
      "[step: 374] loss: 23.57782745361328\n",
      "[step: 375] loss: 24.228759765625\n",
      "[step: 376] loss: 24.302064895629883\n",
      "[step: 377] loss: 24.957988739013672\n",
      "[step: 378] loss: 23.882036209106445\n",
      "[step: 379] loss: 24.421403884887695\n",
      "[step: 380] loss: 23.464799880981445\n",
      "[step: 381] loss: 24.376354217529297\n",
      "[step: 382] loss: 24.88997459411621\n",
      "[step: 383] loss: 24.26959228515625\n",
      "[step: 384] loss: 24.002674102783203\n",
      "[step: 385] loss: 23.985750198364258\n",
      "[step: 386] loss: 24.57758140563965\n",
      "[step: 387] loss: 24.965822219848633\n",
      "[step: 388] loss: 23.948833465576172\n",
      "[step: 389] loss: 24.446107864379883\n",
      "[step: 390] loss: 24.391080856323242\n",
      "[step: 391] loss: 23.88311767578125\n",
      "[step: 392] loss: 24.006277084350586\n",
      "[step: 393] loss: 24.47243309020996\n",
      "[step: 394] loss: 24.51124382019043\n",
      "[step: 395] loss: 23.91019630432129\n",
      "[step: 396] loss: 23.998090744018555\n",
      "[step: 397] loss: 24.318540573120117\n",
      "[step: 398] loss: 24.429546356201172\n",
      "[step: 399] loss: 24.23628044128418\n",
      "[step: 400] loss: 24.412704467773438\n",
      "[step: 401] loss: 24.800045013427734\n",
      "[step: 402] loss: 24.121578216552734\n",
      "[step: 403] loss: 24.203536987304688\n",
      "[step: 404] loss: 24.154834747314453\n",
      "[step: 405] loss: 23.55776596069336\n",
      "[step: 406] loss: 24.50714111328125\n",
      "[step: 407] loss: 24.372329711914062\n",
      "[step: 408] loss: 24.14008903503418\n",
      "[step: 409] loss: 24.229215621948242\n",
      "[step: 410] loss: 24.305139541625977\n",
      "[step: 411] loss: 23.39238929748535\n",
      "[step: 412] loss: 24.454500198364258\n",
      "[step: 413] loss: 24.39667510986328\n",
      "[step: 414] loss: 24.706880569458008\n",
      "[step: 415] loss: 24.148365020751953\n",
      "[step: 416] loss: 24.068262100219727\n",
      "[step: 417] loss: 23.943666458129883\n",
      "[step: 418] loss: 23.494773864746094\n",
      "[step: 419] loss: 24.42095947265625\n",
      "[step: 420] loss: 23.700803756713867\n",
      "[step: 421] loss: 24.370763778686523\n",
      "[step: 422] loss: 24.233585357666016\n",
      "[step: 423] loss: 23.92728042602539\n",
      "[step: 424] loss: 24.501222610473633\n",
      "[step: 425] loss: 23.743247985839844\n",
      "[step: 426] loss: 24.21430015563965\n",
      "[step: 427] loss: 23.560489654541016\n",
      "[step: 428] loss: 23.989784240722656\n",
      "[step: 429] loss: 23.63223648071289\n",
      "[step: 430] loss: 24.556909561157227\n",
      "[step: 431] loss: 24.016002655029297\n",
      "[step: 432] loss: 23.941577911376953\n",
      "[step: 433] loss: 24.21717071533203\n",
      "[step: 434] loss: 24.28586769104004\n",
      "[step: 435] loss: 23.10675048828125\n",
      "[step: 436] loss: 24.102691650390625\n",
      "[step: 437] loss: 24.06002426147461\n",
      "[step: 438] loss: 23.894752502441406\n",
      "[step: 439] loss: 23.625629425048828\n",
      "[step: 440] loss: 24.087308883666992\n",
      "[step: 441] loss: 23.966327667236328\n",
      "[step: 442] loss: 23.530942916870117\n",
      "[step: 443] loss: 24.089168548583984\n",
      "[step: 444] loss: 23.750839233398438\n",
      "[step: 445] loss: 23.92374610900879\n",
      "[step: 446] loss: 24.211997985839844\n",
      "[step: 447] loss: 24.53837013244629\n",
      "[step: 448] loss: 23.66737174987793\n",
      "[step: 449] loss: 23.719924926757812\n",
      "[step: 450] loss: 23.61201286315918\n",
      "[step: 451] loss: 23.028364181518555\n",
      "[step: 452] loss: 24.041004180908203\n",
      "[step: 453] loss: 23.99032211303711\n",
      "[step: 454] loss: 23.939762115478516\n",
      "[step: 455] loss: 24.0943660736084\n",
      "[step: 456] loss: 23.297183990478516\n",
      "[step: 457] loss: 23.760412216186523\n",
      "[step: 458] loss: 24.04447364807129\n",
      "[step: 459] loss: 24.025859832763672\n",
      "[step: 460] loss: 23.940746307373047\n",
      "[step: 461] loss: 23.161916732788086\n",
      "[step: 462] loss: 24.46843147277832\n",
      "[step: 463] loss: 23.98678207397461\n",
      "[step: 464] loss: 24.517677307128906\n",
      "[step: 465] loss: 23.20926856994629\n",
      "[step: 466] loss: 23.862653732299805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 467] loss: 23.54131317138672\n",
      "[step: 468] loss: 24.031085968017578\n",
      "[step: 469] loss: 24.12444305419922\n",
      "[step: 470] loss: 23.58847427368164\n",
      "[step: 471] loss: 23.830322265625\n",
      "[step: 472] loss: 23.552852630615234\n",
      "[step: 473] loss: 24.30992889404297\n",
      "[step: 474] loss: 24.203950881958008\n",
      "[step: 475] loss: 23.664161682128906\n",
      "[step: 476] loss: 23.89391326904297\n",
      "[step: 477] loss: 23.765012741088867\n",
      "[step: 478] loss: 23.472171783447266\n",
      "[step: 479] loss: 23.66794776916504\n",
      "[step: 480] loss: 23.987760543823242\n",
      "[step: 481] loss: 23.645627975463867\n",
      "[step: 482] loss: 24.20146942138672\n",
      "[step: 483] loss: 23.862323760986328\n",
      "[step: 484] loss: 23.21052360534668\n",
      "[step: 485] loss: 23.667036056518555\n",
      "[step: 486] loss: 23.509479522705078\n",
      "[step: 487] loss: 23.052562713623047\n",
      "[step: 488] loss: 23.868824005126953\n",
      "[step: 489] loss: 23.629667282104492\n",
      "[step: 490] loss: 23.719451904296875\n",
      "[step: 491] loss: 23.607223510742188\n",
      "[step: 492] loss: 23.682695388793945\n",
      "[step: 493] loss: 23.969987869262695\n",
      "[step: 494] loss: 23.55608367919922\n",
      "[step: 495] loss: 23.92519187927246\n",
      "[step: 496] loss: 22.95905113220215\n",
      "[step: 497] loss: 23.696271896362305\n",
      "[step: 498] loss: 23.015066146850586\n",
      "[step: 499] loss: 23.612951278686523\n",
      "[step: 500] loss: 23.429546356201172\n",
      "[step: 501] loss: 24.032068252563477\n",
      "[step: 502] loss: 23.57264518737793\n",
      "[step: 503] loss: 23.353376388549805\n",
      "[step: 504] loss: 23.153484344482422\n",
      "[step: 505] loss: 23.46947479248047\n",
      "[step: 506] loss: 23.68255043029785\n",
      "[step: 507] loss: 23.834081649780273\n",
      "[step: 508] loss: 23.576187133789062\n",
      "[step: 509] loss: 23.782535552978516\n",
      "[step: 510] loss: 23.990524291992188\n",
      "[step: 511] loss: 23.583642959594727\n",
      "[step: 512] loss: 23.240571975708008\n",
      "[step: 513] loss: 23.263404846191406\n",
      "[step: 514] loss: 23.9754638671875\n",
      "[step: 515] loss: 23.033506393432617\n",
      "[step: 516] loss: 23.87294578552246\n",
      "[step: 517] loss: 23.419967651367188\n",
      "[step: 518] loss: 23.363523483276367\n",
      "[step: 519] loss: 23.68695640563965\n",
      "[step: 520] loss: 23.13506317138672\n",
      "[step: 521] loss: 23.536529541015625\n",
      "[step: 522] loss: 23.314252853393555\n",
      "[step: 523] loss: 23.023984909057617\n",
      "[step: 524] loss: 23.62000274658203\n",
      "[step: 525] loss: 23.906766891479492\n",
      "[step: 526] loss: 23.65252113342285\n",
      "[step: 527] loss: 23.555431365966797\n",
      "[step: 528] loss: 23.300325393676758\n",
      "[step: 529] loss: 22.725208282470703\n",
      "[step: 530] loss: 23.163450241088867\n",
      "[step: 531] loss: 23.856386184692383\n",
      "[step: 532] loss: 23.535879135131836\n",
      "[step: 533] loss: 23.602643966674805\n",
      "[step: 534] loss: 23.36774253845215\n",
      "[step: 535] loss: 23.22011375427246\n",
      "[step: 536] loss: 23.73208236694336\n",
      "[step: 537] loss: 23.71410369873047\n",
      "[step: 538] loss: 23.36525535583496\n",
      "[step: 539] loss: 23.00039291381836\n",
      "[step: 540] loss: 23.822908401489258\n",
      "[step: 541] loss: 23.557668685913086\n",
      "[step: 542] loss: 23.50239372253418\n",
      "[step: 543] loss: 23.201650619506836\n",
      "[step: 544] loss: 22.99321937561035\n",
      "[step: 545] loss: 22.862924575805664\n",
      "[step: 546] loss: 23.065204620361328\n",
      "[step: 547] loss: 23.494653701782227\n",
      "[step: 548] loss: 23.56902503967285\n",
      "[step: 549] loss: 23.00421142578125\n",
      "[step: 550] loss: 23.797178268432617\n",
      "[step: 551] loss: 23.304109573364258\n",
      "[step: 552] loss: 23.52094078063965\n",
      "[step: 553] loss: 23.39844512939453\n",
      "[step: 554] loss: 23.291229248046875\n",
      "[step: 555] loss: 23.286693572998047\n",
      "[step: 556] loss: 23.10798454284668\n",
      "[step: 557] loss: 23.554162979125977\n",
      "[step: 558] loss: 23.085309982299805\n",
      "[step: 559] loss: 23.350521087646484\n",
      "[step: 560] loss: 23.50261688232422\n",
      "[step: 561] loss: 23.487878799438477\n",
      "[step: 562] loss: 23.21976661682129\n",
      "[step: 563] loss: 23.574472427368164\n",
      "[step: 564] loss: 23.407474517822266\n",
      "[step: 565] loss: 23.100122451782227\n",
      "[step: 566] loss: 23.765899658203125\n",
      "[step: 567] loss: 23.524089813232422\n",
      "[step: 568] loss: 23.61019515991211\n",
      "[step: 569] loss: 23.89725685119629\n",
      "[step: 570] loss: 23.900230407714844\n",
      "[step: 571] loss: 22.891510009765625\n",
      "[step: 572] loss: 24.030113220214844\n",
      "[step: 573] loss: 23.554655075073242\n",
      "[step: 574] loss: 23.294893264770508\n",
      "[step: 575] loss: 23.18363380432129\n",
      "[step: 576] loss: 23.427370071411133\n",
      "[step: 577] loss: 23.153331756591797\n",
      "[step: 578] loss: 23.105329513549805\n",
      "[step: 579] loss: 22.67995262145996\n",
      "[step: 580] loss: 23.119112014770508\n",
      "[step: 581] loss: 23.047908782958984\n",
      "[step: 582] loss: 23.292451858520508\n",
      "[step: 583] loss: 23.723033905029297\n",
      "[step: 584] loss: 23.39847755432129\n",
      "[step: 585] loss: 22.76479721069336\n",
      "[step: 586] loss: 22.989439010620117\n",
      "[step: 587] loss: 23.00822639465332\n",
      "[step: 588] loss: 24.225418090820312\n",
      "[step: 589] loss: 23.187490463256836\n",
      "[step: 590] loss: 22.96352195739746\n",
      "[step: 591] loss: 23.52660369873047\n",
      "[step: 592] loss: 22.795936584472656\n",
      "[step: 593] loss: 23.499935150146484\n",
      "[step: 594] loss: 23.686269760131836\n",
      "[step: 595] loss: 23.732540130615234\n",
      "[step: 596] loss: 23.578533172607422\n",
      "[step: 597] loss: 23.239521026611328\n",
      "[step: 598] loss: 23.08917808532715\n",
      "[step: 599] loss: 23.223323822021484\n",
      "[step: 600] loss: 23.258895874023438\n",
      "[step: 601] loss: 22.845144271850586\n",
      "[step: 602] loss: 23.425195693969727\n",
      "[step: 603] loss: 23.14922523498535\n",
      "[step: 604] loss: 23.311811447143555\n",
      "[step: 605] loss: 23.591283798217773\n",
      "[step: 606] loss: 22.677316665649414\n",
      "[step: 607] loss: 23.750974655151367\n",
      "[step: 608] loss: 23.535661697387695\n",
      "[step: 609] loss: 23.58384132385254\n",
      "[step: 610] loss: 23.54264259338379\n",
      "[step: 611] loss: 23.273521423339844\n",
      "[step: 612] loss: 22.936323165893555\n",
      "[step: 613] loss: 23.380615234375\n",
      "[step: 614] loss: 22.883609771728516\n",
      "[step: 615] loss: 23.32900619506836\n",
      "[step: 616] loss: 23.309925079345703\n",
      "[step: 617] loss: 23.561607360839844\n",
      "[step: 618] loss: 23.36797523498535\n",
      "[step: 619] loss: 23.328454971313477\n",
      "[step: 620] loss: 23.159425735473633\n",
      "[step: 621] loss: 23.272144317626953\n",
      "[step: 622] loss: 22.731882095336914\n",
      "[step: 623] loss: 24.05957794189453\n",
      "[step: 624] loss: 22.730274200439453\n",
      "[step: 625] loss: 23.154611587524414\n",
      "[step: 626] loss: 23.031160354614258\n",
      "[step: 627] loss: 23.504684448242188\n",
      "[step: 628] loss: 23.381149291992188\n",
      "[step: 629] loss: 23.296672821044922\n",
      "[step: 630] loss: 23.652999877929688\n",
      "[step: 631] loss: 22.770605087280273\n",
      "[step: 632] loss: 22.996618270874023\n",
      "[step: 633] loss: 23.76218605041504\n",
      "[step: 634] loss: 22.97145652770996\n",
      "[step: 635] loss: 23.26310157775879\n",
      "[step: 636] loss: 23.361181259155273\n",
      "[step: 637] loss: 23.104461669921875\n",
      "[step: 638] loss: 23.234420776367188\n",
      "[step: 639] loss: 23.350757598876953\n",
      "[step: 640] loss: 23.663616180419922\n",
      "[step: 641] loss: 23.18304443359375\n",
      "[step: 642] loss: 23.091625213623047\n",
      "[step: 643] loss: 23.766294479370117\n",
      "[step: 644] loss: 23.59452247619629\n",
      "[step: 645] loss: 23.02623748779297\n",
      "[step: 646] loss: 23.41337013244629\n",
      "[step: 647] loss: 22.86893081665039\n",
      "[step: 648] loss: 23.101234436035156\n",
      "[step: 649] loss: 23.502981185913086\n",
      "[step: 650] loss: 23.122222900390625\n",
      "[step: 651] loss: 23.01297378540039\n",
      "[step: 652] loss: 23.480403900146484\n",
      "[step: 653] loss: 23.362838745117188\n",
      "[step: 654] loss: 23.592897415161133\n",
      "[step: 655] loss: 23.30869483947754\n",
      "[step: 656] loss: 23.565589904785156\n",
      "[step: 657] loss: 23.05191993713379\n",
      "[step: 658] loss: 23.395118713378906\n",
      "[step: 659] loss: 23.616525650024414\n",
      "[step: 660] loss: 22.88079071044922\n",
      "[step: 661] loss: 23.111478805541992\n",
      "[step: 662] loss: 23.24611473083496\n",
      "[step: 663] loss: 23.004056930541992\n",
      "[step: 664] loss: 24.15479278564453\n",
      "[step: 665] loss: 22.669994354248047\n",
      "[step: 666] loss: 22.776697158813477\n",
      "[step: 667] loss: 23.47486686706543\n",
      "[step: 668] loss: 23.373756408691406\n",
      "[step: 669] loss: 23.513107299804688\n",
      "[step: 670] loss: 23.067180633544922\n",
      "[step: 671] loss: 23.01277732849121\n",
      "[step: 672] loss: 23.129146575927734\n",
      "[step: 673] loss: 23.221036911010742\n",
      "[step: 674] loss: 23.155250549316406\n",
      "[step: 675] loss: 23.202754974365234\n",
      "[step: 676] loss: 23.06368064880371\n",
      "[step: 677] loss: 23.58798599243164\n",
      "[step: 678] loss: 23.258392333984375\n",
      "[step: 679] loss: 23.300373077392578\n",
      "[step: 680] loss: 22.834436416625977\n",
      "[step: 681] loss: 23.583993911743164\n",
      "[step: 682] loss: 23.483980178833008\n",
      "[step: 683] loss: 23.402828216552734\n",
      "[step: 684] loss: 22.982681274414062\n",
      "[step: 685] loss: 22.717098236083984\n",
      "[step: 686] loss: 23.58757972717285\n",
      "[step: 687] loss: 23.063013076782227\n",
      "[step: 688] loss: 23.059478759765625\n",
      "[step: 689] loss: 23.09334945678711\n",
      "[step: 690] loss: 23.20663070678711\n",
      "[step: 691] loss: 23.247180938720703\n",
      "[step: 692] loss: 23.580665588378906\n",
      "[step: 693] loss: 23.32029914855957\n",
      "[step: 694] loss: 22.89018440246582\n",
      "[step: 695] loss: 23.414302825927734\n",
      "[step: 696] loss: 23.374134063720703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 697] loss: 23.14600944519043\n",
      "[step: 698] loss: 23.35934829711914\n",
      "[step: 699] loss: 22.75181007385254\n",
      "[step: 700] loss: 22.987403869628906\n",
      "[step: 701] loss: 22.86614990234375\n",
      "[step: 702] loss: 22.95069694519043\n",
      "[step: 703] loss: 23.620281219482422\n",
      "[step: 704] loss: 23.165184020996094\n",
      "[step: 705] loss: 23.38279914855957\n",
      "[step: 706] loss: 23.198869705200195\n",
      "[step: 707] loss: 22.984960556030273\n",
      "[step: 708] loss: 22.73331642150879\n",
      "[step: 709] loss: 22.752811431884766\n",
      "[step: 710] loss: 22.477157592773438\n",
      "[step: 711] loss: 23.142013549804688\n",
      "[step: 712] loss: 23.32745361328125\n",
      "[step: 713] loss: 23.073637008666992\n",
      "[step: 714] loss: 23.130733489990234\n",
      "[step: 715] loss: 23.19997215270996\n",
      "[step: 716] loss: 23.218114852905273\n",
      "[step: 717] loss: 23.070804595947266\n",
      "[step: 718] loss: 22.888784408569336\n",
      "[step: 719] loss: 22.660892486572266\n",
      "[step: 720] loss: 22.86139678955078\n",
      "[step: 721] loss: 23.151775360107422\n",
      "[step: 722] loss: 23.16602325439453\n",
      "[step: 723] loss: 23.804515838623047\n",
      "[step: 724] loss: 24.022993087768555\n",
      "[step: 725] loss: 22.421789169311523\n",
      "[step: 726] loss: 23.52031898498535\n",
      "[step: 727] loss: 23.41337776184082\n",
      "[step: 728] loss: 22.803010940551758\n",
      "[step: 729] loss: 22.86387825012207\n",
      "[step: 730] loss: 23.11410903930664\n",
      "[step: 731] loss: 23.355144500732422\n",
      "[step: 732] loss: 22.5734920501709\n",
      "[step: 733] loss: 23.631328582763672\n",
      "[step: 734] loss: 23.280893325805664\n",
      "[step: 735] loss: 22.843894958496094\n",
      "[step: 736] loss: 23.069753646850586\n",
      "[step: 737] loss: 23.217294692993164\n",
      "[step: 738] loss: 23.02395248413086\n",
      "[step: 739] loss: 22.88957977294922\n",
      "[step: 740] loss: 22.877056121826172\n",
      "[step: 741] loss: 22.836702346801758\n",
      "[step: 742] loss: 22.784975051879883\n",
      "[step: 743] loss: 22.33253288269043\n",
      "[step: 744] loss: 23.029420852661133\n",
      "[step: 745] loss: 23.40286636352539\n",
      "[step: 746] loss: 23.457061767578125\n",
      "[step: 747] loss: 22.880840301513672\n",
      "[step: 748] loss: 23.12807846069336\n",
      "[step: 749] loss: 22.59223175048828\n",
      "[step: 750] loss: 22.879186630249023\n",
      "[step: 751] loss: 23.480445861816406\n",
      "[step: 752] loss: 23.33574104309082\n",
      "[step: 753] loss: 22.730770111083984\n",
      "[step: 754] loss: 23.220256805419922\n",
      "[step: 755] loss: 23.009971618652344\n",
      "[step: 756] loss: 23.21034812927246\n",
      "[step: 757] loss: 22.712148666381836\n",
      "[step: 758] loss: 22.53723907470703\n",
      "[step: 759] loss: 22.083797454833984\n",
      "[step: 760] loss: 23.312461853027344\n",
      "[step: 761] loss: 22.728275299072266\n",
      "[step: 762] loss: 22.8909969329834\n",
      "[step: 763] loss: 23.157087326049805\n",
      "[step: 764] loss: 23.123716354370117\n",
      "[step: 765] loss: 23.5044002532959\n",
      "[step: 766] loss: 22.788196563720703\n",
      "[step: 767] loss: 23.078060150146484\n",
      "[step: 768] loss: 22.826005935668945\n",
      "[step: 769] loss: 22.38158416748047\n",
      "[step: 770] loss: 22.945390701293945\n",
      "[step: 771] loss: 22.794546127319336\n",
      "[step: 772] loss: 23.73247528076172\n",
      "[step: 773] loss: 22.658321380615234\n",
      "[step: 774] loss: 23.031028747558594\n",
      "[step: 775] loss: 22.97242546081543\n",
      "[step: 776] loss: 23.11630630493164\n",
      "[step: 777] loss: 23.26972198486328\n",
      "[step: 778] loss: 23.30733871459961\n",
      "[step: 779] loss: 22.68796157836914\n",
      "[step: 780] loss: 22.507282257080078\n",
      "[step: 781] loss: 22.65694808959961\n",
      "[step: 782] loss: 23.185352325439453\n",
      "[step: 783] loss: 22.66749382019043\n",
      "[step: 784] loss: 22.946657180786133\n",
      "[step: 785] loss: 22.884918212890625\n",
      "[step: 786] loss: 23.205665588378906\n",
      "[step: 787] loss: 22.763521194458008\n",
      "[step: 788] loss: 22.26445960998535\n",
      "[step: 789] loss: 22.98659324645996\n",
      "[step: 790] loss: 22.63187026977539\n",
      "[step: 791] loss: 22.57605743408203\n",
      "[step: 792] loss: 22.822734832763672\n",
      "[step: 793] loss: 23.416074752807617\n",
      "[step: 794] loss: 22.69957733154297\n",
      "[step: 795] loss: 23.473796844482422\n",
      "[step: 796] loss: 22.38926887512207\n",
      "[step: 797] loss: 23.040138244628906\n",
      "[step: 798] loss: 23.007122039794922\n",
      "[step: 799] loss: 23.05255699157715\n",
      "[step: 800] loss: 23.089035034179688\n",
      "[step: 801] loss: 23.29962158203125\n",
      "[step: 802] loss: 22.861980438232422\n",
      "[step: 803] loss: 22.769411087036133\n",
      "[step: 804] loss: 22.72054672241211\n",
      "[step: 805] loss: 22.79372215270996\n",
      "[step: 806] loss: 22.779006958007812\n",
      "[step: 807] loss: 22.759286880493164\n",
      "[step: 808] loss: 22.612674713134766\n",
      "[step: 809] loss: 22.608768463134766\n",
      "[step: 810] loss: 23.25396156311035\n",
      "[step: 811] loss: 23.244796752929688\n",
      "[step: 812] loss: 22.676584243774414\n",
      "[step: 813] loss: 22.66937255859375\n",
      "[step: 814] loss: 22.978252410888672\n",
      "[step: 815] loss: 23.562910079956055\n",
      "[step: 816] loss: 23.07337188720703\n",
      "[step: 817] loss: 22.95970916748047\n",
      "[step: 818] loss: 23.23684310913086\n",
      "[step: 819] loss: 21.91587257385254\n",
      "[step: 820] loss: 22.602996826171875\n",
      "[step: 821] loss: 23.208322525024414\n",
      "[step: 822] loss: 22.85649299621582\n",
      "[step: 823] loss: 23.1789608001709\n",
      "[step: 824] loss: 23.34075927734375\n",
      "[step: 825] loss: 22.715618133544922\n",
      "[step: 826] loss: 22.418975830078125\n",
      "[step: 827] loss: 22.55438232421875\n",
      "[step: 828] loss: 22.707805633544922\n",
      "[step: 829] loss: 22.673213958740234\n",
      "[step: 830] loss: 22.707721710205078\n",
      "[step: 831] loss: 23.173246383666992\n",
      "[step: 832] loss: 22.618324279785156\n",
      "[step: 833] loss: 22.637691497802734\n",
      "[step: 834] loss: 22.968610763549805\n",
      "[step: 835] loss: 22.92041778564453\n",
      "[step: 836] loss: 22.613828659057617\n",
      "[step: 837] loss: 22.436660766601562\n",
      "[step: 838] loss: 22.703414916992188\n",
      "[step: 839] loss: 22.826812744140625\n",
      "[step: 840] loss: 23.242042541503906\n",
      "[step: 841] loss: 23.069087982177734\n",
      "[step: 842] loss: 22.87729835510254\n",
      "[step: 843] loss: 23.45547866821289\n",
      "[step: 844] loss: 23.251346588134766\n",
      "[step: 845] loss: 22.803190231323242\n",
      "[step: 846] loss: 23.775392532348633\n",
      "[step: 847] loss: 22.902793884277344\n",
      "[step: 848] loss: 22.87924575805664\n",
      "[step: 849] loss: 22.921268463134766\n",
      "[step: 850] loss: 23.397991180419922\n",
      "[step: 851] loss: 23.594419479370117\n",
      "[step: 852] loss: 22.956924438476562\n",
      "[step: 853] loss: 23.104135513305664\n",
      "[step: 854] loss: 22.80145263671875\n",
      "[step: 855] loss: 23.583715438842773\n",
      "[step: 856] loss: 23.158241271972656\n",
      "[step: 857] loss: 23.422487258911133\n",
      "[step: 858] loss: 23.068500518798828\n",
      "[step: 859] loss: 23.704992294311523\n",
      "[step: 860] loss: 23.46909523010254\n",
      "[step: 861] loss: 23.19855499267578\n",
      "[step: 862] loss: 23.133813858032227\n",
      "[step: 863] loss: 23.313068389892578\n",
      "[step: 864] loss: 22.785568237304688\n",
      "[step: 865] loss: 23.512624740600586\n",
      "[step: 866] loss: 23.34294319152832\n",
      "[step: 867] loss: 22.470361709594727\n",
      "[step: 868] loss: 22.451745986938477\n",
      "[step: 869] loss: 22.478994369506836\n",
      "[step: 870] loss: 23.06399917602539\n",
      "[step: 871] loss: 22.702594757080078\n",
      "[step: 872] loss: 22.470182418823242\n",
      "[step: 873] loss: 22.24369239807129\n",
      "[step: 874] loss: 22.558988571166992\n",
      "[step: 875] loss: 22.791833877563477\n",
      "[step: 876] loss: 22.506732940673828\n",
      "[step: 877] loss: 22.51669692993164\n",
      "[step: 878] loss: 22.81349754333496\n",
      "[step: 879] loss: 22.8586368560791\n",
      "[step: 880] loss: 22.81056022644043\n",
      "[step: 881] loss: 23.010786056518555\n",
      "[step: 882] loss: 22.850404739379883\n",
      "[step: 883] loss: 22.122224807739258\n",
      "[step: 884] loss: 22.632173538208008\n",
      "[step: 885] loss: 23.348596572875977\n",
      "[step: 886] loss: 23.04950523376465\n",
      "[step: 887] loss: 22.11697006225586\n",
      "[step: 888] loss: 23.10312271118164\n",
      "[step: 889] loss: 22.930959701538086\n",
      "[step: 890] loss: 22.61812973022461\n",
      "[step: 891] loss: 22.420244216918945\n",
      "[step: 892] loss: 22.85934829711914\n",
      "[step: 893] loss: 23.0460262298584\n",
      "[step: 894] loss: 22.824951171875\n",
      "[step: 895] loss: 22.81331443786621\n",
      "[step: 896] loss: 22.412527084350586\n",
      "[step: 897] loss: 23.20500373840332\n",
      "[step: 898] loss: 22.954755783081055\n",
      "[step: 899] loss: 23.1148681640625\n",
      "[step: 900] loss: 23.05779266357422\n",
      "[step: 901] loss: 22.213857650756836\n",
      "[step: 902] loss: 22.137649536132812\n",
      "[step: 903] loss: 22.708192825317383\n",
      "[step: 904] loss: 22.80158233642578\n",
      "[step: 905] loss: 22.775327682495117\n",
      "[step: 906] loss: 22.329994201660156\n",
      "[step: 907] loss: 21.92242431640625\n",
      "[step: 908] loss: 23.164182662963867\n",
      "[step: 909] loss: 22.890697479248047\n",
      "[step: 910] loss: 22.581775665283203\n",
      "[step: 911] loss: 22.97545623779297\n",
      "[step: 912] loss: 22.34117317199707\n",
      "[step: 913] loss: 23.060306549072266\n",
      "[step: 914] loss: 22.796268463134766\n",
      "[step: 915] loss: 22.803752899169922\n",
      "[step: 916] loss: 22.740419387817383\n",
      "[step: 917] loss: 23.312705993652344\n",
      "[step: 918] loss: 22.586824417114258\n",
      "[step: 919] loss: 22.622812271118164\n",
      "[step: 920] loss: 22.06045150756836\n",
      "[step: 921] loss: 22.226364135742188\n",
      "[step: 922] loss: 22.85972023010254\n",
      "[step: 923] loss: 22.389440536499023\n",
      "[step: 924] loss: 22.971641540527344\n",
      "[step: 925] loss: 22.3039608001709\n",
      "[step: 926] loss: 22.425357818603516\n",
      "[step: 927] loss: 22.598424911499023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 928] loss: 22.706026077270508\n",
      "[step: 929] loss: 22.952369689941406\n",
      "[step: 930] loss: 22.51117706298828\n",
      "[step: 931] loss: 22.83049774169922\n",
      "[step: 932] loss: 22.70072364807129\n",
      "[step: 933] loss: 22.088424682617188\n",
      "[step: 934] loss: 23.18568229675293\n",
      "[step: 935] loss: 22.581592559814453\n",
      "[step: 936] loss: 22.606496810913086\n",
      "[step: 937] loss: 22.573720932006836\n",
      "[step: 938] loss: 22.74169921875\n",
      "[step: 939] loss: 22.62115478515625\n",
      "[step: 940] loss: 23.044157028198242\n",
      "[step: 941] loss: 23.859323501586914\n",
      "[step: 942] loss: 22.784887313842773\n",
      "[step: 943] loss: 22.632457733154297\n",
      "[step: 944] loss: 22.77842140197754\n",
      "[step: 945] loss: 22.542787551879883\n",
      "[step: 946] loss: 23.174562454223633\n",
      "[step: 947] loss: 22.21107292175293\n",
      "[step: 948] loss: 22.398508071899414\n",
      "[step: 949] loss: 22.724609375\n",
      "[step: 950] loss: 22.867387771606445\n",
      "[step: 951] loss: 22.376605987548828\n",
      "[step: 952] loss: 22.463485717773438\n",
      "[step: 953] loss: 23.08601951599121\n",
      "[step: 954] loss: 22.752967834472656\n",
      "[step: 955] loss: 23.01343536376953\n",
      "[step: 956] loss: 22.187267303466797\n",
      "[step: 957] loss: 22.373031616210938\n",
      "[step: 958] loss: 23.166824340820312\n",
      "[step: 959] loss: 22.855491638183594\n",
      "[step: 960] loss: 22.310949325561523\n",
      "[step: 961] loss: 22.391759872436523\n",
      "[step: 962] loss: 23.320615768432617\n",
      "[step: 963] loss: 22.379884719848633\n",
      "[step: 964] loss: 22.57888412475586\n",
      "[step: 965] loss: 22.89514923095703\n",
      "[step: 966] loss: 22.66099739074707\n",
      "[step: 967] loss: 22.31536293029785\n",
      "[step: 968] loss: 22.659507751464844\n",
      "[step: 969] loss: 22.212295532226562\n",
      "[step: 970] loss: 22.95098876953125\n",
      "[step: 971] loss: 22.417776107788086\n",
      "[step: 972] loss: 23.105730056762695\n",
      "[step: 973] loss: 23.296483993530273\n",
      "[step: 974] loss: 22.70098304748535\n",
      "[step: 975] loss: 22.49234962463379\n",
      "[step: 976] loss: 23.139484405517578\n",
      "[step: 977] loss: 22.486286163330078\n",
      "[step: 978] loss: 22.62160301208496\n",
      "[step: 979] loss: 21.871551513671875\n",
      "[step: 980] loss: 22.592432022094727\n",
      "[step: 981] loss: 22.769298553466797\n",
      "[step: 982] loss: 22.257081985473633\n",
      "[step: 983] loss: 22.832475662231445\n",
      "[step: 984] loss: 22.268047332763672\n",
      "[step: 985] loss: 22.883298873901367\n",
      "[step: 986] loss: 22.442031860351562\n",
      "[step: 987] loss: 22.248950958251953\n",
      "[step: 988] loss: 22.624868392944336\n",
      "[step: 989] loss: 23.087074279785156\n",
      "[step: 990] loss: 22.800901412963867\n",
      "[step: 991] loss: 22.57804298400879\n",
      "[step: 992] loss: 22.20939826965332\n",
      "[step: 993] loss: 22.98995018005371\n",
      "[step: 994] loss: 22.444311141967773\n",
      "[step: 995] loss: 22.34373664855957\n",
      "[step: 996] loss: 22.625253677368164\n",
      "[step: 997] loss: 21.96881675720215\n",
      "[step: 998] loss: 22.25732421875\n",
      "[step: 999] loss: 23.022077560424805\n",
      "[step: 1000] loss: 22.819204330444336\n",
      "RMSE: 2.508105460405129\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6cffc59f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "def predict(file):\n",
    "    # train Parameters\n",
    "    seq_length = 6\n",
    "    data_dim = 1\n",
    "    hidden_dim = 12 # 내 맘대로 정해도 됨\n",
    "    output_dim = 1\n",
    "    learning_rate = 0.05\n",
    "    iterations = 1000\n",
    "    layer_num=3\n",
    "\n",
    "    # train Data: Open, High, Low, Volume, Close\n",
    "    origin_xy = np.loadtxt('train.csv', delimiter=',', skiprows=1, usecols=range(1,8))\n",
    "    xy = MinMaxScaler(origin_xy)\n",
    "    x = xy[:,:-1]\n",
    "    y = xy[:,-1]\n",
    "    x = x.reshape(-1,seq_length,data_dim)\n",
    "    y = y.reshape(-1,data_dim) \n",
    "\n",
    "    # train/validation split\n",
    "    train_size = int(len(y) * 0.7)\n",
    "    test_size = len(y) - train_size\n",
    "    trainX, validX = np.array(x[0:train_size]), np.array(x[train_size:])\n",
    "    trainY, validY = np.array(y[0:train_size]), np.array(y[train_size:])\n",
    "\n",
    "    #test data\n",
    "    test_x=np.loadtxt(file, delimiter=',', skiprows=1, usecols=range(1,7))\n",
    "    test_x = test_x.reshape(-1,seq_length, data_dim)\n",
    "    \n",
    "    # input place holders\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # build a LSTM network\n",
    "    cells=[]\n",
    "    for _ in range(layer_num):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.7)\n",
    "        cells.append(cell)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "    # cost/loss\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # RMSE\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "    sess=tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i+1, step_loss))\n",
    "\n",
    "    # validation step\n",
    "    valid_predict = sess.run(Y_pred, feed_dict={X: validX})\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validY, predictions: valid_predict})\n",
    "    revised_rmse = rmse_val*(origin_xy.max()-origin_xy.min()+1e-7) + origin_xy.min()\n",
    "    print(\"RMSE: {}\".format(revised_rmse))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(validY)\n",
    "    plt.plot(valid_predict)\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Temperature\")\n",
    "    plt.show()\n",
    "    \n",
    "    #test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: test_x})\n",
    "    prediction_list = []\n",
    "    for i in test_predict:\n",
    "        revised_pred = i[0]*(origin_xy.max()-origin_xy.min()+1e-7) + origin_xy.min()\n",
    "        prediction_list.append(revised_pred)\n",
    "    return prediction_list\n",
    "\n",
    "def write_result(predictions):\n",
    "    # You don't need to modify this function.\n",
    "    with open('result.csv', 'w') as f:\n",
    "        f.write('Value\\n')\n",
    "        for l in predictions:\n",
    "            f.write('{}\\n'.format(l))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # You don't need to modify this function.\n",
    "    predictions = predict('test.csv')\n",
    "    write_result(predictions)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # You don't need to modify this part.\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
