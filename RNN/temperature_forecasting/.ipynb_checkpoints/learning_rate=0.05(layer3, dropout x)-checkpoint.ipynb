{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to add any functions, import statements, and variables.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3279"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train data\n",
    "xy = np.loadtxt('train.csv', delimiter=',', skiprows=1, usecols=range(1,8))\n",
    "len(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    ''' Min Max Normalization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        input data to be normalized\n",
    "        shape: [Batch size, dimension]\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    data : numpy.ndarry\n",
    "        normalized data\n",
    "        shape: [Batch size, dimension]\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "\n",
    "    '''\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "def predict(file):\n",
    "    #test data\n",
    "    test_xy=np.loadtxt(file, delimiter=',', skiprows=1, usecols=range(1,7))\n",
    "    # Fill in this function. This function should return a list of length 52\n",
    "    #   which is filled with floating point numbers. For example, the current\n",
    "    #   implementation predicts all the instances in test.csv as 10.0.\n",
    "    return list([10.0 for _ in range(52)])\n",
    "\n",
    "\n",
    "def write_result(predictions):\n",
    "    # You don't need to modify this function.\n",
    "    with open('result.csv', 'w') as f:\n",
    "        f.write('Value\\n')\n",
    "        for l in predictions:\n",
    "            f.write('{}\\n'.format(l))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # You don't need to modify this function.\n",
    "    predictions = predict('test.csv')\n",
    "    write_result(predictions)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # You don't need to modify this part.\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Parameters\n",
    "seq_length = 6\n",
    "data_dim = 1\n",
    "hidden_dim = 12 # 내 맘대로 정해도 됨\n",
    "output_dim = 1\n",
    "learning_rate = 0.05\n",
    "iterations = 1000\n",
    "layer_num=3\n",
    "\n",
    "# train Data: Open, High, Low, Volume, Close\n",
    "origin_xy = np.loadtxt('train.csv', delimiter=',', skiprows=1, usecols=range(1,8))\n",
    "xy = MinMaxScaler(origin_xy)\n",
    "\n",
    "x = xy[:,:-1]\n",
    "y = xy[:,-1]\n",
    "\n",
    "#data reshape \n",
    "x = x.reshape(-1,6,1)\n",
    "y = y.reshape(-1,1)  # Close as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3279\n",
      "3279\n"
     ]
    }
   ],
   "source": [
    "print(len(x))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 397.0250244140625\n",
      "[step: 1] loss: 725.7177124023438\n",
      "[step: 2] loss: 175.51486206054688\n",
      "[step: 3] loss: 315.0476379394531\n",
      "[step: 4] loss: 244.97425842285156\n",
      "[step: 5] loss: 147.96180725097656\n",
      "[step: 6] loss: 76.06208801269531\n",
      "[step: 7] loss: 56.08314895629883\n",
      "[step: 8] loss: 94.60950469970703\n",
      "[step: 9] loss: 92.73951721191406\n",
      "[step: 10] loss: 67.453125\n",
      "[step: 11] loss: 55.25889205932617\n",
      "[step: 12] loss: 55.59984588623047\n",
      "[step: 13] loss: 60.01667785644531\n",
      "[step: 14] loss: 63.4661979675293\n",
      "[step: 15] loss: 64.27460479736328\n",
      "[step: 16] loss: 62.75430679321289\n",
      "[step: 17] loss: 60.06919860839844\n",
      "[step: 18] loss: 57.44149398803711\n",
      "[step: 19] loss: 55.712608337402344\n",
      "[step: 20] loss: 55.19747543334961\n",
      "[step: 21] loss: 55.73323440551758\n",
      "[step: 22] loss: 56.843048095703125\n",
      "[step: 23] loss: 57.95509719848633\n",
      "[step: 24] loss: 58.612178802490234\n",
      "[step: 25] loss: 58.60708999633789\n",
      "[step: 26] loss: 58.00448226928711\n",
      "[step: 27] loss: 57.060935974121094\n",
      "[step: 28] loss: 56.09524917602539\n",
      "[step: 29] loss: 55.36880111694336\n",
      "[step: 30] loss: 55.01443862915039\n",
      "[step: 31] loss: 55.02317428588867\n",
      "[step: 32] loss: 55.27661895751953\n",
      "[step: 33] loss: 55.604923248291016\n",
      "[step: 34] loss: 55.84804916381836\n",
      "[step: 35] loss: 55.90278244018555\n",
      "[step: 36] loss: 55.74462127685547\n",
      "[step: 37] loss: 55.42167282104492\n",
      "[step: 38] loss: 55.026824951171875\n",
      "[step: 39] loss: 54.660030364990234\n",
      "[step: 40] loss: 54.39365768432617\n",
      "[step: 41] loss: 54.25188446044922\n",
      "[step: 42] loss: 54.208465576171875\n",
      "[step: 43] loss: 54.201473236083984\n",
      "[step: 44] loss: 54.15825653076172\n",
      "[step: 45] loss: 54.02049255371094\n",
      "[step: 46] loss: 53.761592864990234\n",
      "[step: 47] loss: 53.389583587646484\n",
      "[step: 48] loss: 52.93760299682617\n",
      "[step: 49] loss: 52.44483184814453\n",
      "[step: 50] loss: 51.93628692626953\n",
      "[step: 51] loss: 51.4063835144043\n",
      "[step: 52] loss: 50.81053161621094\n",
      "[step: 53] loss: 50.06404495239258\n",
      "[step: 54] loss: 49.047019958496094\n",
      "[step: 55] loss: 47.61481857299805\n",
      "[step: 56] loss: 45.624473571777344\n",
      "[step: 57] loss: 43.01211166381836\n",
      "[step: 58] loss: 39.9335823059082\n",
      "[step: 59] loss: 36.79768371582031\n",
      "[step: 60] loss: 34.16387939453125\n",
      "[step: 61] loss: 32.78220748901367\n",
      "[step: 62] loss: 32.7365608215332\n",
      "[step: 63] loss: 33.42734146118164\n",
      "[step: 64] loss: 33.49341583251953\n",
      "[step: 65] loss: 32.23347473144531\n",
      "[step: 66] loss: 30.344301223754883\n",
      "[step: 67] loss: 29.290800094604492\n",
      "[step: 68] loss: 29.55803680419922\n",
      "[step: 69] loss: 30.511754989624023\n",
      "[step: 70] loss: 30.898359298706055\n",
      "[step: 71] loss: 30.45090103149414\n",
      "[step: 72] loss: 29.60283088684082\n",
      "[step: 73] loss: 28.9842529296875\n",
      "[step: 74] loss: 28.797893524169922\n",
      "[step: 75] loss: 28.973196029663086\n",
      "[step: 76] loss: 29.1562442779541\n",
      "[step: 77] loss: 29.0665340423584\n",
      "[step: 78] loss: 28.7238826751709\n",
      "[step: 79] loss: 28.269676208496094\n",
      "[step: 80] loss: 27.900196075439453\n",
      "[step: 81] loss: 27.719913482666016\n",
      "[step: 82] loss: 27.694120407104492\n",
      "[step: 83] loss: 27.7279109954834\n",
      "[step: 84] loss: 27.699636459350586\n",
      "[step: 85] loss: 27.561866760253906\n",
      "[step: 86] loss: 27.327871322631836\n",
      "[step: 87] loss: 27.037858963012695\n",
      "[step: 88] loss: 26.77587127685547\n",
      "[step: 89] loss: 26.583879470825195\n",
      "[step: 90] loss: 26.457300186157227\n",
      "[step: 91] loss: 26.363723754882812\n",
      "[step: 92] loss: 26.242721557617188\n",
      "[step: 93] loss: 26.052555084228516\n",
      "[step: 94] loss: 25.814258575439453\n",
      "[step: 95] loss: 25.578506469726562\n",
      "[step: 96] loss: 25.38312339782715\n",
      "[step: 97] loss: 25.24933624267578\n",
      "[step: 98] loss: 25.132038116455078\n",
      "[step: 99] loss: 24.98025131225586\n",
      "[step: 100] loss: 24.76498031616211\n",
      "[step: 101] loss: 24.52225685119629\n",
      "[step: 102] loss: 24.322166442871094\n",
      "[step: 103] loss: 24.205524444580078\n",
      "[step: 104] loss: 24.218141555786133\n",
      "[step: 105] loss: 24.506105422973633\n",
      "[step: 106] loss: 25.269651412963867\n",
      "[step: 107] loss: 24.396190643310547\n",
      "[step: 108] loss: 23.492069244384766\n",
      "[step: 109] loss: 23.942039489746094\n",
      "[step: 110] loss: 24.062002182006836\n",
      "[step: 111] loss: 23.349430084228516\n",
      "[step: 112] loss: 23.211389541625977\n",
      "[step: 113] loss: 23.509977340698242\n",
      "[step: 114] loss: 23.107301712036133\n",
      "[step: 115] loss: 22.796939849853516\n",
      "[step: 116] loss: 23.062570571899414\n",
      "[step: 117] loss: 22.769027709960938\n",
      "[step: 118] loss: 22.42399024963379\n",
      "[step: 119] loss: 22.629852294921875\n",
      "[step: 120] loss: 22.37774085998535\n",
      "[step: 121] loss: 22.097654342651367\n",
      "[step: 122] loss: 22.252307891845703\n",
      "[step: 123] loss: 22.001516342163086\n",
      "[step: 124] loss: 21.76304054260254\n",
      "[step: 125] loss: 21.890117645263672\n",
      "[step: 126] loss: 21.706951141357422\n",
      "[step: 127] loss: 21.466276168823242\n",
      "[step: 128] loss: 21.565366744995117\n",
      "[step: 129] loss: 21.51163673400879\n",
      "[step: 130] loss: 21.249605178833008\n",
      "[step: 131] loss: 21.250240325927734\n",
      "[step: 132] loss: 21.35981559753418\n",
      "[step: 133] loss: 21.24936294555664\n",
      "[step: 134] loss: 21.100038528442383\n",
      "[step: 135] loss: 21.09949493408203\n",
      "[step: 136] loss: 21.209400177001953\n",
      "[step: 137] loss: 21.309219360351562\n",
      "[step: 138] loss: 21.29665756225586\n",
      "[step: 139] loss: 21.2313175201416\n",
      "[step: 140] loss: 21.142793655395508\n",
      "[step: 141] loss: 21.101560592651367\n",
      "[step: 142] loss: 21.118549346923828\n",
      "[step: 143] loss: 21.15601921081543\n",
      "[step: 144] loss: 21.162948608398438\n",
      "[step: 145] loss: 21.109737396240234\n",
      "[step: 146] loss: 21.04900360107422\n",
      "[step: 147] loss: 21.031442642211914\n",
      "[step: 148] loss: 21.04505157470703\n",
      "[step: 149] loss: 21.03630256652832\n",
      "[step: 150] loss: 20.997692108154297\n",
      "[step: 151] loss: 20.98072624206543\n",
      "[step: 152] loss: 20.992258071899414\n",
      "[step: 153] loss: 20.988882064819336\n",
      "[step: 154] loss: 20.966800689697266\n",
      "[step: 155] loss: 20.959989547729492\n",
      "[step: 156] loss: 20.96861457824707\n",
      "[step: 157] loss: 20.96418571472168\n",
      "[step: 158] loss: 20.948104858398438\n",
      "[step: 159] loss: 20.94315528869629\n",
      "[step: 160] loss: 20.94705581665039\n",
      "[step: 161] loss: 20.942489624023438\n",
      "[step: 162] loss: 20.92970085144043\n",
      "[step: 163] loss: 20.920217514038086\n",
      "[step: 164] loss: 20.918325424194336\n",
      "[step: 165] loss: 20.91813850402832\n",
      "[step: 166] loss: 20.913524627685547\n",
      "[step: 167] loss: 20.90520477294922\n",
      "[step: 168] loss: 20.895801544189453\n",
      "[step: 169] loss: 20.88813591003418\n",
      "[step: 170] loss: 20.882678985595703\n",
      "[step: 171] loss: 20.879167556762695\n",
      "[step: 172] loss: 20.87746810913086\n",
      "[step: 173] loss: 20.878353118896484\n",
      "[step: 174] loss: 20.88549041748047\n",
      "[step: 175] loss: 20.907848358154297\n",
      "[step: 176] loss: 20.976478576660156\n",
      "[step: 177] loss: 21.165912628173828\n",
      "[step: 178] loss: 21.730548858642578\n",
      "[step: 179] loss: 22.85306739807129\n",
      "[step: 180] loss: 24.553884506225586\n",
      "[step: 181] loss: 22.692108154296875\n",
      "[step: 182] loss: 20.94171142578125\n",
      "[step: 183] loss: 22.206653594970703\n",
      "[step: 184] loss: 21.315746307373047\n",
      "[step: 185] loss: 21.347139358520508\n",
      "[step: 186] loss: 21.5963191986084\n",
      "[step: 187] loss: 21.049808502197266\n",
      "[step: 188] loss: 21.624210357666016\n",
      "[step: 189] loss: 20.968971252441406\n",
      "[step: 190] loss: 21.482892990112305\n",
      "[step: 191] loss: 20.96103858947754\n",
      "[step: 192] loss: 21.34810447692871\n",
      "[step: 193] loss: 20.953956604003906\n",
      "[step: 194] loss: 21.231048583984375\n",
      "[step: 195] loss: 20.936120986938477\n",
      "[step: 196] loss: 21.16368293762207\n",
      "[step: 197] loss: 20.917024612426758\n",
      "[step: 198] loss: 21.086410522460938\n",
      "[step: 199] loss: 20.885385513305664\n",
      "[step: 200] loss: 21.032859802246094\n",
      "[step: 201] loss: 20.880701065063477\n",
      "[step: 202] loss: 20.989532470703125\n",
      "[step: 203] loss: 20.881284713745117\n",
      "[step: 204] loss: 20.951990127563477\n",
      "[step: 205] loss: 20.884798049926758\n",
      "[step: 206] loss: 20.916933059692383\n",
      "[step: 207] loss: 20.88510513305664\n",
      "[step: 208] loss: 20.89234733581543\n",
      "[step: 209] loss: 20.88726043701172\n",
      "[step: 210] loss: 20.871524810791016\n",
      "[step: 211] loss: 20.87993049621582\n",
      "[step: 212] loss: 20.847875595092773\n",
      "[step: 213] loss: 20.86994743347168\n",
      "[step: 214] loss: 20.831289291381836\n",
      "[step: 215] loss: 20.85993766784668\n",
      "[step: 216] loss: 20.818696975708008\n",
      "[step: 217] loss: 20.8452091217041\n",
      "[step: 218] loss: 20.809207916259766\n",
      "[step: 219] loss: 20.82985496520996\n",
      "[step: 220] loss: 20.804031372070312\n",
      "[step: 221] loss: 20.816150665283203\n",
      "[step: 222] loss: 20.80080795288086\n",
      "[step: 223] loss: 20.803417205810547\n",
      "[step: 224] loss: 20.797161102294922\n",
      "[step: 225] loss: 20.791465759277344\n",
      "[step: 226] loss: 20.792890548706055\n",
      "[step: 227] loss: 20.78171730041504\n",
      "[step: 228] loss: 20.787504196166992\n",
      "[step: 229] loss: 20.774181365966797\n",
      "[step: 230] loss: 20.779382705688477\n",
      "[step: 231] loss: 20.768362045288086\n",
      "[step: 232] loss: 20.769664764404297\n",
      "[step: 233] loss: 20.764188766479492\n",
      "[step: 234] loss: 20.76032066345215\n",
      "[step: 235] loss: 20.75983238220215\n",
      "[step: 236] loss: 20.75237464904785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 237] loss: 20.753753662109375\n",
      "[step: 238] loss: 20.746929168701172\n",
      "[step: 239] loss: 20.746294021606445\n",
      "[step: 240] loss: 20.74319839477539\n",
      "[step: 241] loss: 20.738859176635742\n",
      "[step: 242] loss: 20.738515853881836\n",
      "[step: 243] loss: 20.733169555664062\n",
      "[step: 244] loss: 20.732051849365234\n",
      "[step: 245] loss: 20.729074478149414\n",
      "[step: 246] loss: 20.7253360748291\n",
      "[step: 247] loss: 20.724185943603516\n",
      "[step: 248] loss: 20.71995735168457\n",
      "[step: 249] loss: 20.71782112121582\n",
      "[step: 250] loss: 20.71550178527832\n",
      "[step: 251] loss: 20.711782455444336\n",
      "[step: 252] loss: 20.7100887298584\n",
      "[step: 253] loss: 20.706981658935547\n",
      "[step: 254] loss: 20.703989028930664\n",
      "[step: 255] loss: 20.702085494995117\n",
      "[step: 256] loss: 20.698856353759766\n",
      "[step: 257] loss: 20.69636344909668\n",
      "[step: 258] loss: 20.694129943847656\n",
      "[step: 259] loss: 20.690996170043945\n",
      "[step: 260] loss: 20.688663482666016\n",
      "[step: 261] loss: 20.68624496459961\n",
      "[step: 262] loss: 20.683265686035156\n",
      "[step: 263] loss: 20.680925369262695\n",
      "[step: 264] loss: 20.678415298461914\n",
      "[step: 265] loss: 20.675561904907227\n",
      "[step: 266] loss: 20.67318344116211\n",
      "[step: 267] loss: 20.67070198059082\n",
      "[step: 268] loss: 20.667932510375977\n",
      "[step: 269] loss: 20.66549301147461\n",
      "[step: 270] loss: 20.663055419921875\n",
      "[step: 271] loss: 20.66037368774414\n",
      "[step: 272] loss: 20.657896041870117\n",
      "[step: 273] loss: 20.655500411987305\n",
      "[step: 274] loss: 20.65291976928711\n",
      "[step: 275] loss: 20.65038299560547\n",
      "[step: 276] loss: 20.648008346557617\n",
      "[step: 277] loss: 20.645532608032227\n",
      "[step: 278] loss: 20.64299774169922\n",
      "[step: 279] loss: 20.640579223632812\n",
      "[step: 280] loss: 20.638185501098633\n",
      "[step: 281] loss: 20.635717391967773\n",
      "[step: 282] loss: 20.633262634277344\n",
      "[step: 283] loss: 20.630874633789062\n",
      "[step: 284] loss: 20.628503799438477\n",
      "[step: 285] loss: 20.626081466674805\n",
      "[step: 286] loss: 20.62367820739746\n",
      "[step: 287] loss: 20.621339797973633\n",
      "[step: 288] loss: 20.61898422241211\n",
      "[step: 289] loss: 20.616622924804688\n",
      "[step: 290] loss: 20.6142635345459\n",
      "[step: 291] loss: 20.611940383911133\n",
      "[step: 292] loss: 20.609636306762695\n",
      "[step: 293] loss: 20.607318878173828\n",
      "[step: 294] loss: 20.604999542236328\n",
      "[step: 295] loss: 20.602697372436523\n",
      "[step: 296] loss: 20.60041618347168\n",
      "[step: 297] loss: 20.5981502532959\n",
      "[step: 298] loss: 20.59587287902832\n",
      "[step: 299] loss: 20.59359359741211\n",
      "[step: 300] loss: 20.59133529663086\n",
      "[step: 301] loss: 20.589080810546875\n",
      "[step: 302] loss: 20.586837768554688\n",
      "[step: 303] loss: 20.584619522094727\n",
      "[step: 304] loss: 20.582395553588867\n",
      "[step: 305] loss: 20.58016014099121\n",
      "[step: 306] loss: 20.577939987182617\n",
      "[step: 307] loss: 20.575721740722656\n",
      "[step: 308] loss: 20.573518753051758\n",
      "[step: 309] loss: 20.57132339477539\n",
      "[step: 310] loss: 20.569129943847656\n",
      "[step: 311] loss: 20.566946029663086\n",
      "[step: 312] loss: 20.56476402282715\n",
      "[step: 313] loss: 20.562599182128906\n",
      "[step: 314] loss: 20.560428619384766\n",
      "[step: 315] loss: 20.558265686035156\n",
      "[step: 316] loss: 20.55612564086914\n",
      "[step: 317] loss: 20.554006576538086\n",
      "[step: 318] loss: 20.551939010620117\n",
      "[step: 319] loss: 20.549978256225586\n",
      "[step: 320] loss: 20.548267364501953\n",
      "[step: 321] loss: 20.54720687866211\n",
      "[step: 322] loss: 20.547788619995117\n",
      "[step: 323] loss: 20.553062438964844\n",
      "[step: 324] loss: 20.57124900817871\n",
      "[step: 325] loss: 20.629863739013672\n",
      "[step: 326] loss: 20.800935745239258\n",
      "[step: 327] loss: 21.34000587463379\n",
      "[step: 328] loss: 22.54193115234375\n",
      "[step: 329] loss: 24.94878387451172\n",
      "[step: 330] loss: 23.382020950317383\n",
      "[step: 331] loss: 20.845951080322266\n",
      "[step: 332] loss: 22.022750854492188\n",
      "[step: 333] loss: 21.238162994384766\n",
      "[step: 334] loss: 21.12618637084961\n",
      "[step: 335] loss: 21.534982681274414\n",
      "[step: 336] loss: 20.8842716217041\n",
      "[step: 337] loss: 21.570545196533203\n",
      "[step: 338] loss: 20.750377655029297\n",
      "[step: 339] loss: 21.319883346557617\n",
      "[step: 340] loss: 20.731266021728516\n",
      "[step: 341] loss: 21.199308395385742\n",
      "[step: 342] loss: 20.726314544677734\n",
      "[step: 343] loss: 21.053237915039062\n",
      "[step: 344] loss: 20.677988052368164\n",
      "[step: 345] loss: 20.94765281677246\n",
      "[step: 346] loss: 20.64805030822754\n",
      "[step: 347] loss: 20.88746452331543\n",
      "[step: 348] loss: 20.64731788635254\n",
      "[step: 349] loss: 20.838903427124023\n",
      "[step: 350] loss: 20.626977920532227\n",
      "[step: 351] loss: 20.787403106689453\n",
      "[step: 352] loss: 20.63304328918457\n",
      "[step: 353] loss: 20.760828018188477\n",
      "[step: 354] loss: 20.620447158813477\n",
      "[step: 355] loss: 20.713090896606445\n",
      "[step: 356] loss: 20.598224639892578\n",
      "[step: 357] loss: 20.679658889770508\n",
      "[step: 358] loss: 20.584800720214844\n",
      "[step: 359] loss: 20.647552490234375\n",
      "[step: 360] loss: 20.565303802490234\n",
      "[step: 361] loss: 20.616615295410156\n",
      "[step: 362] loss: 20.55409812927246\n",
      "[step: 363] loss: 20.59844970703125\n",
      "[step: 364] loss: 20.548446655273438\n",
      "[step: 365] loss: 20.581785202026367\n",
      "[step: 366] loss: 20.53950309753418\n",
      "[step: 367] loss: 20.5690860748291\n",
      "[step: 368] loss: 20.535953521728516\n",
      "[step: 369] loss: 20.559160232543945\n",
      "[step: 370] loss: 20.530290603637695\n",
      "[step: 371] loss: 20.54618263244629\n",
      "[step: 372] loss: 20.525476455688477\n",
      "[step: 373] loss: 20.537405014038086\n",
      "[step: 374] loss: 20.522361755371094\n",
      "[step: 375] loss: 20.527223587036133\n",
      "[step: 376] loss: 20.517868041992188\n",
      "[step: 377] loss: 20.518054962158203\n",
      "[step: 378] loss: 20.51582908630371\n",
      "[step: 379] loss: 20.510101318359375\n",
      "[step: 380] loss: 20.511682510375977\n",
      "[step: 381] loss: 20.502500534057617\n",
      "[step: 382] loss: 20.506946563720703\n",
      "[step: 383] loss: 20.497610092163086\n",
      "[step: 384] loss: 20.50092124938965\n",
      "[step: 385] loss: 20.49298667907715\n",
      "[step: 386] loss: 20.49395179748535\n",
      "[step: 387] loss: 20.489736557006836\n",
      "[step: 388] loss: 20.48737335205078\n",
      "[step: 389] loss: 20.48598289489746\n",
      "[step: 390] loss: 20.4812068939209\n",
      "[step: 391] loss: 20.481739044189453\n",
      "[step: 392] loss: 20.47676658630371\n",
      "[step: 393] loss: 20.47686004638672\n",
      "[step: 394] loss: 20.47295379638672\n",
      "[step: 395] loss: 20.471637725830078\n",
      "[step: 396] loss: 20.469852447509766\n",
      "[step: 397] loss: 20.46695899963379\n",
      "[step: 398] loss: 20.4661922454834\n",
      "[step: 399] loss: 20.462797164916992\n",
      "[step: 400] loss: 20.461999893188477\n",
      "[step: 401] loss: 20.459409713745117\n",
      "[step: 402] loss: 20.45748519897461\n",
      "[step: 403] loss: 20.4559326171875\n",
      "[step: 404] loss: 20.453292846679688\n",
      "[step: 405] loss: 20.452178955078125\n",
      "[step: 406] loss: 20.44972801208496\n",
      "[step: 407] loss: 20.447996139526367\n",
      "[step: 408] loss: 20.446340560913086\n",
      "[step: 409] loss: 20.444063186645508\n",
      "[step: 410] loss: 20.442684173583984\n",
      "[step: 411] loss: 20.440542221069336\n",
      "[step: 412] loss: 20.438724517822266\n",
      "[step: 413] loss: 20.437171936035156\n",
      "[step: 414] loss: 20.4350643157959\n",
      "[step: 415] loss: 20.433502197265625\n",
      "[step: 416] loss: 20.431703567504883\n",
      "[step: 417] loss: 20.4298152923584\n",
      "[step: 418] loss: 20.428302764892578\n",
      "[step: 419] loss: 20.42641830444336\n",
      "[step: 420] loss: 20.4246768951416\n",
      "[step: 421] loss: 20.423099517822266\n",
      "[step: 422] loss: 20.421255111694336\n",
      "[step: 423] loss: 20.419580459594727\n",
      "[step: 424] loss: 20.41796112060547\n",
      "[step: 425] loss: 20.416189193725586\n",
      "[step: 426] loss: 20.4145565032959\n",
      "[step: 427] loss: 20.412925720214844\n",
      "[step: 428] loss: 20.411203384399414\n",
      "[step: 429] loss: 20.40959358215332\n",
      "[step: 430] loss: 20.407970428466797\n",
      "[step: 431] loss: 20.406269073486328\n",
      "[step: 432] loss: 20.404666900634766\n",
      "[step: 433] loss: 20.403076171875\n",
      "[step: 434] loss: 20.40140724182129\n",
      "[step: 435] loss: 20.399799346923828\n",
      "[step: 436] loss: 20.398229598999023\n",
      "[step: 437] loss: 20.396608352661133\n",
      "[step: 438] loss: 20.395009994506836\n",
      "[step: 439] loss: 20.39344596862793\n",
      "[step: 440] loss: 20.391870498657227\n",
      "[step: 441] loss: 20.390287399291992\n",
      "[step: 442] loss: 20.38874626159668\n",
      "[step: 443] loss: 20.387197494506836\n",
      "[step: 444] loss: 20.385644912719727\n",
      "[step: 445] loss: 20.384105682373047\n",
      "[step: 446] loss: 20.38258934020996\n",
      "[step: 447] loss: 20.38105583190918\n",
      "[step: 448] loss: 20.379539489746094\n",
      "[step: 449] loss: 20.378047943115234\n",
      "[step: 450] loss: 20.37654685974121\n",
      "[step: 451] loss: 20.375051498413086\n",
      "[step: 452] loss: 20.373571395874023\n",
      "[step: 453] loss: 20.37209701538086\n",
      "[step: 454] loss: 20.370635986328125\n",
      "[step: 455] loss: 20.369176864624023\n",
      "[step: 456] loss: 20.36772346496582\n",
      "[step: 457] loss: 20.366287231445312\n",
      "[step: 458] loss: 20.364858627319336\n",
      "[step: 459] loss: 20.363426208496094\n",
      "[step: 460] loss: 20.362001419067383\n",
      "[step: 461] loss: 20.360599517822266\n",
      "[step: 462] loss: 20.359207153320312\n",
      "[step: 463] loss: 20.357812881469727\n",
      "[step: 464] loss: 20.356412887573242\n",
      "[step: 465] loss: 20.355037689208984\n",
      "[step: 466] loss: 20.35366439819336\n",
      "[step: 467] loss: 20.35230255126953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 468] loss: 20.3509464263916\n",
      "[step: 469] loss: 20.349599838256836\n",
      "[step: 470] loss: 20.3482608795166\n",
      "[step: 471] loss: 20.3469181060791\n",
      "[step: 472] loss: 20.3455867767334\n",
      "[step: 473] loss: 20.344270706176758\n",
      "[step: 474] loss: 20.342960357666016\n",
      "[step: 475] loss: 20.341638565063477\n",
      "[step: 476] loss: 20.34034538269043\n",
      "[step: 477] loss: 20.339046478271484\n",
      "[step: 478] loss: 20.33775520324707\n",
      "[step: 479] loss: 20.336463928222656\n",
      "[step: 480] loss: 20.33518409729004\n",
      "[step: 481] loss: 20.333911895751953\n",
      "[step: 482] loss: 20.33264923095703\n",
      "[step: 483] loss: 20.331384658813477\n",
      "[step: 484] loss: 20.330123901367188\n",
      "[step: 485] loss: 20.328880310058594\n",
      "[step: 486] loss: 20.327634811401367\n",
      "[step: 487] loss: 20.326393127441406\n",
      "[step: 488] loss: 20.325157165527344\n",
      "[step: 489] loss: 20.323930740356445\n",
      "[step: 490] loss: 20.322694778442383\n",
      "[step: 491] loss: 20.321483612060547\n",
      "[step: 492] loss: 20.32026481628418\n",
      "[step: 493] loss: 20.31905746459961\n",
      "[step: 494] loss: 20.31784439086914\n",
      "[step: 495] loss: 20.3166446685791\n",
      "[step: 496] loss: 20.31544303894043\n",
      "[step: 497] loss: 20.314260482788086\n",
      "[step: 498] loss: 20.313081741333008\n",
      "[step: 499] loss: 20.311891555786133\n",
      "[step: 500] loss: 20.31070899963379\n",
      "[step: 501] loss: 20.30953025817871\n",
      "[step: 502] loss: 20.308372497558594\n",
      "[step: 503] loss: 20.307199478149414\n",
      "[step: 504] loss: 20.30604362487793\n",
      "[step: 505] loss: 20.304880142211914\n",
      "[step: 506] loss: 20.30373764038086\n",
      "[step: 507] loss: 20.302579879760742\n",
      "[step: 508] loss: 20.30143928527832\n",
      "[step: 509] loss: 20.300304412841797\n",
      "[step: 510] loss: 20.299179077148438\n",
      "[step: 511] loss: 20.298046112060547\n",
      "[step: 512] loss: 20.29692268371582\n",
      "[step: 513] loss: 20.295801162719727\n",
      "[step: 514] loss: 20.294679641723633\n",
      "[step: 515] loss: 20.293575286865234\n",
      "[step: 516] loss: 20.292465209960938\n",
      "[step: 517] loss: 20.291366577148438\n",
      "[step: 518] loss: 20.290264129638672\n",
      "[step: 519] loss: 20.2891788482666\n",
      "[step: 520] loss: 20.288084030151367\n",
      "[step: 521] loss: 20.287002563476562\n",
      "[step: 522] loss: 20.285930633544922\n",
      "[step: 523] loss: 20.284860610961914\n",
      "[step: 524] loss: 20.283782958984375\n",
      "[step: 525] loss: 20.282724380493164\n",
      "[step: 526] loss: 20.28167152404785\n",
      "[step: 527] loss: 20.280620574951172\n",
      "[step: 528] loss: 20.279571533203125\n",
      "[step: 529] loss: 20.27853012084961\n",
      "[step: 530] loss: 20.27748680114746\n",
      "[step: 531] loss: 20.276460647583008\n",
      "[step: 532] loss: 20.27543067932129\n",
      "[step: 533] loss: 20.2744083404541\n",
      "[step: 534] loss: 20.27338981628418\n",
      "[step: 535] loss: 20.27237892150879\n",
      "[step: 536] loss: 20.271377563476562\n",
      "[step: 537] loss: 20.2703800201416\n",
      "[step: 538] loss: 20.269399642944336\n",
      "[step: 539] loss: 20.268428802490234\n",
      "[step: 540] loss: 20.267499923706055\n",
      "[step: 541] loss: 20.266620635986328\n",
      "[step: 542] loss: 20.265853881835938\n",
      "[step: 543] loss: 20.265357971191406\n",
      "[step: 544] loss: 20.26540756225586\n",
      "[step: 545] loss: 20.266738891601562\n",
      "[step: 546] loss: 20.270902633666992\n",
      "[step: 547] loss: 20.281888961791992\n",
      "[step: 548] loss: 20.30805015563965\n",
      "[step: 549] loss: 20.37193489074707\n",
      "[step: 550] loss: 20.511199951171875\n",
      "[step: 551] loss: 20.827510833740234\n",
      "[step: 552] loss: 21.302396774291992\n",
      "[step: 553] loss: 21.872159957885742\n",
      "[step: 554] loss: 21.37928009033203\n",
      "[step: 555] loss: 20.45943260192871\n",
      "[step: 556] loss: 20.592220306396484\n",
      "[step: 557] loss: 21.044086456298828\n",
      "[step: 558] loss: 20.561893463134766\n",
      "[step: 559] loss: 20.431806564331055\n",
      "[step: 560] loss: 20.758731842041016\n",
      "[step: 561] loss: 20.409194946289062\n",
      "[step: 562] loss: 20.45610237121582\n",
      "[step: 563] loss: 20.56892967224121\n",
      "[step: 564] loss: 20.295429229736328\n",
      "[step: 565] loss: 20.435691833496094\n",
      "[step: 566] loss: 20.469205856323242\n",
      "[step: 567] loss: 20.284698486328125\n",
      "[step: 568] loss: 20.411924362182617\n",
      "[step: 569] loss: 20.380380630493164\n",
      "[step: 570] loss: 20.272968292236328\n",
      "[step: 571] loss: 20.375179290771484\n",
      "[step: 572] loss: 20.31174659729004\n",
      "[step: 573] loss: 20.27097511291504\n",
      "[step: 574] loss: 20.336923599243164\n",
      "[step: 575] loss: 20.26795768737793\n",
      "[step: 576] loss: 20.301218032836914\n",
      "[step: 577] loss: 20.29899024963379\n",
      "[step: 578] loss: 20.263790130615234\n",
      "[step: 579] loss: 20.30164909362793\n",
      "[step: 580] loss: 20.25940704345703\n",
      "[step: 581] loss: 20.267908096313477\n",
      "[step: 582] loss: 20.27486801147461\n",
      "[step: 583] loss: 20.243513107299805\n",
      "[step: 584] loss: 20.268512725830078\n",
      "[step: 585] loss: 20.255619049072266\n",
      "[step: 586] loss: 20.243635177612305\n",
      "[step: 587] loss: 20.2603816986084\n",
      "[step: 588] loss: 20.241689682006836\n",
      "[step: 589] loss: 20.243675231933594\n",
      "[step: 590] loss: 20.24815559387207\n",
      "[step: 591] loss: 20.23416519165039\n",
      "[step: 592] loss: 20.24285125732422\n",
      "[step: 593] loss: 20.23860740661621\n",
      "[step: 594] loss: 20.23269271850586\n",
      "[step: 595] loss: 20.2395076751709\n",
      "[step: 596] loss: 20.23064613342285\n",
      "[step: 597] loss: 20.23069953918457\n",
      "[step: 598] loss: 20.232633590698242\n",
      "[step: 599] loss: 20.224946975708008\n",
      "[step: 600] loss: 20.22755241394043\n",
      "[step: 601] loss: 20.226648330688477\n",
      "[step: 602] loss: 20.22191619873047\n",
      "[step: 603] loss: 20.224529266357422\n",
      "[step: 604] loss: 20.222078323364258\n",
      "[step: 605] loss: 20.21902084350586\n",
      "[step: 606] loss: 20.220500946044922\n",
      "[step: 607] loss: 20.217615127563477\n",
      "[step: 608] loss: 20.21592903137207\n",
      "[step: 609] loss: 20.21662139892578\n",
      "[step: 610] loss: 20.213937759399414\n",
      "[step: 611] loss: 20.213075637817383\n",
      "[step: 612] loss: 20.21297264099121\n",
      "[step: 613] loss: 20.210514068603516\n",
      "[step: 614] loss: 20.209924697875977\n",
      "[step: 615] loss: 20.20929527282715\n",
      "[step: 616] loss: 20.207250595092773\n",
      "[step: 617] loss: 20.206819534301758\n",
      "[step: 618] loss: 20.206022262573242\n",
      "[step: 619] loss: 20.204301834106445\n",
      "[step: 620] loss: 20.20380210876465\n",
      "[step: 621] loss: 20.202836990356445\n",
      "[step: 622] loss: 20.201284408569336\n",
      "[step: 623] loss: 20.200645446777344\n",
      "[step: 624] loss: 20.199644088745117\n",
      "[step: 625] loss: 20.19825553894043\n",
      "[step: 626] loss: 20.197551727294922\n",
      "[step: 627] loss: 20.196565628051758\n",
      "[step: 628] loss: 20.19529151916504\n",
      "[step: 629] loss: 20.19449234008789\n",
      "[step: 630] loss: 20.193504333496094\n",
      "[step: 631] loss: 20.19228744506836\n",
      "[step: 632] loss: 20.191434860229492\n",
      "[step: 633] loss: 20.190479278564453\n",
      "[step: 634] loss: 20.189334869384766\n",
      "[step: 635] loss: 20.188430786132812\n",
      "[step: 636] loss: 20.1875\n",
      "[step: 637] loss: 20.1864013671875\n",
      "[step: 638] loss: 20.185440063476562\n",
      "[step: 639] loss: 20.18453025817871\n",
      "[step: 640] loss: 20.183483123779297\n",
      "[step: 641] loss: 20.182491302490234\n",
      "[step: 642] loss: 20.181596755981445\n",
      "[step: 643] loss: 20.180593490600586\n",
      "[step: 644] loss: 20.179595947265625\n",
      "[step: 645] loss: 20.178682327270508\n",
      "[step: 646] loss: 20.17772102355957\n",
      "[step: 647] loss: 20.17671775817871\n",
      "[step: 648] loss: 20.175779342651367\n",
      "[step: 649] loss: 20.174833297729492\n",
      "[step: 650] loss: 20.173851013183594\n",
      "[step: 651] loss: 20.172895431518555\n",
      "[step: 652] loss: 20.17197036743164\n",
      "[step: 653] loss: 20.170997619628906\n",
      "[step: 654] loss: 20.170024871826172\n",
      "[step: 655] loss: 20.169092178344727\n",
      "[step: 656] loss: 20.168142318725586\n",
      "[step: 657] loss: 20.16718101501465\n",
      "[step: 658] loss: 20.16623306274414\n",
      "[step: 659] loss: 20.16529083251953\n",
      "[step: 660] loss: 20.164344787597656\n",
      "[step: 661] loss: 20.16338539123535\n",
      "[step: 662] loss: 20.162446975708008\n",
      "[step: 663] loss: 20.1615047454834\n",
      "[step: 664] loss: 20.160552978515625\n",
      "[step: 665] loss: 20.159605026245117\n",
      "[step: 666] loss: 20.15867042541504\n",
      "[step: 667] loss: 20.157726287841797\n",
      "[step: 668] loss: 20.156776428222656\n",
      "[step: 669] loss: 20.155838012695312\n",
      "[step: 670] loss: 20.15489959716797\n",
      "[step: 671] loss: 20.153959274291992\n",
      "[step: 672] loss: 20.15301513671875\n",
      "[step: 673] loss: 20.152082443237305\n",
      "[step: 674] loss: 20.151147842407227\n",
      "[step: 675] loss: 20.150209426879883\n",
      "[step: 676] loss: 20.14927101135254\n",
      "[step: 677] loss: 20.148351669311523\n",
      "[step: 678] loss: 20.147411346435547\n",
      "[step: 679] loss: 20.146486282348633\n",
      "[step: 680] loss: 20.14555549621582\n",
      "[step: 681] loss: 20.14461898803711\n",
      "[step: 682] loss: 20.143693923950195\n",
      "[step: 683] loss: 20.14276695251465\n",
      "[step: 684] loss: 20.1418399810791\n",
      "[step: 685] loss: 20.140918731689453\n",
      "[step: 686] loss: 20.139989852905273\n",
      "[step: 687] loss: 20.139074325561523\n",
      "[step: 688] loss: 20.138151168823242\n",
      "[step: 689] loss: 20.13722801208496\n",
      "[step: 690] loss: 20.136310577392578\n",
      "[step: 691] loss: 20.13539695739746\n",
      "[step: 692] loss: 20.13448143005371\n",
      "[step: 693] loss: 20.13356590270996\n",
      "[step: 694] loss: 20.13265037536621\n",
      "[step: 695] loss: 20.131736755371094\n",
      "[step: 696] loss: 20.130828857421875\n",
      "[step: 697] loss: 20.12991714477539\n",
      "[step: 698] loss: 20.129009246826172\n",
      "[step: 699] loss: 20.128108978271484\n",
      "[step: 700] loss: 20.127193450927734\n",
      "[step: 701] loss: 20.12629508972168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 702] loss: 20.12538719177246\n",
      "[step: 703] loss: 20.124473571777344\n",
      "[step: 704] loss: 20.123580932617188\n",
      "[step: 705] loss: 20.122676849365234\n",
      "[step: 706] loss: 20.121782302856445\n",
      "[step: 707] loss: 20.120874404907227\n",
      "[step: 708] loss: 20.119983673095703\n",
      "[step: 709] loss: 20.119083404541016\n",
      "[step: 710] loss: 20.118186950683594\n",
      "[step: 711] loss: 20.117294311523438\n",
      "[step: 712] loss: 20.116403579711914\n",
      "[step: 713] loss: 20.115509033203125\n",
      "[step: 714] loss: 20.11461067199707\n",
      "[step: 715] loss: 20.113727569580078\n",
      "[step: 716] loss: 20.112829208374023\n",
      "[step: 717] loss: 20.111949920654297\n",
      "[step: 718] loss: 20.11104965209961\n",
      "[step: 719] loss: 20.11016273498535\n",
      "[step: 720] loss: 20.109275817871094\n",
      "[step: 721] loss: 20.10838508605957\n",
      "[step: 722] loss: 20.107494354248047\n",
      "[step: 723] loss: 20.106609344482422\n",
      "[step: 724] loss: 20.105724334716797\n",
      "[step: 725] loss: 20.104841232299805\n",
      "[step: 726] loss: 20.103958129882812\n",
      "[step: 727] loss: 20.103069305419922\n",
      "[step: 728] loss: 20.10219383239746\n",
      "[step: 729] loss: 20.101303100585938\n",
      "[step: 730] loss: 20.100418090820312\n",
      "[step: 731] loss: 20.09953498840332\n",
      "[step: 732] loss: 20.09864616394043\n",
      "[step: 733] loss: 20.097761154174805\n",
      "[step: 734] loss: 20.09687042236328\n",
      "[step: 735] loss: 20.095989227294922\n",
      "[step: 736] loss: 20.095109939575195\n",
      "[step: 737] loss: 20.094213485717773\n",
      "[step: 738] loss: 20.093334197998047\n",
      "[step: 739] loss: 20.092449188232422\n",
      "[step: 740] loss: 20.091562271118164\n",
      "[step: 741] loss: 20.09067153930664\n",
      "[step: 742] loss: 20.089794158935547\n",
      "[step: 743] loss: 20.08890151977539\n",
      "[step: 744] loss: 20.08800506591797\n",
      "[step: 745] loss: 20.087114334106445\n",
      "[step: 746] loss: 20.08622169494629\n",
      "[step: 747] loss: 20.085325241088867\n",
      "[step: 748] loss: 20.084428787231445\n",
      "[step: 749] loss: 20.08353614807129\n",
      "[step: 750] loss: 20.0826473236084\n",
      "[step: 751] loss: 20.08173942565918\n",
      "[step: 752] loss: 20.08083724975586\n",
      "[step: 753] loss: 20.079933166503906\n",
      "[step: 754] loss: 20.07902717590332\n",
      "[step: 755] loss: 20.078125\n",
      "[step: 756] loss: 20.07720947265625\n",
      "[step: 757] loss: 20.076295852661133\n",
      "[step: 758] loss: 20.075382232666016\n",
      "[step: 759] loss: 20.0744686126709\n",
      "[step: 760] loss: 20.073543548583984\n",
      "[step: 761] loss: 20.07262420654297\n",
      "[step: 762] loss: 20.071704864501953\n",
      "[step: 763] loss: 20.070775985717773\n",
      "[step: 764] loss: 20.069847106933594\n",
      "[step: 765] loss: 20.068912506103516\n",
      "[step: 766] loss: 20.0679874420166\n",
      "[step: 767] loss: 20.067073822021484\n",
      "[step: 768] loss: 20.066184997558594\n",
      "[step: 769] loss: 20.065364837646484\n",
      "[step: 770] loss: 20.064693450927734\n",
      "[step: 771] loss: 20.06439781188965\n",
      "[step: 772] loss: 20.06494140625\n",
      "[step: 773] loss: 20.067527770996094\n",
      "[step: 774] loss: 20.075191497802734\n",
      "[step: 775] loss: 20.094743728637695\n",
      "[step: 776] loss: 20.145824432373047\n",
      "[step: 777] loss: 20.26502227783203\n",
      "[step: 778] loss: 20.56882667541504\n",
      "[step: 779] loss: 21.10527992248535\n",
      "[step: 780] loss: 22.087142944335938\n",
      "[step: 781] loss: 21.914512634277344\n",
      "[step: 782] loss: 20.86608123779297\n",
      "[step: 783] loss: 20.213973999023438\n",
      "[step: 784] loss: 21.250648498535156\n",
      "[step: 785] loss: 21.310022354125977\n",
      "[step: 786] loss: 20.238046646118164\n",
      "[step: 787] loss: 20.997879028320312\n",
      "[step: 788] loss: 20.727863311767578\n",
      "[step: 789] loss: 20.180538177490234\n",
      "[step: 790] loss: 20.991119384765625\n",
      "[step: 791] loss: 20.443117141723633\n",
      "[step: 792] loss: 20.287967681884766\n",
      "[step: 793] loss: 20.701417922973633\n",
      "[step: 794] loss: 20.137380599975586\n",
      "[step: 795] loss: 20.42205047607422\n",
      "[step: 796] loss: 20.402956008911133\n",
      "[step: 797] loss: 20.134021759033203\n",
      "[step: 798] loss: 20.410633087158203\n",
      "[step: 799] loss: 20.101118087768555\n",
      "[step: 800] loss: 20.32172966003418\n",
      "[step: 801] loss: 20.1942081451416\n",
      "[step: 802] loss: 20.162137985229492\n",
      "[step: 803] loss: 20.238656997680664\n",
      "[step: 804] loss: 20.099205017089844\n",
      "[step: 805] loss: 20.227584838867188\n",
      "[step: 806] loss: 20.08474349975586\n",
      "[step: 807] loss: 20.177364349365234\n",
      "[step: 808] loss: 20.117258071899414\n",
      "[step: 809] loss: 20.113737106323242\n",
      "[step: 810] loss: 20.142200469970703\n",
      "[step: 811] loss: 20.06852149963379\n",
      "[step: 812] loss: 20.14059066772461\n",
      "[step: 813] loss: 20.067462921142578\n",
      "[step: 814] loss: 20.10164451599121\n",
      "[step: 815] loss: 20.086233139038086\n",
      "[step: 816] loss: 20.068523406982422\n",
      "[step: 817] loss: 20.094688415527344\n",
      "[step: 818] loss: 20.052640914916992\n",
      "[step: 819] loss: 20.08190155029297\n",
      "[step: 820] loss: 20.05430793762207\n",
      "[step: 821] loss: 20.06290626525879\n",
      "[step: 822] loss: 20.060203552246094\n",
      "[step: 823] loss: 20.045734405517578\n",
      "[step: 824] loss: 20.06114959716797\n",
      "[step: 825] loss: 20.03933334350586\n",
      "[step: 826] loss: 20.050369262695312\n",
      "[step: 827] loss: 20.041732788085938\n",
      "[step: 828] loss: 20.036333084106445\n",
      "[step: 829] loss: 20.043872833251953\n",
      "[step: 830] loss: 20.029529571533203\n",
      "[step: 831] loss: 20.037378311157227\n",
      "[step: 832] loss: 20.030250549316406\n",
      "[step: 833] loss: 20.027175903320312\n",
      "[step: 834] loss: 20.030670166015625\n",
      "[step: 835] loss: 20.021221160888672\n",
      "[step: 836] loss: 20.0252628326416\n",
      "[step: 837] loss: 20.021034240722656\n",
      "[step: 838] loss: 20.017854690551758\n",
      "[step: 839] loss: 20.020118713378906\n",
      "[step: 840] loss: 20.01383399963379\n",
      "[step: 841] loss: 20.014678955078125\n",
      "[step: 842] loss: 20.012784957885742\n",
      "[step: 843] loss: 20.00897216796875\n",
      "[step: 844] loss: 20.010143280029297\n",
      "[step: 845] loss: 20.006332397460938\n",
      "[step: 846] loss: 20.005130767822266\n",
      "[step: 847] loss: 20.004547119140625\n",
      "[step: 848] loss: 20.00098991394043\n",
      "[step: 849] loss: 20.000810623168945\n",
      "[step: 850] loss: 19.998722076416016\n",
      "[step: 851] loss: 19.996328353881836\n",
      "[step: 852] loss: 19.995956420898438\n",
      "[step: 853] loss: 19.993350982666016\n",
      "[step: 854] loss: 19.991857528686523\n",
      "[step: 855] loss: 19.990821838378906\n",
      "[step: 856] loss: 19.988283157348633\n",
      "[step: 857] loss: 19.987157821655273\n",
      "[step: 858] loss: 19.985584259033203\n",
      "[step: 859] loss: 19.983394622802734\n",
      "[step: 860] loss: 19.982301712036133\n",
      "[step: 861] loss: 19.980411529541016\n",
      "[step: 862] loss: 19.978515625\n",
      "[step: 863] loss: 19.977235794067383\n",
      "[step: 864] loss: 19.975248336791992\n",
      "[step: 865] loss: 19.973533630371094\n",
      "[step: 866] loss: 19.972084045410156\n",
      "[step: 867] loss: 19.970125198364258\n",
      "[step: 868] loss: 19.968456268310547\n",
      "[step: 869] loss: 19.966859817504883\n",
      "[step: 870] loss: 19.96493911743164\n",
      "[step: 871] loss: 19.963253021240234\n",
      "[step: 872] loss: 19.961589813232422\n",
      "[step: 873] loss: 19.959693908691406\n",
      "[step: 874] loss: 19.957977294921875\n",
      "[step: 875] loss: 19.956256866455078\n",
      "[step: 876] loss: 19.954362869262695\n",
      "[step: 877] loss: 19.95261001586914\n",
      "[step: 878] loss: 19.950857162475586\n",
      "[step: 879] loss: 19.948955535888672\n",
      "[step: 880] loss: 19.947189331054688\n",
      "[step: 881] loss: 19.945388793945312\n",
      "[step: 882] loss: 19.94349479675293\n",
      "[step: 883] loss: 19.94167709350586\n",
      "[step: 884] loss: 19.939851760864258\n",
      "[step: 885] loss: 19.93796157836914\n",
      "[step: 886] loss: 19.936100006103516\n",
      "[step: 887] loss: 19.93426513671875\n",
      "[step: 888] loss: 19.932363510131836\n",
      "[step: 889] loss: 19.930482864379883\n",
      "[step: 890] loss: 19.928621292114258\n",
      "[step: 891] loss: 19.92671012878418\n",
      "[step: 892] loss: 19.9248046875\n",
      "[step: 893] loss: 19.922924041748047\n",
      "[step: 894] loss: 19.921016693115234\n",
      "[step: 895] loss: 19.91909408569336\n",
      "[step: 896] loss: 19.917192459106445\n",
      "[step: 897] loss: 19.9152889251709\n",
      "[step: 898] loss: 19.91335105895996\n",
      "[step: 899] loss: 19.91143226623535\n",
      "[step: 900] loss: 19.909526824951172\n",
      "[step: 901] loss: 19.907590866088867\n",
      "[step: 902] loss: 19.90566062927246\n",
      "[step: 903] loss: 19.90374183654785\n",
      "[step: 904] loss: 19.901809692382812\n",
      "[step: 905] loss: 19.899877548217773\n",
      "[step: 906] loss: 19.89794921875\n",
      "[step: 907] loss: 19.89603042602539\n",
      "[step: 908] loss: 19.894094467163086\n",
      "[step: 909] loss: 19.892162322998047\n",
      "[step: 910] loss: 19.890243530273438\n",
      "[step: 911] loss: 19.888317108154297\n",
      "[step: 912] loss: 19.886381149291992\n",
      "[step: 913] loss: 19.88446807861328\n",
      "[step: 914] loss: 19.88255500793457\n",
      "[step: 915] loss: 19.880626678466797\n",
      "[step: 916] loss: 19.87871551513672\n",
      "[step: 917] loss: 19.876798629760742\n",
      "[step: 918] loss: 19.874879837036133\n",
      "[step: 919] loss: 19.872970581054688\n",
      "[step: 920] loss: 19.87107276916504\n",
      "[step: 921] loss: 19.869159698486328\n",
      "[step: 922] loss: 19.867263793945312\n",
      "[step: 923] loss: 19.865358352661133\n",
      "[step: 924] loss: 19.86345672607422\n",
      "[step: 925] loss: 19.861560821533203\n",
      "[step: 926] loss: 19.859670639038086\n",
      "[step: 927] loss: 19.857784271240234\n",
      "[step: 928] loss: 19.855892181396484\n",
      "[step: 929] loss: 19.854005813598633\n",
      "[step: 930] loss: 19.85212516784668\n",
      "[step: 931] loss: 19.85024642944336\n",
      "[step: 932] loss: 19.848365783691406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 933] loss: 19.84649085998535\n",
      "[step: 934] loss: 19.844608306884766\n",
      "[step: 935] loss: 19.842735290527344\n",
      "[step: 936] loss: 19.84086799621582\n",
      "[step: 937] loss: 19.83898162841797\n",
      "[step: 938] loss: 19.83711051940918\n",
      "[step: 939] loss: 19.835241317749023\n",
      "[step: 940] loss: 19.833364486694336\n",
      "[step: 941] loss: 19.831491470336914\n",
      "[step: 942] loss: 19.829618453979492\n",
      "[step: 943] loss: 19.827743530273438\n",
      "[step: 944] loss: 19.825862884521484\n",
      "[step: 945] loss: 19.823993682861328\n",
      "[step: 946] loss: 19.822111129760742\n",
      "[step: 947] loss: 19.820222854614258\n",
      "[step: 948] loss: 19.818336486816406\n",
      "[step: 949] loss: 19.81645393371582\n",
      "[step: 950] loss: 19.814565658569336\n",
      "[step: 951] loss: 19.812671661376953\n",
      "[step: 952] loss: 19.810760498046875\n",
      "[step: 953] loss: 19.808862686157227\n",
      "[step: 954] loss: 19.806961059570312\n",
      "[step: 955] loss: 19.805051803588867\n",
      "[step: 956] loss: 19.803129196166992\n",
      "[step: 957] loss: 19.80121612548828\n",
      "[step: 958] loss: 19.799287796020508\n",
      "[step: 959] loss: 19.797351837158203\n",
      "[step: 960] loss: 19.795412063598633\n",
      "[step: 961] loss: 19.793466567993164\n",
      "[step: 962] loss: 19.791505813598633\n",
      "[step: 963] loss: 19.789552688598633\n",
      "[step: 964] loss: 19.78758430480957\n",
      "[step: 965] loss: 19.785614013671875\n",
      "[step: 966] loss: 19.783620834350586\n",
      "[step: 967] loss: 19.781639099121094\n",
      "[step: 968] loss: 19.779630661010742\n",
      "[step: 969] loss: 19.777626037597656\n",
      "[step: 970] loss: 19.775606155395508\n",
      "[step: 971] loss: 19.773582458496094\n",
      "[step: 972] loss: 19.77155303955078\n",
      "[step: 973] loss: 19.769493103027344\n",
      "[step: 974] loss: 19.76744270324707\n",
      "[step: 975] loss: 19.7653751373291\n",
      "[step: 976] loss: 19.76329231262207\n",
      "[step: 977] loss: 19.761205673217773\n",
      "[step: 978] loss: 19.75910186767578\n",
      "[step: 979] loss: 19.757001876831055\n",
      "[step: 980] loss: 19.754873275756836\n",
      "[step: 981] loss: 19.752735137939453\n",
      "[step: 982] loss: 19.750595092773438\n",
      "[step: 983] loss: 19.74843978881836\n",
      "[step: 984] loss: 19.746267318725586\n",
      "[step: 985] loss: 19.74407958984375\n",
      "[step: 986] loss: 19.74188232421875\n",
      "[step: 987] loss: 19.739673614501953\n",
      "[step: 988] loss: 19.73744773864746\n",
      "[step: 989] loss: 19.735218048095703\n",
      "[step: 990] loss: 19.73296546936035\n",
      "[step: 991] loss: 19.730710983276367\n",
      "[step: 992] loss: 19.72842788696289\n",
      "[step: 993] loss: 19.726137161254883\n",
      "[step: 994] loss: 19.72383689880371\n",
      "[step: 995] loss: 19.72152328491211\n",
      "[step: 996] loss: 19.71918296813965\n",
      "[step: 997] loss: 19.71684455871582\n",
      "[step: 998] loss: 19.714481353759766\n",
      "[step: 999] loss: 19.71211051940918\n",
      "RMSE: 2.3567321129345857\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEivs2TdvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6090b1def0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train/validation split\n",
    "train_size = int(len(y) * 0.7)\n",
    "test_size = len(y) - train_size\n",
    "trainX, testX = np.array(x[0:train_size]), np.array(x[train_size:])\n",
    "trainY, testY = np.array(y[0:train_size]), np.array(y[train_size:])\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cells=[]\n",
    "for _ in range(layer_num):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    #cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.7)\n",
    "    cells.append(cell)\n",
    "    \n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "sess=tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Training step\n",
    "for i in range(iterations):\n",
    "    _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "    print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "# Test step\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
    "revised_rmse = rmse_val*(origin_xy.max()-origin_xy.min()+1e-7) + origin_xy.min()\n",
    "print(\"RMSE: {}\".format(revised_rmse))\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(testY)\n",
    "plt.plot(test_predict)\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"Temperature\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
