{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to add any functions, import statements, and variables.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1] loss: 526.3223876953125\n",
      "[step: 2] loss: 58.188507080078125\n",
      "[step: 3] loss: 127.2806167602539\n",
      "[step: 4] loss: 33.204734802246094\n",
      "[step: 5] loss: 44.99404525756836\n",
      "[step: 6] loss: 69.9940185546875\n",
      "[step: 7] loss: 68.78807067871094\n",
      "[step: 8] loss: 51.65983963012695\n",
      "[step: 9] loss: 35.991607666015625\n",
      "[step: 10] loss: 31.50693702697754\n",
      "[step: 11] loss: 37.560035705566406\n",
      "[step: 12] loss: 45.62152862548828\n",
      "[step: 13] loss: 47.351219177246094\n",
      "[step: 14] loss: 42.05704879760742\n",
      "[step: 15] loss: 35.08865737915039\n",
      "[step: 16] loss: 31.31057357788086\n",
      "[step: 17] loss: 31.79559326171875\n",
      "[step: 18] loss: 34.64952087402344\n",
      "[step: 19] loss: 37.171390533447266\n",
      "[step: 20] loss: 37.5283203125\n",
      "[step: 21] loss: 35.47459411621094\n",
      "[step: 22] loss: 32.18660354614258\n",
      "[step: 23] loss: 29.461435317993164\n",
      "[step: 24] loss: 28.624391555786133\n",
      "[step: 25] loss: 29.63703727722168\n",
      "[step: 26] loss: 31.053564071655273\n",
      "[step: 27] loss: 31.163454055786133\n",
      "[step: 28] loss: 29.517608642578125\n",
      "[step: 29] loss: 27.272804260253906\n",
      "[step: 30] loss: 25.975299835205078\n",
      "[step: 31] loss: 26.143049240112305\n",
      "[step: 32] loss: 26.972257614135742\n",
      "[step: 33] loss: 27.206968307495117\n",
      "[step: 34] loss: 26.295669555664062\n",
      "[step: 35] loss: 24.869508743286133\n",
      "[step: 36] loss: 24.112041473388672\n",
      "[step: 37] loss: 24.50047492980957\n",
      "[step: 38] loss: 25.202302932739258\n",
      "[step: 39] loss: 25.07749366760254\n",
      "[step: 40] loss: 24.194860458374023\n",
      "[step: 41] loss: 23.666013717651367\n",
      "[step: 42] loss: 24.029565811157227\n",
      "[step: 43] loss: 24.593257904052734\n",
      "[step: 44] loss: 24.51523208618164\n",
      "[step: 45] loss: 23.952728271484375\n",
      "[step: 46] loss: 23.724403381347656\n",
      "[step: 47] loss: 24.061058044433594\n",
      "[step: 48] loss: 24.312894821166992\n",
      "[step: 49] loss: 24.039291381835938\n",
      "[step: 50] loss: 23.653446197509766\n",
      "[step: 51] loss: 23.644180297851562\n",
      "[step: 52] loss: 23.842941284179688\n",
      "[step: 53] loss: 23.816617965698242\n",
      "[step: 54] loss: 23.543481826782227\n",
      "[step: 55] loss: 23.361623764038086\n",
      "[step: 56] loss: 23.42212677001953\n",
      "[step: 57] loss: 23.509126663208008\n",
      "[step: 58] loss: 23.42072868347168\n",
      "[step: 59] loss: 23.25732421875\n",
      "[step: 60] loss: 23.213489532470703\n",
      "[step: 61] loss: 23.282440185546875\n",
      "[step: 62] loss: 23.308349609375\n",
      "[step: 63] loss: 23.22971534729004\n",
      "[step: 64] loss: 23.14301300048828\n",
      "[step: 65] loss: 23.13951873779297\n",
      "[step: 66] loss: 23.177906036376953\n",
      "[step: 67] loss: 23.164045333862305\n",
      "[step: 68] loss: 23.09389305114746\n",
      "[step: 69] loss: 23.043548583984375\n",
      "[step: 70] loss: 23.045677185058594\n",
      "[step: 71] loss: 23.050561904907227\n",
      "[step: 72] loss: 23.012405395507812\n",
      "[step: 73] loss: 22.955427169799805\n",
      "[step: 74] loss: 22.92780113220215\n",
      "[step: 75] loss: 22.926620483398438\n",
      "[step: 76] loss: 22.910064697265625\n",
      "[step: 77] loss: 22.867755889892578\n",
      "[step: 78] loss: 22.83235740661621\n",
      "[step: 79] loss: 22.821699142456055\n",
      "[step: 80] loss: 22.813203811645508\n",
      "[step: 81] loss: 22.785938262939453\n",
      "[step: 82] loss: 22.753551483154297\n",
      "[step: 83] loss: 22.736642837524414\n",
      "[step: 84] loss: 22.72791290283203\n",
      "[step: 85] loss: 22.708261489868164\n",
      "[step: 86] loss: 22.680177688598633\n",
      "[step: 87] loss: 22.659915924072266\n",
      "[step: 88] loss: 22.647768020629883\n",
      "[step: 89] loss: 22.630245208740234\n",
      "[step: 90] loss: 22.605201721191406\n",
      "[step: 91] loss: 22.583520889282227\n",
      "[step: 92] loss: 22.568429946899414\n",
      "[step: 93] loss: 22.551332473754883\n",
      "[step: 94] loss: 22.528757095336914\n",
      "[step: 95] loss: 22.507583618164062\n",
      "[step: 96] loss: 22.491317749023438\n",
      "[step: 97] loss: 22.47462272644043\n",
      "[step: 98] loss: 22.454227447509766\n",
      "[step: 99] loss: 22.434282302856445\n",
      "[step: 100] loss: 22.417766571044922\n",
      "[step: 101] loss: 22.4013729095459\n",
      "[step: 102] loss: 22.382465362548828\n",
      "[step: 103] loss: 22.363645553588867\n",
      "[step: 104] loss: 22.34708595275879\n",
      "[step: 105] loss: 22.330698013305664\n",
      "[step: 106] loss: 22.31258773803711\n",
      "[step: 107] loss: 22.29445457458496\n",
      "[step: 108] loss: 22.277851104736328\n",
      "[step: 109] loss: 22.261369705200195\n",
      "[step: 110] loss: 22.243736267089844\n",
      "[step: 111] loss: 22.226171493530273\n",
      "[step: 112] loss: 22.20969009399414\n",
      "[step: 113] loss: 22.19324493408203\n",
      "[step: 114] loss: 22.17608070373535\n",
      "[step: 115] loss: 22.159103393554688\n",
      "[step: 116] loss: 22.142900466918945\n",
      "[step: 117] loss: 22.126657485961914\n",
      "[step: 118] loss: 22.109966278076172\n",
      "[step: 119] loss: 22.093568801879883\n",
      "[step: 120] loss: 22.077688217163086\n",
      "[step: 121] loss: 22.06168556213379\n",
      "[step: 122] loss: 22.04545021057129\n",
      "[step: 123] loss: 22.029541015625\n",
      "[step: 124] loss: 22.013933181762695\n",
      "[step: 125] loss: 21.998172760009766\n",
      "[step: 126] loss: 21.982362747192383\n",
      "[step: 127] loss: 21.966842651367188\n",
      "[step: 128] loss: 21.95147705078125\n",
      "[step: 129] loss: 21.93600082397461\n",
      "[step: 130] loss: 21.920604705810547\n",
      "[step: 131] loss: 21.90546989440918\n",
      "[step: 132] loss: 21.8903865814209\n",
      "[step: 133] loss: 21.87526512145996\n",
      "[step: 134] loss: 21.86030387878418\n",
      "[step: 135] loss: 21.84552764892578\n",
      "[step: 136] loss: 21.83077049255371\n",
      "[step: 137] loss: 21.816064834594727\n",
      "[step: 138] loss: 21.80155372619629\n",
      "[step: 139] loss: 21.787139892578125\n",
      "[step: 140] loss: 21.77277183532715\n",
      "[step: 141] loss: 21.758514404296875\n",
      "[step: 142] loss: 21.74441146850586\n",
      "[step: 143] loss: 21.730403900146484\n",
      "[step: 144] loss: 21.716461181640625\n",
      "[step: 145] loss: 21.702682495117188\n",
      "[step: 146] loss: 21.68902587890625\n",
      "[step: 147] loss: 21.675457000732422\n",
      "[step: 148] loss: 21.662010192871094\n",
      "[step: 149] loss: 21.64873695373535\n",
      "[step: 150] loss: 21.635555267333984\n",
      "[step: 151] loss: 21.62251091003418\n",
      "[step: 152] loss: 21.609636306762695\n",
      "[step: 153] loss: 21.596893310546875\n",
      "[step: 154] loss: 21.58428382873535\n",
      "[step: 155] loss: 21.57183265686035\n",
      "[step: 156] loss: 21.559539794921875\n",
      "[step: 157] loss: 21.54740333557129\n",
      "[step: 158] loss: 21.535430908203125\n",
      "[step: 159] loss: 21.523643493652344\n",
      "[step: 160] loss: 21.51201629638672\n",
      "[step: 161] loss: 21.500545501708984\n",
      "[step: 162] loss: 21.489273071289062\n",
      "[step: 163] loss: 21.478187561035156\n",
      "[step: 164] loss: 21.4672794342041\n",
      "[step: 165] loss: 21.456573486328125\n",
      "[step: 166] loss: 21.446060180664062\n",
      "[step: 167] loss: 21.435745239257812\n",
      "[step: 168] loss: 21.425640106201172\n",
      "[step: 169] loss: 21.415725708007812\n",
      "[step: 170] loss: 21.406028747558594\n",
      "[step: 171] loss: 21.396541595458984\n",
      "[step: 172] loss: 21.38727378845215\n",
      "[step: 173] loss: 21.378206253051758\n",
      "[step: 174] loss: 21.369367599487305\n",
      "[step: 175] loss: 21.36073875427246\n",
      "[step: 176] loss: 21.352331161499023\n",
      "[step: 177] loss: 21.344135284423828\n",
      "[step: 178] loss: 21.33616828918457\n",
      "[step: 179] loss: 21.328405380249023\n",
      "[step: 180] loss: 21.32086753845215\n",
      "[step: 181] loss: 21.31354522705078\n",
      "[step: 182] loss: 21.30643081665039\n",
      "[step: 183] loss: 21.299528121948242\n",
      "[step: 184] loss: 21.292827606201172\n",
      "[step: 185] loss: 21.286338806152344\n",
      "[step: 186] loss: 21.28005027770996\n",
      "[step: 187] loss: 21.273950576782227\n",
      "[step: 188] loss: 21.268041610717773\n",
      "[step: 189] loss: 21.262319564819336\n",
      "[step: 190] loss: 21.256778717041016\n",
      "[step: 191] loss: 21.25140953063965\n",
      "[step: 192] loss: 21.246213912963867\n",
      "[step: 193] loss: 21.24117088317871\n",
      "[step: 194] loss: 21.236291885375977\n",
      "[step: 195] loss: 21.2315731048584\n",
      "[step: 196] loss: 21.22698974609375\n",
      "[step: 197] loss: 21.2225341796875\n",
      "[step: 198] loss: 21.21822738647461\n",
      "[step: 199] loss: 21.214027404785156\n",
      "[step: 200] loss: 21.2099609375\n",
      "[step: 201] loss: 21.206003189086914\n",
      "[step: 202] loss: 21.202146530151367\n",
      "[step: 203] loss: 21.198392868041992\n",
      "[step: 204] loss: 21.194738388061523\n",
      "[step: 205] loss: 21.191162109375\n",
      "[step: 206] loss: 21.18766975402832\n",
      "[step: 207] loss: 21.184263229370117\n",
      "[step: 208] loss: 21.180931091308594\n",
      "[step: 209] loss: 21.17765235900879\n",
      "[step: 210] loss: 21.1744441986084\n",
      "[step: 211] loss: 21.171297073364258\n",
      "[step: 212] loss: 21.16819953918457\n",
      "[step: 213] loss: 21.165170669555664\n",
      "[step: 214] loss: 21.16217613220215\n",
      "[step: 215] loss: 21.159231185913086\n",
      "[step: 216] loss: 21.156330108642578\n",
      "[step: 217] loss: 21.153465270996094\n",
      "[step: 218] loss: 21.1506404876709\n",
      "[step: 219] loss: 21.147857666015625\n",
      "[step: 220] loss: 21.145095825195312\n",
      "[step: 221] loss: 21.14236831665039\n",
      "[step: 222] loss: 21.13967514038086\n",
      "[step: 223] loss: 21.137008666992188\n",
      "[step: 224] loss: 21.134368896484375\n",
      "[step: 225] loss: 21.13174819946289\n",
      "[step: 226] loss: 21.12915802001953\n",
      "[step: 227] loss: 21.126588821411133\n",
      "[step: 228] loss: 21.12404441833496\n",
      "[step: 229] loss: 21.121517181396484\n",
      "[step: 230] loss: 21.11900520324707\n",
      "[step: 231] loss: 21.116527557373047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 232] loss: 21.114059448242188\n",
      "[step: 233] loss: 21.111614227294922\n",
      "[step: 234] loss: 21.109180450439453\n",
      "[step: 235] loss: 21.10676383972168\n",
      "[step: 236] loss: 21.1043643951416\n",
      "[step: 237] loss: 21.101980209350586\n",
      "[step: 238] loss: 21.099611282348633\n",
      "[step: 239] loss: 21.09726333618164\n",
      "[step: 240] loss: 21.094928741455078\n",
      "[step: 241] loss: 21.092615127563477\n",
      "[step: 242] loss: 21.090303421020508\n",
      "[step: 243] loss: 21.0880126953125\n",
      "[step: 244] loss: 21.085739135742188\n",
      "[step: 245] loss: 21.083471298217773\n",
      "[step: 246] loss: 21.08123016357422\n",
      "[step: 247] loss: 21.078989028930664\n",
      "[step: 248] loss: 21.0767822265625\n",
      "[step: 249] loss: 21.07457733154297\n",
      "[step: 250] loss: 21.072383880615234\n",
      "[step: 251] loss: 21.070213317871094\n",
      "[step: 252] loss: 21.068052291870117\n",
      "[step: 253] loss: 21.065898895263672\n",
      "[step: 254] loss: 21.063756942749023\n",
      "[step: 255] loss: 21.061641693115234\n",
      "[step: 256] loss: 21.059537887573242\n",
      "[step: 257] loss: 21.05744171142578\n",
      "[step: 258] loss: 21.055370330810547\n",
      "[step: 259] loss: 21.053306579589844\n",
      "[step: 260] loss: 21.05125617980957\n",
      "[step: 261] loss: 21.049217224121094\n",
      "[step: 262] loss: 21.047204971313477\n",
      "[step: 263] loss: 21.045196533203125\n",
      "[step: 264] loss: 21.043201446533203\n",
      "[step: 265] loss: 21.041223526000977\n",
      "[step: 266] loss: 21.03926658630371\n",
      "[step: 267] loss: 21.037311553955078\n",
      "[step: 268] loss: 21.035381317138672\n",
      "[step: 269] loss: 21.03346061706543\n",
      "[step: 270] loss: 21.031553268432617\n",
      "[step: 271] loss: 21.029664993286133\n",
      "[step: 272] loss: 21.027782440185547\n",
      "[step: 273] loss: 21.025920867919922\n",
      "[step: 274] loss: 21.02407455444336\n",
      "[step: 275] loss: 21.022241592407227\n",
      "[step: 276] loss: 21.020423889160156\n",
      "[step: 277] loss: 21.018617630004883\n",
      "[step: 278] loss: 21.016830444335938\n",
      "[step: 279] loss: 21.015060424804688\n",
      "[step: 280] loss: 21.013282775878906\n",
      "[step: 281] loss: 21.011539459228516\n",
      "[step: 282] loss: 21.009807586669922\n",
      "[step: 283] loss: 21.008092880249023\n",
      "[step: 284] loss: 21.00638198852539\n",
      "[step: 285] loss: 21.004697799682617\n",
      "[step: 286] loss: 21.00301170349121\n",
      "[step: 287] loss: 21.00135612487793\n",
      "[step: 288] loss: 20.999704360961914\n",
      "[step: 289] loss: 20.998064041137695\n",
      "[step: 290] loss: 20.99645233154297\n",
      "[step: 291] loss: 20.994848251342773\n",
      "[step: 292] loss: 20.993244171142578\n",
      "[step: 293] loss: 20.991666793823242\n",
      "[step: 294] loss: 20.990100860595703\n",
      "[step: 295] loss: 20.98854637145996\n",
      "[step: 296] loss: 20.987014770507812\n",
      "[step: 297] loss: 20.985483169555664\n",
      "[step: 298] loss: 20.983972549438477\n",
      "[step: 299] loss: 20.982481002807617\n",
      "[step: 300] loss: 20.98099136352539\n",
      "[step: 301] loss: 20.979516983032227\n",
      "[step: 302] loss: 20.97805404663086\n",
      "[step: 303] loss: 20.976613998413086\n",
      "[step: 304] loss: 20.975175857543945\n",
      "[step: 305] loss: 20.973758697509766\n",
      "[step: 306] loss: 20.972349166870117\n",
      "[step: 307] loss: 20.970949172973633\n",
      "[step: 308] loss: 20.969573974609375\n",
      "[step: 309] loss: 20.96820068359375\n",
      "[step: 310] loss: 20.966842651367188\n",
      "[step: 311] loss: 20.965499877929688\n",
      "[step: 312] loss: 20.964157104492188\n",
      "[step: 313] loss: 20.962841033935547\n",
      "[step: 314] loss: 20.961528778076172\n",
      "[step: 315] loss: 20.96023941040039\n",
      "[step: 316] loss: 20.958942413330078\n",
      "[step: 317] loss: 20.957660675048828\n",
      "[step: 318] loss: 20.956398010253906\n",
      "[step: 319] loss: 20.955141067504883\n",
      "[step: 320] loss: 20.953903198242188\n",
      "[step: 321] loss: 20.95267105102539\n",
      "[step: 322] loss: 20.951457977294922\n",
      "[step: 323] loss: 20.950244903564453\n",
      "[step: 324] loss: 20.94904327392578\n",
      "[step: 325] loss: 20.94786262512207\n",
      "[step: 326] loss: 20.946674346923828\n",
      "[step: 327] loss: 20.945505142211914\n",
      "[step: 328] loss: 20.944351196289062\n",
      "[step: 329] loss: 20.943193435668945\n",
      "[step: 330] loss: 20.942058563232422\n",
      "[step: 331] loss: 20.940919876098633\n",
      "[step: 332] loss: 20.939807891845703\n",
      "[step: 333] loss: 20.938697814941406\n",
      "[step: 334] loss: 20.93758773803711\n",
      "[step: 335] loss: 20.936498641967773\n",
      "[step: 336] loss: 20.935409545898438\n",
      "[step: 337] loss: 20.934335708618164\n",
      "[step: 338] loss: 20.93326759338379\n",
      "[step: 339] loss: 20.932201385498047\n",
      "[step: 340] loss: 20.931150436401367\n",
      "[step: 341] loss: 20.930103302001953\n",
      "[step: 342] loss: 20.92906951904297\n",
      "[step: 343] loss: 20.92804718017578\n",
      "[step: 344] loss: 20.927019119262695\n",
      "[step: 345] loss: 20.926013946533203\n",
      "[step: 346] loss: 20.92500114440918\n",
      "[step: 347] loss: 20.92400550842285\n",
      "[step: 348] loss: 20.923006057739258\n",
      "[step: 349] loss: 20.922019958496094\n",
      "[step: 350] loss: 20.92104148864746\n",
      "[step: 351] loss: 20.920066833496094\n",
      "[step: 352] loss: 20.919103622436523\n",
      "[step: 353] loss: 20.918132781982422\n",
      "[step: 354] loss: 20.917186737060547\n",
      "[step: 355] loss: 20.91623306274414\n",
      "[step: 356] loss: 20.915287017822266\n",
      "[step: 357] loss: 20.914342880249023\n",
      "[step: 358] loss: 20.913419723510742\n",
      "[step: 359] loss: 20.912492752075195\n",
      "[step: 360] loss: 20.911556243896484\n",
      "[step: 361] loss: 20.910655975341797\n",
      "[step: 362] loss: 20.909732818603516\n",
      "[step: 363] loss: 20.908823013305664\n",
      "[step: 364] loss: 20.907909393310547\n",
      "[step: 365] loss: 20.907014846801758\n",
      "[step: 366] loss: 20.90612030029297\n",
      "[step: 367] loss: 20.905223846435547\n",
      "[step: 368] loss: 20.904333114624023\n",
      "[step: 369] loss: 20.90345001220703\n",
      "[step: 370] loss: 20.90257453918457\n",
      "[step: 371] loss: 20.901689529418945\n",
      "[step: 372] loss: 20.900814056396484\n",
      "[step: 373] loss: 20.899946212768555\n",
      "[step: 374] loss: 20.899084091186523\n",
      "[step: 375] loss: 20.89821434020996\n",
      "[step: 376] loss: 20.897350311279297\n",
      "[step: 377] loss: 20.896488189697266\n",
      "[step: 378] loss: 20.895631790161133\n",
      "[step: 379] loss: 20.8947811126709\n",
      "[step: 380] loss: 20.893930435180664\n",
      "[step: 381] loss: 20.893091201782227\n",
      "[step: 382] loss: 20.892230987548828\n",
      "[step: 383] loss: 20.891389846801758\n",
      "[step: 384] loss: 20.89055061340332\n",
      "[step: 385] loss: 20.88970947265625\n",
      "[step: 386] loss: 20.888864517211914\n",
      "[step: 387] loss: 20.888036727905273\n",
      "[step: 388] loss: 20.88719367980957\n",
      "[step: 389] loss: 20.886367797851562\n",
      "[step: 390] loss: 20.885526657104492\n",
      "[step: 391] loss: 20.884695053100586\n",
      "[step: 392] loss: 20.88386344909668\n",
      "[step: 393] loss: 20.883033752441406\n",
      "[step: 394] loss: 20.882205963134766\n",
      "[step: 395] loss: 20.881385803222656\n",
      "[step: 396] loss: 20.880552291870117\n",
      "[step: 397] loss: 20.879728317260742\n",
      "[step: 398] loss: 20.87890625\n",
      "[step: 399] loss: 20.878080368041992\n",
      "[step: 400] loss: 20.87726402282715\n",
      "[step: 401] loss: 20.876434326171875\n",
      "[step: 402] loss: 20.8756103515625\n",
      "[step: 403] loss: 20.874792098999023\n",
      "[step: 404] loss: 20.87396240234375\n",
      "[step: 405] loss: 20.873151779174805\n",
      "[step: 406] loss: 20.87232780456543\n",
      "[step: 407] loss: 20.87150764465332\n",
      "[step: 408] loss: 20.870691299438477\n",
      "[step: 409] loss: 20.869863510131836\n",
      "[step: 410] loss: 20.869043350219727\n",
      "[step: 411] loss: 20.868221282958984\n",
      "[step: 412] loss: 20.867387771606445\n",
      "[step: 413] loss: 20.866588592529297\n",
      "[step: 414] loss: 20.865760803222656\n",
      "[step: 415] loss: 20.864933013916016\n",
      "[step: 416] loss: 20.86411476135254\n",
      "[step: 417] loss: 20.86328887939453\n",
      "[step: 418] loss: 20.862462997436523\n",
      "[step: 419] loss: 20.86163330078125\n",
      "[step: 420] loss: 20.860815048217773\n",
      "[step: 421] loss: 20.8599796295166\n",
      "[step: 422] loss: 20.85915756225586\n",
      "[step: 423] loss: 20.85832977294922\n",
      "[step: 424] loss: 20.857513427734375\n",
      "[step: 425] loss: 20.856679916381836\n",
      "[step: 426] loss: 20.855852127075195\n",
      "[step: 427] loss: 20.855024337768555\n",
      "[step: 428] loss: 20.854185104370117\n",
      "[step: 429] loss: 20.853349685668945\n",
      "[step: 430] loss: 20.85252571105957\n",
      "[step: 431] loss: 20.851680755615234\n",
      "[step: 432] loss: 20.850845336914062\n",
      "[step: 433] loss: 20.850008010864258\n",
      "[step: 434] loss: 20.849166870117188\n",
      "[step: 435] loss: 20.84833335876465\n",
      "[step: 436] loss: 20.847488403320312\n",
      "[step: 437] loss: 20.846649169921875\n",
      "[step: 438] loss: 20.84579849243164\n",
      "[step: 439] loss: 20.844951629638672\n",
      "[step: 440] loss: 20.844104766845703\n",
      "[step: 441] loss: 20.843250274658203\n",
      "[step: 442] loss: 20.842405319213867\n",
      "[step: 443] loss: 20.84156036376953\n",
      "[step: 444] loss: 20.840696334838867\n",
      "[step: 445] loss: 20.839845657348633\n",
      "[step: 446] loss: 20.838985443115234\n",
      "[step: 447] loss: 20.838136672973633\n",
      "[step: 448] loss: 20.8372745513916\n",
      "[step: 449] loss: 20.836402893066406\n",
      "[step: 450] loss: 20.83554458618164\n",
      "[step: 451] loss: 20.834684371948242\n",
      "[step: 452] loss: 20.833816528320312\n",
      "[step: 453] loss: 20.83293914794922\n",
      "[step: 454] loss: 20.832077026367188\n",
      "[step: 455] loss: 20.831186294555664\n",
      "[step: 456] loss: 20.830324172973633\n",
      "[step: 457] loss: 20.82944679260254\n",
      "[step: 458] loss: 20.828571319580078\n",
      "[step: 459] loss: 20.82769012451172\n",
      "[step: 460] loss: 20.826793670654297\n",
      "[step: 461] loss: 20.825910568237305\n",
      "[step: 462] loss: 20.82502555847168\n",
      "[step: 463] loss: 20.824138641357422\n",
      "[step: 464] loss: 20.823240280151367\n",
      "[step: 465] loss: 20.822345733642578\n",
      "[step: 466] loss: 20.821455001831055\n",
      "[step: 467] loss: 20.82054901123047\n",
      "[step: 468] loss: 20.819650650024414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 469] loss: 20.818740844726562\n",
      "[step: 470] loss: 20.81783676147461\n",
      "[step: 471] loss: 20.816925048828125\n",
      "[step: 472] loss: 20.81603240966797\n",
      "[step: 473] loss: 20.815101623535156\n",
      "[step: 474] loss: 20.814197540283203\n",
      "[step: 475] loss: 20.813274383544922\n",
      "[step: 476] loss: 20.812349319458008\n",
      "[step: 477] loss: 20.81142807006836\n",
      "[step: 478] loss: 20.810501098632812\n",
      "[step: 479] loss: 20.809579849243164\n",
      "[step: 480] loss: 20.80864906311035\n",
      "[step: 481] loss: 20.807714462280273\n",
      "[step: 482] loss: 20.806777954101562\n",
      "[step: 483] loss: 20.80583953857422\n",
      "[step: 484] loss: 20.804895401000977\n",
      "[step: 485] loss: 20.803955078125\n",
      "[step: 486] loss: 20.80301284790039\n",
      "[step: 487] loss: 20.80207061767578\n",
      "[step: 488] loss: 20.801116943359375\n",
      "[step: 489] loss: 20.8001651763916\n",
      "[step: 490] loss: 20.79920768737793\n",
      "[step: 491] loss: 20.79824447631836\n",
      "[step: 492] loss: 20.797290802001953\n",
      "[step: 493] loss: 20.79633140563965\n",
      "[step: 494] loss: 20.795368194580078\n",
      "[step: 495] loss: 20.79439353942871\n",
      "[step: 496] loss: 20.79342269897461\n",
      "[step: 497] loss: 20.792463302612305\n",
      "[step: 498] loss: 20.791486740112305\n",
      "[step: 499] loss: 20.79050064086914\n",
      "[step: 500] loss: 20.789531707763672\n",
      "[step: 501] loss: 20.78855323791504\n",
      "[step: 502] loss: 20.787569046020508\n",
      "[step: 503] loss: 20.786579132080078\n",
      "[step: 504] loss: 20.785594940185547\n",
      "[step: 505] loss: 20.78460121154785\n",
      "[step: 506] loss: 20.783611297607422\n",
      "[step: 507] loss: 20.782621383666992\n",
      "[step: 508] loss: 20.781627655029297\n",
      "[step: 509] loss: 20.78063201904297\n",
      "[step: 510] loss: 20.779634475708008\n",
      "[step: 511] loss: 20.77863311767578\n",
      "[step: 512] loss: 20.777626037597656\n",
      "[step: 513] loss: 20.776626586914062\n",
      "[step: 514] loss: 20.775617599487305\n",
      "[step: 515] loss: 20.774614334106445\n",
      "[step: 516] loss: 20.773597717285156\n",
      "[step: 517] loss: 20.772598266601562\n",
      "[step: 518] loss: 20.771589279174805\n",
      "[step: 519] loss: 20.770570755004883\n",
      "[step: 520] loss: 20.769556045532227\n",
      "[step: 521] loss: 20.768537521362305\n",
      "[step: 522] loss: 20.767526626586914\n",
      "[step: 523] loss: 20.766502380371094\n",
      "[step: 524] loss: 20.765493392944336\n",
      "[step: 525] loss: 20.764476776123047\n",
      "[step: 526] loss: 20.763456344604492\n",
      "[step: 527] loss: 20.762441635131836\n",
      "[step: 528] loss: 20.761423110961914\n",
      "[step: 529] loss: 20.760398864746094\n",
      "[step: 530] loss: 20.759376525878906\n",
      "[step: 531] loss: 20.758363723754883\n",
      "[step: 532] loss: 20.757343292236328\n",
      "[step: 533] loss: 20.756319046020508\n",
      "[step: 534] loss: 20.755308151245117\n",
      "[step: 535] loss: 20.75428581237793\n",
      "[step: 536] loss: 20.753273010253906\n",
      "[step: 537] loss: 20.75225830078125\n",
      "[step: 538] loss: 20.751237869262695\n",
      "[step: 539] loss: 20.750225067138672\n",
      "[step: 540] loss: 20.749210357666016\n",
      "[step: 541] loss: 20.748197555541992\n",
      "[step: 542] loss: 20.7471923828125\n",
      "[step: 543] loss: 20.746183395385742\n",
      "[step: 544] loss: 20.745180130004883\n",
      "[step: 545] loss: 20.744169235229492\n",
      "[step: 546] loss: 20.7431697845459\n",
      "[step: 547] loss: 20.74216651916504\n",
      "[step: 548] loss: 20.74117660522461\n",
      "[step: 549] loss: 20.740188598632812\n",
      "[step: 550] loss: 20.73921012878418\n",
      "[step: 551] loss: 20.73824119567871\n",
      "[step: 552] loss: 20.7373046875\n",
      "[step: 553] loss: 20.736448287963867\n",
      "[step: 554] loss: 20.735841751098633\n",
      "[step: 555] loss: 20.735910415649414\n",
      "[step: 556] loss: 20.737882614135742\n",
      "[step: 557] loss: 20.745460510253906\n",
      "[step: 558] loss: 20.76935386657715\n",
      "[step: 559] loss: 20.84169578552246\n",
      "[step: 560] loss: 21.05340003967285\n",
      "[step: 561] loss: 21.657697677612305\n",
      "[step: 562] loss: 23.111488342285156\n",
      "[step: 563] loss: 25.704654693603516\n",
      "[step: 564] loss: 25.72823143005371\n",
      "[step: 565] loss: 22.12217903137207\n",
      "[step: 566] loss: 21.664756774902344\n",
      "[step: 567] loss: 22.998777389526367\n",
      "[step: 568] loss: 20.928813934326172\n",
      "[step: 569] loss: 22.324115753173828\n",
      "[step: 570] loss: 20.862533569335938\n",
      "[step: 571] loss: 21.922950744628906\n",
      "[step: 572] loss: 20.919872283935547\n",
      "[step: 573] loss: 21.64217185974121\n",
      "[step: 574] loss: 20.915626525878906\n",
      "[step: 575] loss: 21.504514694213867\n",
      "[step: 576] loss: 20.93160629272461\n",
      "[step: 577] loss: 21.354904174804688\n",
      "[step: 578] loss: 20.848379135131836\n",
      "[step: 579] loss: 21.25732421875\n",
      "[step: 580] loss: 20.825237274169922\n",
      "[step: 581] loss: 21.159231185913086\n",
      "[step: 582] loss: 20.850830078125\n",
      "[step: 583] loss: 21.050434112548828\n",
      "[step: 584] loss: 20.881052017211914\n",
      "[step: 585] loss: 20.923072814941406\n",
      "[step: 586] loss: 20.90586280822754\n",
      "[step: 587] loss: 20.818056106567383\n",
      "[step: 588] loss: 20.90557289123535\n",
      "[step: 589] loss: 20.771846771240234\n",
      "[step: 590] loss: 20.88858985900879\n",
      "[step: 591] loss: 20.754783630371094\n",
      "[step: 592] loss: 20.855676651000977\n",
      "[step: 593] loss: 20.749378204345703\n",
      "[step: 594] loss: 20.811107635498047\n",
      "[step: 595] loss: 20.7542724609375\n",
      "[step: 596] loss: 20.772817611694336\n",
      "[step: 597] loss: 20.76299476623535\n",
      "[step: 598] loss: 20.74837875366211\n",
      "[step: 599] loss: 20.76915168762207\n",
      "[step: 600] loss: 20.731042861938477\n",
      "[step: 601] loss: 20.764299392700195\n",
      "[step: 602] loss: 20.721755981445312\n",
      "[step: 603] loss: 20.753276824951172\n",
      "[step: 604] loss: 20.719865798950195\n",
      "[step: 605] loss: 20.742734909057617\n",
      "[step: 606] loss: 20.721399307250977\n",
      "[step: 607] loss: 20.731943130493164\n",
      "[step: 608] loss: 20.72164535522461\n",
      "[step: 609] loss: 20.720714569091797\n",
      "[step: 610] loss: 20.71941375732422\n",
      "[step: 611] loss: 20.71183204650879\n",
      "[step: 612] loss: 20.71689796447754\n",
      "[step: 613] loss: 20.705793380737305\n",
      "[step: 614] loss: 20.713294982910156\n",
      "[step: 615] loss: 20.701248168945312\n",
      "[step: 616] loss: 20.707866668701172\n",
      "[step: 617] loss: 20.69733238220215\n",
      "[step: 618] loss: 20.702186584472656\n",
      "[step: 619] loss: 20.69462776184082\n",
      "[step: 620] loss: 20.6972713470459\n",
      "[step: 621] loss: 20.692668914794922\n",
      "[step: 622] loss: 20.692874908447266\n",
      "[step: 623] loss: 20.69040298461914\n",
      "[step: 624] loss: 20.688770294189453\n",
      "[step: 625] loss: 20.68804931640625\n",
      "[step: 626] loss: 20.685462951660156\n",
      "[step: 627] loss: 20.685916900634766\n",
      "[step: 628] loss: 20.68291664123535\n",
      "[step: 629] loss: 20.68372344970703\n",
      "[step: 630] loss: 20.680591583251953\n",
      "[step: 631] loss: 20.681325912475586\n",
      "[step: 632] loss: 20.678478240966797\n",
      "[step: 633] loss: 20.679031372070312\n",
      "[step: 634] loss: 20.67669677734375\n",
      "[step: 635] loss: 20.676921844482422\n",
      "[step: 636] loss: 20.675029754638672\n",
      "[step: 637] loss: 20.674863815307617\n",
      "[step: 638] loss: 20.67334747314453\n",
      "[step: 639] loss: 20.67290687561035\n",
      "[step: 640] loss: 20.671781539916992\n",
      "[step: 641] loss: 20.671161651611328\n",
      "[step: 642] loss: 20.670318603515625\n",
      "[step: 643] loss: 20.669551849365234\n",
      "[step: 644] loss: 20.668880462646484\n",
      "[step: 645] loss: 20.66803741455078\n",
      "[step: 646] loss: 20.667490005493164\n",
      "[step: 647] loss: 20.666648864746094\n",
      "[step: 648] loss: 20.666210174560547\n",
      "[step: 649] loss: 20.66539764404297\n",
      "[step: 650] loss: 20.66499137878418\n",
      "[step: 651] loss: 20.66420555114746\n",
      "[step: 652] loss: 20.663822174072266\n",
      "[step: 653] loss: 20.663089752197266\n",
      "[step: 654] loss: 20.6627254486084\n",
      "[step: 655] loss: 20.66204833984375\n",
      "[step: 656] loss: 20.66166877746582\n",
      "[step: 657] loss: 20.661039352416992\n",
      "[step: 658] loss: 20.66066551208496\n",
      "[step: 659] loss: 20.660072326660156\n",
      "[step: 660] loss: 20.659683227539062\n",
      "[step: 661] loss: 20.659130096435547\n",
      "[step: 662] loss: 20.65875244140625\n",
      "[step: 663] loss: 20.658227920532227\n",
      "[step: 664] loss: 20.6578426361084\n",
      "[step: 665] loss: 20.657339096069336\n",
      "[step: 666] loss: 20.656946182250977\n",
      "[step: 667] loss: 20.656476974487305\n",
      "[step: 668] loss: 20.656085968017578\n",
      "[step: 669] loss: 20.65562629699707\n",
      "[step: 670] loss: 20.65523910522461\n",
      "[step: 671] loss: 20.65479850769043\n",
      "[step: 672] loss: 20.65440559387207\n",
      "[step: 673] loss: 20.653976440429688\n",
      "[step: 674] loss: 20.653589248657227\n",
      "[step: 675] loss: 20.65316390991211\n",
      "[step: 676] loss: 20.652774810791016\n",
      "[step: 677] loss: 20.652360916137695\n",
      "[step: 678] loss: 20.651981353759766\n",
      "[step: 679] loss: 20.651573181152344\n",
      "[step: 680] loss: 20.651187896728516\n",
      "[step: 681] loss: 20.65077781677246\n",
      "[step: 682] loss: 20.65039825439453\n",
      "[step: 683] loss: 20.65001106262207\n",
      "[step: 684] loss: 20.649614334106445\n",
      "[step: 685] loss: 20.649229049682617\n",
      "[step: 686] loss: 20.648847579956055\n",
      "[step: 687] loss: 20.64846420288086\n",
      "[step: 688] loss: 20.648082733154297\n",
      "[step: 689] loss: 20.64769744873047\n",
      "[step: 690] loss: 20.64731216430664\n",
      "[step: 691] loss: 20.646921157836914\n",
      "[step: 692] loss: 20.646547317504883\n",
      "[step: 693] loss: 20.646163940429688\n",
      "[step: 694] loss: 20.645782470703125\n",
      "[step: 695] loss: 20.645404815673828\n",
      "[step: 696] loss: 20.645034790039062\n",
      "[step: 697] loss: 20.644651412963867\n",
      "[step: 698] loss: 20.644269943237305\n",
      "[step: 699] loss: 20.643890380859375\n",
      "[step: 700] loss: 20.64352035522461\n",
      "[step: 701] loss: 20.643142700195312\n",
      "[step: 702] loss: 20.642780303955078\n",
      "[step: 703] loss: 20.642393112182617\n",
      "[step: 704] loss: 20.64202117919922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 705] loss: 20.641647338867188\n",
      "[step: 706] loss: 20.64128303527832\n",
      "[step: 707] loss: 20.640899658203125\n",
      "[step: 708] loss: 20.640533447265625\n",
      "[step: 709] loss: 20.64016342163086\n",
      "[step: 710] loss: 20.63979148864746\n",
      "[step: 711] loss: 20.639427185058594\n",
      "[step: 712] loss: 20.63905906677246\n",
      "[step: 713] loss: 20.638687133789062\n",
      "[step: 714] loss: 20.638317108154297\n",
      "[step: 715] loss: 20.637950897216797\n",
      "[step: 716] loss: 20.63758659362793\n",
      "[step: 717] loss: 20.63722038269043\n",
      "[step: 718] loss: 20.636852264404297\n",
      "[step: 719] loss: 20.63648796081543\n",
      "[step: 720] loss: 20.636119842529297\n",
      "[step: 721] loss: 20.635757446289062\n",
      "[step: 722] loss: 20.635393142700195\n",
      "[step: 723] loss: 20.63502311706543\n",
      "[step: 724] loss: 20.634672164916992\n",
      "[step: 725] loss: 20.634307861328125\n",
      "[step: 726] loss: 20.633949279785156\n",
      "[step: 727] loss: 20.633581161499023\n",
      "[step: 728] loss: 20.63321876525879\n",
      "[step: 729] loss: 20.63286018371582\n",
      "[step: 730] loss: 20.63250160217285\n",
      "[step: 731] loss: 20.632143020629883\n",
      "[step: 732] loss: 20.63178062438965\n",
      "[step: 733] loss: 20.631425857543945\n",
      "[step: 734] loss: 20.631072998046875\n",
      "[step: 735] loss: 20.630712509155273\n",
      "[step: 736] loss: 20.63035774230957\n",
      "[step: 737] loss: 20.630001068115234\n",
      "[step: 738] loss: 20.6296443939209\n",
      "[step: 739] loss: 20.629289627075195\n",
      "[step: 740] loss: 20.628931045532227\n",
      "[step: 741] loss: 20.628572463989258\n",
      "[step: 742] loss: 20.628225326538086\n",
      "[step: 743] loss: 20.62787437438965\n",
      "[step: 744] loss: 20.627517700195312\n",
      "[step: 745] loss: 20.627164840698242\n",
      "[step: 746] loss: 20.626815795898438\n",
      "[step: 747] loss: 20.626466751098633\n",
      "[step: 748] loss: 20.62611198425293\n",
      "[step: 749] loss: 20.625761032104492\n",
      "[step: 750] loss: 20.625410079956055\n",
      "[step: 751] loss: 20.625059127807617\n",
      "[step: 752] loss: 20.624710083007812\n",
      "[step: 753] loss: 20.62436294555664\n",
      "[step: 754] loss: 20.624011993408203\n",
      "[step: 755] loss: 20.623661041259766\n",
      "[step: 756] loss: 20.623321533203125\n",
      "[step: 757] loss: 20.622968673706055\n",
      "[step: 758] loss: 20.622617721557617\n",
      "[step: 759] loss: 20.62226676940918\n",
      "[step: 760] loss: 20.621931076049805\n",
      "[step: 761] loss: 20.62157440185547\n",
      "[step: 762] loss: 20.621244430541992\n",
      "[step: 763] loss: 20.620887756347656\n",
      "[step: 764] loss: 20.620548248291016\n",
      "[step: 765] loss: 20.620203018188477\n",
      "[step: 766] loss: 20.61985969543457\n",
      "[step: 767] loss: 20.619510650634766\n",
      "[step: 768] loss: 20.61916732788086\n",
      "[step: 769] loss: 20.618831634521484\n",
      "[step: 770] loss: 20.61847686767578\n",
      "[step: 771] loss: 20.618141174316406\n",
      "[step: 772] loss: 20.617799758911133\n",
      "[step: 773] loss: 20.617450714111328\n",
      "[step: 774] loss: 20.617111206054688\n",
      "[step: 775] loss: 20.616771697998047\n",
      "[step: 776] loss: 20.616432189941406\n",
      "[step: 777] loss: 20.616085052490234\n",
      "[step: 778] loss: 20.615747451782227\n",
      "[step: 779] loss: 20.615402221679688\n",
      "[step: 780] loss: 20.615070343017578\n",
      "[step: 781] loss: 20.614734649658203\n",
      "[step: 782] loss: 20.61439323425293\n",
      "[step: 783] loss: 20.61405372619629\n",
      "[step: 784] loss: 20.613718032836914\n",
      "[step: 785] loss: 20.613378524780273\n",
      "[step: 786] loss: 20.613033294677734\n",
      "[step: 787] loss: 20.612709045410156\n",
      "[step: 788] loss: 20.61237335205078\n",
      "[step: 789] loss: 20.612028121948242\n",
      "[step: 790] loss: 20.611698150634766\n",
      "[step: 791] loss: 20.611360549926758\n",
      "[step: 792] loss: 20.611019134521484\n",
      "[step: 793] loss: 20.610685348510742\n",
      "[step: 794] loss: 20.6103515625\n",
      "[step: 795] loss: 20.610008239746094\n",
      "[step: 796] loss: 20.60967254638672\n",
      "[step: 797] loss: 20.609342575073242\n",
      "[step: 798] loss: 20.6090030670166\n",
      "[step: 799] loss: 20.608671188354492\n",
      "[step: 800] loss: 20.60833740234375\n",
      "[step: 801] loss: 20.607999801635742\n",
      "[step: 802] loss: 20.607669830322266\n",
      "[step: 803] loss: 20.607336044311523\n",
      "[step: 804] loss: 20.60700225830078\n",
      "[step: 805] loss: 20.606670379638672\n",
      "[step: 806] loss: 20.606334686279297\n",
      "[step: 807] loss: 20.606002807617188\n",
      "[step: 808] loss: 20.60567283630371\n",
      "[step: 809] loss: 20.6053409576416\n",
      "[step: 810] loss: 20.605010986328125\n",
      "[step: 811] loss: 20.604673385620117\n",
      "[step: 812] loss: 20.604347229003906\n",
      "[step: 813] loss: 20.604007720947266\n",
      "[step: 814] loss: 20.603681564331055\n",
      "[step: 815] loss: 20.60335350036621\n",
      "[step: 816] loss: 20.60302734375\n",
      "[step: 817] loss: 20.60268783569336\n",
      "[step: 818] loss: 20.60236167907715\n",
      "[step: 819] loss: 20.60202407836914\n",
      "[step: 820] loss: 20.60169792175293\n",
      "[step: 821] loss: 20.60136604309082\n",
      "[step: 822] loss: 20.601045608520508\n",
      "[step: 823] loss: 20.600711822509766\n",
      "[step: 824] loss: 20.60038185119629\n",
      "[step: 825] loss: 20.600046157836914\n",
      "[step: 826] loss: 20.599727630615234\n",
      "[step: 827] loss: 20.599393844604492\n",
      "[step: 828] loss: 20.599061965942383\n",
      "[step: 829] loss: 20.59873390197754\n",
      "[step: 830] loss: 20.59840965270996\n",
      "[step: 831] loss: 20.598079681396484\n",
      "[step: 832] loss: 20.59775161743164\n",
      "[step: 833] loss: 20.59742546081543\n",
      "[step: 834] loss: 20.59709930419922\n",
      "[step: 835] loss: 20.596769332885742\n",
      "[step: 836] loss: 20.596439361572266\n",
      "[step: 837] loss: 20.596113204956055\n",
      "[step: 838] loss: 20.595787048339844\n",
      "[step: 839] loss: 20.59545135498047\n",
      "[step: 840] loss: 20.595130920410156\n",
      "[step: 841] loss: 20.594806671142578\n",
      "[step: 842] loss: 20.594472885131836\n",
      "[step: 843] loss: 20.594152450561523\n",
      "[step: 844] loss: 20.59382438659668\n",
      "[step: 845] loss: 20.593496322631836\n",
      "[step: 846] loss: 20.59316635131836\n",
      "[step: 847] loss: 20.592849731445312\n",
      "[step: 848] loss: 20.592514038085938\n",
      "[step: 849] loss: 20.592187881469727\n",
      "[step: 850] loss: 20.59186363220215\n",
      "[step: 851] loss: 20.59153938293457\n",
      "[step: 852] loss: 20.591211318969727\n",
      "[step: 853] loss: 20.590890884399414\n",
      "[step: 854] loss: 20.590557098388672\n",
      "[step: 855] loss: 20.590240478515625\n",
      "[step: 856] loss: 20.58991050720215\n",
      "[step: 857] loss: 20.58958625793457\n",
      "[step: 858] loss: 20.589258193969727\n",
      "[step: 859] loss: 20.58893394470215\n",
      "[step: 860] loss: 20.588605880737305\n",
      "[step: 861] loss: 20.588279724121094\n",
      "[step: 862] loss: 20.587961196899414\n",
      "[step: 863] loss: 20.5876407623291\n",
      "[step: 864] loss: 20.587308883666992\n",
      "[step: 865] loss: 20.586986541748047\n",
      "[step: 866] loss: 20.586660385131836\n",
      "[step: 867] loss: 20.58633041381836\n",
      "[step: 868] loss: 20.586009979248047\n",
      "[step: 869] loss: 20.585676193237305\n",
      "[step: 870] loss: 20.585359573364258\n",
      "[step: 871] loss: 20.585033416748047\n",
      "[step: 872] loss: 20.584701538085938\n",
      "[step: 873] loss: 20.584383010864258\n",
      "[step: 874] loss: 20.584062576293945\n",
      "[step: 875] loss: 20.5837345123291\n",
      "[step: 876] loss: 20.583419799804688\n",
      "[step: 877] loss: 20.583084106445312\n",
      "[step: 878] loss: 20.582765579223633\n",
      "[step: 879] loss: 20.58243751525879\n",
      "[step: 880] loss: 20.58211326599121\n",
      "[step: 881] loss: 20.581789016723633\n",
      "[step: 882] loss: 20.58146095275879\n",
      "[step: 883] loss: 20.581146240234375\n",
      "[step: 884] loss: 20.580821990966797\n",
      "[step: 885] loss: 20.580495834350586\n",
      "[step: 886] loss: 20.580175399780273\n",
      "[step: 887] loss: 20.57984733581543\n",
      "[step: 888] loss: 20.57952117919922\n",
      "[step: 889] loss: 20.579195022583008\n",
      "[step: 890] loss: 20.578876495361328\n",
      "[step: 891] loss: 20.578550338745117\n",
      "[step: 892] loss: 20.57823371887207\n",
      "[step: 893] loss: 20.57790184020996\n",
      "[step: 894] loss: 20.57756996154785\n",
      "[step: 895] loss: 20.57724952697754\n",
      "[step: 896] loss: 20.57693099975586\n",
      "[step: 897] loss: 20.576610565185547\n",
      "[step: 898] loss: 20.576282501220703\n",
      "[step: 899] loss: 20.57595443725586\n",
      "[step: 900] loss: 20.575626373291016\n",
      "[step: 901] loss: 20.57530975341797\n",
      "[step: 902] loss: 20.574989318847656\n",
      "[step: 903] loss: 20.574670791625977\n",
      "[step: 904] loss: 20.574337005615234\n",
      "[step: 905] loss: 20.574012756347656\n",
      "[step: 906] loss: 20.573684692382812\n",
      "[step: 907] loss: 20.5733699798584\n",
      "[step: 908] loss: 20.573036193847656\n",
      "[step: 909] loss: 20.57271385192871\n",
      "[step: 910] loss: 20.572397232055664\n",
      "[step: 911] loss: 20.572067260742188\n",
      "[step: 912] loss: 20.571744918823242\n",
      "[step: 913] loss: 20.5714168548584\n",
      "[step: 914] loss: 20.57109260559082\n",
      "[step: 915] loss: 20.570775985717773\n",
      "[step: 916] loss: 20.570444107055664\n",
      "[step: 917] loss: 20.57012367248535\n",
      "[step: 918] loss: 20.569799423217773\n",
      "[step: 919] loss: 20.569480895996094\n",
      "[step: 920] loss: 20.569149017333984\n",
      "[step: 921] loss: 20.56882095336914\n",
      "[step: 922] loss: 20.568506240844727\n",
      "[step: 923] loss: 20.56816864013672\n",
      "[step: 924] loss: 20.567852020263672\n",
      "[step: 925] loss: 20.567522048950195\n",
      "[step: 926] loss: 20.567209243774414\n",
      "[step: 927] loss: 20.566871643066406\n",
      "[step: 928] loss: 20.566551208496094\n",
      "[step: 929] loss: 20.566225051879883\n",
      "[step: 930] loss: 20.56590461730957\n",
      "[step: 931] loss: 20.565580368041992\n",
      "[step: 932] loss: 20.56525230407715\n",
      "[step: 933] loss: 20.564926147460938\n",
      "[step: 934] loss: 20.564605712890625\n",
      "[step: 935] loss: 20.564273834228516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 936] loss: 20.563945770263672\n",
      "[step: 937] loss: 20.563621520996094\n",
      "[step: 938] loss: 20.563304901123047\n",
      "[step: 939] loss: 20.562978744506836\n",
      "[step: 940] loss: 20.562646865844727\n",
      "[step: 941] loss: 20.562328338623047\n",
      "[step: 942] loss: 20.562002182006836\n",
      "[step: 943] loss: 20.561676025390625\n",
      "[step: 944] loss: 20.561351776123047\n",
      "[step: 945] loss: 20.561023712158203\n",
      "[step: 946] loss: 20.560693740844727\n",
      "[step: 947] loss: 20.56037139892578\n",
      "[step: 948] loss: 20.560041427612305\n",
      "[step: 949] loss: 20.55971336364746\n",
      "[step: 950] loss: 20.55939483642578\n",
      "[step: 951] loss: 20.559070587158203\n",
      "[step: 952] loss: 20.558734893798828\n",
      "[step: 953] loss: 20.558414459228516\n",
      "[step: 954] loss: 20.558086395263672\n",
      "[step: 955] loss: 20.557767868041992\n",
      "[step: 956] loss: 20.557432174682617\n",
      "[step: 957] loss: 20.557111740112305\n",
      "[step: 958] loss: 20.556785583496094\n",
      "[step: 959] loss: 20.556455612182617\n",
      "[step: 960] loss: 20.556133270263672\n",
      "[step: 961] loss: 20.555797576904297\n",
      "[step: 962] loss: 20.55547332763672\n",
      "[step: 963] loss: 20.555147171020508\n",
      "[step: 964] loss: 20.554819107055664\n",
      "[step: 965] loss: 20.554489135742188\n",
      "[step: 966] loss: 20.554166793823242\n",
      "[step: 967] loss: 20.553836822509766\n",
      "[step: 968] loss: 20.553512573242188\n",
      "[step: 969] loss: 20.553178787231445\n",
      "[step: 970] loss: 20.5528564453125\n",
      "[step: 971] loss: 20.552532196044922\n",
      "[step: 972] loss: 20.552202224731445\n",
      "[step: 973] loss: 20.551883697509766\n",
      "[step: 974] loss: 20.551549911499023\n",
      "[step: 975] loss: 20.55122184753418\n",
      "[step: 976] loss: 20.550886154174805\n",
      "[step: 977] loss: 20.550559997558594\n",
      "[step: 978] loss: 20.55023193359375\n",
      "[step: 979] loss: 20.549907684326172\n",
      "[step: 980] loss: 20.54958152770996\n",
      "[step: 981] loss: 20.549257278442383\n",
      "[step: 982] loss: 20.548919677734375\n",
      "[step: 983] loss: 20.54859733581543\n",
      "[step: 984] loss: 20.548261642456055\n",
      "[step: 985] loss: 20.547931671142578\n",
      "[step: 986] loss: 20.547605514526367\n",
      "[step: 987] loss: 20.54727554321289\n",
      "[step: 988] loss: 20.546951293945312\n",
      "[step: 989] loss: 20.54661750793457\n",
      "[step: 990] loss: 20.54629135131836\n",
      "[step: 991] loss: 20.545961380004883\n",
      "[step: 992] loss: 20.54563331604004\n",
      "[step: 993] loss: 20.545299530029297\n",
      "[step: 994] loss: 20.54497718811035\n",
      "[step: 995] loss: 20.544635772705078\n",
      "[step: 996] loss: 20.544315338134766\n",
      "[step: 997] loss: 20.543991088867188\n",
      "[step: 998] loss: 20.54365348815918\n",
      "[step: 999] loss: 20.54332160949707\n",
      "[step: 1000] loss: 20.54299545288086\n",
      "RMSE: 2.3538929894661207\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c1a77ac18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "def predict(file):\n",
    "    # train Parameters\n",
    "    seq_length = 6\n",
    "    data_dim = 1\n",
    "    hidden_dim = 12 # 내 맘대로 정해도 됨\n",
    "    output_dim = 1\n",
    "    learning_rate = 0.05\n",
    "    iterations = 1000\n",
    "    layer_num=1\n",
    "\n",
    "    # train Data: Open, High, Low, Volume, Close\n",
    "    origin_xy = np.loadtxt('train.csv', delimiter=',', skiprows=1, usecols=range(1,8))\n",
    "    xy = MinMaxScaler(origin_xy)\n",
    "    x = xy[:,:-1]\n",
    "    y = xy[:,-1]\n",
    "    x = x.reshape(-1,seq_length,data_dim)\n",
    "    y = y.reshape(-1,data_dim) \n",
    "\n",
    "    # train/validation split\n",
    "    train_size = int(len(y) * 0.7)\n",
    "    test_size = len(y) - train_size\n",
    "    trainX, validX = np.array(x[0:train_size]), np.array(x[train_size:])\n",
    "    trainY, validY = np.array(y[0:train_size]), np.array(y[train_size:])\n",
    "\n",
    "    #test data\n",
    "    test_x=np.loadtxt(file, delimiter=',', skiprows=1, usecols=range(1,7))\n",
    "    test_x = test_x.reshape(-1,seq_length, data_dim)\n",
    "    \n",
    "    # input place holders\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # build a LSTM network\n",
    "    cells=[]\n",
    "    for _ in range(layer_num):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "        #cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.7)\n",
    "        cells.append(cell)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "    # cost/loss\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # RMSE\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "    sess=tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i+1, step_loss))\n",
    "\n",
    "    # validation step\n",
    "    valid_predict = sess.run(Y_pred, feed_dict={X: validX})\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: validY, predictions: valid_predict})\n",
    "    revised_rmse = rmse_val*(origin_xy.max()-origin_xy.min()+1e-7) + origin_xy.min()\n",
    "    print(\"RMSE: {}\".format(revised_rmse))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(validY)\n",
    "    plt.plot(valid_predict)\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Temperature\")\n",
    "    plt.show()\n",
    "    \n",
    "    #test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: test_x})\n",
    "    prediction_list = []\n",
    "    for i in test_predict:\n",
    "        revised_pred = i[0]*(origin_xy.max()-origin_xy.min()+1e-7) + origin_xy.min()\n",
    "        prediction_list.append(revised_pred)\n",
    "    return prediction_list\n",
    "\n",
    "def write_result(predictions):\n",
    "    # You don't need to modify this function.\n",
    "    with open('result.csv', 'w') as f:\n",
    "        f.write('Value\\n')\n",
    "        for l in predictions:\n",
    "            f.write('{}\\n'.format(l))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # You don't need to modify this function.\n",
    "    predictions = predict('test.csv')\n",
    "    write_result(predictions)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # You don't need to modify this part.\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
