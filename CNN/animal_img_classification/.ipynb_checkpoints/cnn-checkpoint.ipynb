{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)\n",
    "#import matplotlib.pyplot as plt\n",
    "#from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the CIFAR-10\n",
    "def load_CIFAR10(pos, n_chunks=1):\n",
    "    Xtr = []\n",
    "    Ytr = []\n",
    "    for i in range(n_chunks):\n",
    "        train = unpickle(pos + '/data_batch_{0}'.format(i + 1))\n",
    "        #print(type(train))\n",
    "        Xtr.extend(train[b'data'])\n",
    "        Ytr.extend(train[b'labels'])\n",
    "        test = unpickle(pos + '/test_batch')\n",
    "        Xte = test[b'data']\n",
    "        Yte = test[b'labels']\n",
    "    return np.array(Xtr), np.array(Ytr), np.array(Xte), np.array(Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expresses the label data in one-hot encoding.\n",
    "def onehot_encoding (Ytr, Yte):\n",
    "    Ytr_onehot = np.zeros((Ytr.size, 10))\n",
    "    Yte_onehot = np.zeros((Yte.size, 10))\n",
    "    for i in range(Ytr.size):\n",
    "        Ytr_onehot[i][Ytr[i]] = 1\n",
    "    for i in range(Yte.size):\n",
    "        Yte_onehot[i][Yte[i]] = 1\n",
    "    return Ytr_onehot, Yte_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the train and test data\n",
    "Xtr, Ytr, Xte, Yte = load_CIFAR10('cifar-10-batches-py', 5)\n",
    "\n",
    "#print(Xtr.shape)\n",
    "# image data, each data size is 32*32*3\n",
    "Xtr = Xtr.reshape(50000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "Xte= Xte.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "#print(Xtr.shape)\n",
    "# label data of train and test data, label data is represented by one-hot encoding\n",
    "Ytr_onehot, Yte_onehot = onehot_encoding(Ytr, Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(Xtr.shape)\n",
    "print(Xtr[0:100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(Ytr_onehot.shape)\n",
    "print(Ytr_onehot[0:100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "\n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "            # Layer1\n",
    "            conv1 = tf.layers.conv2d(inputs=self.X, filters=32, kernel_size=[3, 3],padding=\"SAME\", activation=tf.nn.relu)\n",
    "            norm1=tf.contrib.layers.batch_norm(conv1)\n",
    "            #dropout1 = tf.layers.dropout(inputs=pool1,rate=Model.prob, training=self.training)\n",
    "\n",
    "            #Layer2\n",
    "            conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3, 3],padding=\"SAME\", activation=tf.nn.relu)\n",
    "            norm2=tf.contrib.layers.batch_norm(conv2)\n",
    "            #pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], padding=\"SAME\", strides=1)\n",
    "            #dropout2 = tf.layers.dropout(inputs=norm2,rate=Model.prob, training=self.training)\n",
    "\n",
    "            # Layer3\n",
    "            conv3 = tf.layers.conv2d(inputs=norm2, filters=128, kernel_size=[3, 3], padding=\"SAME\", activation=tf.nn.relu)\n",
    "            norm3=tf.contrib.layers.batch_norm(conv3)\n",
    "            #dropout3 = tf.layers.dropout(inputs=norm3,rate=Model.prob, training=self.training)\n",
    "\n",
    "            # Layer4\n",
    "            conv4 = tf.layers.conv2d(inputs= norm3, filters=256, kernel_size=[3, 3],padding=\"SAME\", activation=tf.nn.relu)\n",
    "            norm4=tf.contrib.layers.batch_norm(conv4)\n",
    "            #dropout4 = tf.layers.dropout(inputs=norm4,rate=Model.prob, training=self.training)\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=norm4, pool_size=[2, 2],padding=\"SAME\", strides=2)\n",
    "            dropout5 = tf.layers.dropout(inputs=pool2,rate=0.7, training=self.training)\n",
    "            \n",
    "            # Dense Layer with Relu\n",
    "            flat = tf.reshape(dropout5, [-1, 256 * 16 * 16])\n",
    "            dense = tf.layers.dense(inputs=flat, units=256, activation=tf.nn.relu)\n",
    "            \n",
    "            norm5=tf.contrib.layers.batch_norm(dense)\n",
    "\n",
    "            # Logits (no activation) Layer: L5 Final FC 256 inputs -> 10 outputs\n",
    "\n",
    "            self.W = tf.get_variable(\"W\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.b = tf.Variable(tf.random_normal([10]))\n",
    "            self.model = tf.matmul(norm5, self.W) + self.b\n",
    "\n",
    "        # define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.model, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(self.model, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        return self.sess.run(self.model,feed_dict={self.X: x_test, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X: x_test,self.Y: y_test, self.training: training})\n",
    "\n",
    "    def train(self, x_data, y_data, training=True):\n",
    "        return self.sess.run([self.optimizer, self.cost, self.accuracy], feed_dict={\n",
    "            self.X: x_data, self.Y: y_data, self.training: training})\n",
    "    \n",
    "    def test(self, x_data, y_data, training=False):\n",
    "        return self.sess.run([self.model, self.accuracy], feed_dict={self.X: x_data, self.Y: y_data, self.training:training})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.002\n",
    "training_epochs = 50\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Started!\n",
      "Epoch: 0001 cost = [ 1.35271633] train accuracy = [ 0.52562]\n",
      "Epoch: 0002 cost = [ 0.90021647] train accuracy = [ 0.68186]\n",
      "Epoch: 0003 cost = [ 0.69779753] train accuracy = [ 0.7543]\n",
      "Epoch: 0004 cost = [ 0.55193843] train accuracy = [ 0.8077]\n",
      "Epoch: 0005 cost = [ 0.41887396] train accuracy = [ 0.8528]\n",
      "Epoch: 0006 cost = [ 0.32174959] train accuracy = [ 0.88626]\n",
      "Epoch: 0007 cost = [ 0.2405612] train accuracy = [ 0.91432]\n",
      "Epoch: 0008 cost = [ 0.18384681] train accuracy = [ 0.93656]\n",
      "Epoch: 0009 cost = [ 0.15021277] train accuracy = [ 0.94798]\n",
      "Epoch: 0010 cost = [ 0.12628206] train accuracy = [ 0.95724]\n",
      "Epoch: 0011 cost = [ 0.10688589] train accuracy = [ 0.96362]\n",
      "Epoch: 0012 cost = [ 0.09556276] train accuracy = [ 0.96736001]\n",
      "Epoch: 0013 cost = [ 0.09306888] train accuracy = [ 0.96802001]\n",
      "Epoch: 0014 cost = [ 0.08243235] train accuracy = [ 0.97122001]\n",
      "Epoch: 0015 cost = [ 0.07116789] train accuracy = [ 0.97540001]\n",
      "Epoch: 0016 cost = [ 0.0693458] train accuracy = [ 0.97608001]\n",
      "Epoch: 0017 cost = [ 0.06156514] train accuracy = [ 0.97934001]\n",
      "Epoch: 0018 cost = [ 0.06067322] train accuracy = [ 0.97934001]\n",
      "Epoch: 0019 cost = [ 0.06105591] train accuracy = [ 0.97892001]\n",
      "Epoch: 0020 cost = [ 0.05098861] train accuracy = [ 0.98306001]\n",
      "Epoch: 0021 cost = [ 0.04830713] train accuracy = [ 0.98346001]\n",
      "Epoch: 0022 cost = [ 0.05361515] train accuracy = [ 0.98102001]\n",
      "Epoch: 0023 cost = [ 0.04640934] train accuracy = [ 0.98416001]\n",
      "Epoch: 0024 cost = [ 0.04053553] train accuracy = [ 0.98606001]\n",
      "Epoch: 0025 cost = [ 0.04160259] train accuracy = [ 0.98542001]\n",
      "Epoch: 0026 cost = [ 0.04560962] train accuracy = [ 0.98472001]\n",
      "Epoch: 0027 cost = [ 0.03883167] train accuracy = [ 0.98670001]\n",
      "Epoch: 0028 cost = [ 0.03933782] train accuracy = [ 0.98666001]\n",
      "Epoch: 0029 cost = [ 0.03407225] train accuracy = [ 0.98782001]\n",
      "Epoch: 0030 cost = [ 0.03493297] train accuracy = [ 0.98870001]\n",
      "Epoch: 0031 cost = [ 0.03567595] train accuracy = [ 0.98756001]\n",
      "Epoch: 0032 cost = [ 0.03041432] train accuracy = [ 0.98946001]\n",
      "Epoch: 0033 cost = [ 0.03064437] train accuracy = [ 0.98934001]\n",
      "Epoch: 0034 cost = [ 0.03036512] train accuracy = [ 0.98982001]\n",
      "Epoch: 0035 cost = [ 0.03063024] train accuracy = [ 0.98956001]\n",
      "Epoch: 0036 cost = [ 0.0294858] train accuracy = [ 0.99004001]\n",
      "Epoch: 0037 cost = [ 0.02717074] train accuracy = [ 0.99080001]\n",
      "Epoch: 0038 cost = [ 0.02826722] train accuracy = [ 0.99002001]\n",
      "Epoch: 0039 cost = [ 0.02945508] train accuracy = [ 0.98992001]\n",
      "Epoch: 0040 cost = [ 0.02481047] train accuracy = [ 0.99154001]\n",
      "Epoch: 0041 cost = [ 0.02169712] train accuracy = [ 0.99286001]\n",
      "Epoch: 0042 cost = [ 0.02187127] train accuracy = [ 0.99200001]\n",
      "Epoch: 0043 cost = [ 0.02533298] train accuracy = [ 0.99112001]\n",
      "Epoch: 0044 cost = [ 0.02256274] train accuracy = [ 0.99182001]\n",
      "Epoch: 0045 cost = [ 0.02496087] train accuracy = [ 0.99148001]\n",
      "Epoch: 0046 cost = [ 0.02435636] train accuracy = [ 0.99182001]\n",
      "Epoch: 0047 cost = [ 0.02252104] train accuracy = [ 0.99228001]\n",
      "Epoch: 0048 cost = [ 0.02007188] train accuracy = [ 0.99322001]\n",
      "Epoch: 0049 cost = [ 0.0206115] train accuracy = [ 0.99296001]\n",
      "Epoch: 0050 cost = [ 0.01891353] train accuracy = [ 0.99334001]\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "\n",
    "models = []\n",
    "num_models = 1\n",
    "for m in range(num_models):\n",
    "    models.append(Model(sess, \"model\" + str(m)))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Learning Started!')\n",
    "\n",
    "# train & test my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost_list = np.zeros(len(models))\n",
    "    train_accuracy= np.zeros(len(models))\n",
    "    total_batch = int(len(Xtr)/ batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        num=i*batch_size\n",
    "        batch_xs, batch_ys = Xtr[num:num+batch_size], Ytr_onehot[num:num+batch_size]\n",
    "        # train each model\n",
    "        for m_idx, m in enumerate(models):\n",
    "            _, c, a = m.train(batch_xs, batch_ys)\n",
    "            avg_cost_list[m_idx] += c/total_batch\n",
    "            train_accuracy[m_idx] += a/total_batch\n",
    "            \n",
    "    print('Epoch:','%04d' % (epoch + 1), 'cost =', avg_cost_list, 'train accuracy =',train_accuracy)\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = [ 0.7914]\n"
     ]
    }
   ],
   "source": [
    "#Test model\n",
    "test_batch_size=500\n",
    "total_test_batch = int(len(Xte)/ test_batch_size)\n",
    "test_accuracy = np.zeros(len(models))\n",
    "\n",
    "for j in range(total_test_batch):\n",
    "    num=j*test_batch_size\n",
    "    batch_xt, batch_yt = Xte[num:num+test_batch_size], Yte_onehot[num:num+test_batch_size]\n",
    "    #test ecah model\n",
    "    for m_idx, m in enumerate(models):\n",
    "        _, a = m.test(batch_xt, batch_yt) \n",
    "        test_accuracy[m_idx]+= a/total_test_batch\n",
    "print('Test Accuracy =', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
