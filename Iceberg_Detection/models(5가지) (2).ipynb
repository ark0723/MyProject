{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import pickle, random, copy, platform, os, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import email_sending\n",
    "import tflearn\n",
    "import tflearn.activations as activations\n",
    "from PIL import Image, ImageDraw\n",
    "from __future__ import division, print_function, absolute_import\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.activations import relu\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.conv import avg_pool_2d, conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import dropout, flatten, fully_connected, input_data\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "from tflearn.utils import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    with open('resnet_data', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        Xtr = data[0]\n",
    "        Xtr = np.reshape(Xtr, (-1,75,75,2))\n",
    "        Ytr = data[1]\n",
    "        Xte =data[2]\n",
    "        Xte = np.reshape(Xte, (-1,75,75,2))\n",
    "        rf =data[3]\n",
    "    return Xtr, Ytr, Xte, rf\n",
    "\n",
    "# expresses the label data in one-hot encoding.\n",
    "def onehot_encoding (Y, y_class):\n",
    "    Y_onehot = np.zeros((Y.size, int(y_class)))\n",
    "    for i in range(Y.size):\n",
    "        Y_onehot[i][Y[i]] = 1\n",
    "    return Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet32(x, classes, n = 5): #classes =2\n",
    "    net = tflearn.conv_2d(x, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "    net = tflearn.residual_block(net, n, 16)\n",
    "    net = tflearn.residual_block(net, 1, 32, downsample=True)\n",
    "    net = tflearn.residual_block(net, n - 1, 32)\n",
    "    net = tflearn.residual_block(net, 1, 64, downsample=True)\n",
    "    net = tflearn.residual_block(net, n - 1, 64)\n",
    "    net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.activation(net, 'relu')\n",
    "    net = tflearn.global_avg_pool(net)\n",
    "    net = tflearn.fully_connected(net, classes, activation='softmax')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnext(x, classes, n =5):\n",
    "    net = tflearn.conv_2d(x, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "    net = tflearn.resnext_block(net, n, 16, 32)\n",
    "    net = tflearn.resnext_block(net, 1, 32, 32, downsample=True)\n",
    "    net = tflearn.resnext_block(net, n-1, 32, 32)\n",
    "    net = tflearn.resnext_block(net, 1, 64, 32, downsample=True)\n",
    "    net = tflearn.resnext_block(net, n-1, 64, 32)\n",
    "    net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.activation(net, 'relu')\n",
    "    net = tflearn.global_avg_pool(net)\n",
    "    net = tflearn.fully_connected(net, classes, activation='softmax')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg(x, classes):\n",
    "    \n",
    "    network = conv_2d(x, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, classes, activation='softmax')\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def block35(net, scale=1.0, activation=\"relu\"):\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None,name='Conv2d_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 32, 3, bias=False, activation=None,name='Conv2d_0b_3x3')))\n",
    "    tower_conv2_0 = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2_0, 48,3, bias=False, activation=None, name='Conv2d_0b_3x3')))\n",
    "    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 64,3, bias=False, activation=None, name='Conv2d_0c_3x3')))\n",
    "    tower_mixed = merge([tower_conv, tower_conv1_1, tower_conv2_2], mode='concat', axis=3)\n",
    "    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    net += scale * tower_out\n",
    "    if activation:\n",
    "        if isinstance(activation, str):\n",
    "            net = activations.get(activation)(net)\n",
    "        elif hasattr(activation, '__call__'):\n",
    "            net = activation(net)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation.\")\n",
    "    return net\n",
    "\n",
    "def block17(net, scale=1.0, activation=\"relu\"):\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    tower_conv_1_0 = relu(batch_normalization(conv_2d(net, 128, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv_1_1 = relu(batch_normalization(conv_2d(tower_conv_1_0, 160,[1,7], bias=False, activation=None,name='Conv2d_0b_1x7')))\n",
    "    tower_conv_1_2 = relu(batch_normalization(conv_2d(tower_conv_1_1, 192, [7,1], bias=False, activation=None,name='Conv2d_0c_7x1')))\n",
    "    tower_mixed = merge([tower_conv,tower_conv_1_2], mode='concat', axis=3)\n",
    "    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    net += scale * tower_out\n",
    "    if activation:\n",
    "        if isinstance(activation, str):\n",
    "            net = activations.get(activation)(net)\n",
    "        elif hasattr(activation, '__call__'):\n",
    "            net = activation(net)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation.\")\n",
    "    return net\n",
    "\n",
    "\n",
    "def block8(net, scale=1.0, activation=\"relu\"):\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 224, [1,3], bias=False, activation=None, name='Conv2d_0b_1x3')))\n",
    "    tower_conv1_2 = relu(batch_normalization(conv_2d(tower_conv1_1, 256, [3,1], bias=False, name='Conv2d_0c_3x1')))\n",
    "    tower_mixed = merge([tower_conv,tower_conv1_2], mode='concat', axis=3)\n",
    "    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    net += scale * tower_out\n",
    "    if activation:\n",
    "        if isinstance(activation, str):\n",
    "            net = activations.get(activation)(net)\n",
    "        elif hasattr(activation, '__call__'):\n",
    "            net = activation(net)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation.\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inception_resnet_v2(x, classes):\n",
    "\n",
    "    dropout_keep_prob = 0.8\n",
    "\n",
    "    conv1a_3_3 = relu(batch_normalization(conv_2d(x, 32, 3, strides=2, bias=False, padding='VALID',activation=None,name='Conv2d_1a_3x3')))\n",
    "    conv2a_3_3 = relu(batch_normalization(conv_2d(conv1a_3_3, 32, 3, bias=False, padding='VALID',activation=None, name='Conv2d_2a_3x3')))\n",
    "    conv2b_3_3 = relu(batch_normalization(conv_2d(conv2a_3_3, 64, 3, bias=False, activation=None, name='Conv2d_2b_3x3')))\n",
    "    maxpool3a_3_3 = max_pool_2d(conv2b_3_3, 3, strides=2, padding='VALID', name='MaxPool_3a_3x3')\n",
    "    conv3b_1_1 = relu(batch_normalization(conv_2d(maxpool3a_3_3, 80, 1, bias=False, padding='VALID',activation=None, name='Conv2d_3b_1x1')))\n",
    "    conv4a_3_3 = relu(batch_normalization(conv_2d(conv3b_1_1, 192, 3, bias=False, padding='VALID',activation=None, name='Conv2d_4a_3x3')))\n",
    "    maxpool5a_3_3 = max_pool_2d(conv4a_3_3, 3, strides=2, padding='VALID', name='MaxPool_5a_3x3')\n",
    "\n",
    "    tower_conv = relu(batch_normalization(conv_2d(maxpool5a_3_3, 96, 1, bias=False, activation=None, name='Conv2d_5b_b0_1x1')))\n",
    "\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(maxpool5a_3_3, 48, 1, bias=False, activation=None, name='Conv2d_5b_b1_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 64, 5, bias=False, activation=None, name='Conv2d_5b_b1_0b_5x5')))\n",
    "\n",
    "    tower_conv2_0 = relu(batch_normalization(conv_2d(maxpool5a_3_3, 64, 1, bias=False, activation=None, name='Conv2d_5b_b2_0a_1x1')))\n",
    "    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2_0, 96, 3, bias=False, activation=None, name='Conv2d_5b_b2_0b_3x3')))\n",
    "    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 96, 3, bias=False, activation=None,name='Conv2d_5b_b2_0c_3x3')))\n",
    "\n",
    "    tower_pool3_0 = avg_pool_2d(maxpool5a_3_3, 3, strides=1, padding='same', name='AvgPool_5b_b3_0a_3x3')\n",
    "    tower_conv3_1 = relu(batch_normalization(conv_2d(tower_pool3_0, 64, 1, bias=False, activation=None,name='Conv2d_5b_b3_0b_1x1')))\n",
    "\n",
    "    tower_5b_out = merge([tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_1], mode='concat', axis=3)\n",
    "\n",
    "    net = repeat(tower_5b_out, 10, block35, scale=0.17)\n",
    "\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 384, 3, bias=False, strides=2,activation=None, padding='VALID', name='Conv2d_6a_b0_0a_3x3')))\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, activation=None, name='Conv2d_6a_b1_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 256, 3, bias=False, activation=None, name='Conv2d_6a_b1_0b_3x3')))\n",
    "    tower_conv1_2 = relu(batch_normalization(conv_2d(tower_conv1_1, 384, 3, bias=False, strides=2, padding='VALID', activation=None,name='Conv2d_6a_b1_0c_3x3')))\n",
    "    tower_pool = max_pool_2d(net, 3, strides=2, padding='VALID',name='MaxPool_1a_3x3')\n",
    "    net = merge([tower_conv, tower_conv1_2, tower_pool], mode='concat', axis=3)\n",
    "    net = repeat(net, 20, block17, scale=0.1)\n",
    "\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv0_1 = relu(batch_normalization(conv_2d(tower_conv, 384, 3, bias=False, strides=2, padding='VALID', activation=None,name='Conv2d_0a_1x1')))\n",
    "\n",
    "    tower_conv1 = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, padding='VALID', activation=None,name='Conv2d_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1,288,3, bias=False, strides=2, padding='VALID',activation=None, name='COnv2d_1a_3x3')))\n",
    "\n",
    "    tower_conv2 = relu(batch_normalization(conv_2d(net, 256,1, bias=False, activation=None,name='Conv2d_0a_1x1')))\n",
    "    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2, 288,3, bias=False, name='Conv2d_0b_3x3',activation=None)))\n",
    "    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 320, 3, bias=False, strides=2, padding='VALID',activation=None, name='Conv2d_1a_3x3')))\n",
    "\n",
    "    tower_pool = max_pool_2d(net, 3, strides=2, padding='VALID', name='MaxPool_1a_3x3')\n",
    "    net = merge([tower_conv0_1, tower_conv1_1,tower_conv2_2, tower_pool], mode='concat', axis=3)\n",
    "\n",
    "    net = repeat(net, 9, block8, scale=0.2)\n",
    "    net = block8(net, activation=None)\n",
    "\n",
    "    net = relu(batch_normalization(conv_2d(net, 1536, 1, bias=False, activation=None, name='Conv2d_7b_1x1')))\n",
    "    net = avg_pool_2d(net, net.get_shape().as_list()[1:3],strides=2, padding='VALID', name='AvgPool_1a_8x8')\n",
    "    net = flatten(net)\n",
    "    net = dropout(net, dropout_keep_prob)\n",
    "    loss = fully_connected(net, classes,activation='softmax')\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def googlenet(x, classes):\n",
    "    conv1_7_7 = conv_2d(x, 64, 7, strides=2, activation='relu', name='conv1_7_7_s2')\n",
    "    pool1_3_3 = max_pool_2d(conv1_7_7, 3, strides=2)\n",
    "    pool1_3_3 = local_response_normalization(pool1_3_3)\n",
    "    conv2_3_3_reduce = conv_2d(pool1_3_3, 64, 1, activation='relu', name='conv2_3_3_reduce')\n",
    "    conv2_3_3 = conv_2d(conv2_3_3_reduce, 192, 3, activation='relu', name='conv2_3_3')\n",
    "    conv2_3_3 = local_response_normalization(conv2_3_3)\n",
    "    pool2_3_3 = max_pool_2d(conv2_3_3, kernel_size=3, strides=2, name='pool2_3_3_s2')\n",
    "\n",
    "    # 3a\n",
    "    inception_3a_1_1 = conv_2d(pool2_3_3, 64, 1, activation='relu', name='inception_3a_1_1')\n",
    "    inception_3a_3_3_reduce = conv_2d(pool2_3_3, 96, 1, activation='relu', name='inception_3a_3_3_reduce')\n",
    "    inception_3a_3_3 = conv_2d(inception_3a_3_3_reduce, 128, filter_size=3,  activation='relu', name='inception_3a_3_3')\n",
    "    inception_3a_5_5_reduce = conv_2d(pool2_3_3, 16, filter_size=1, activation='relu', name='inception_3a_5_5_reduce')\n",
    "    inception_3a_5_5 = conv_2d(inception_3a_5_5_reduce, 32, filter_size=5, activation='relu', name='inception_3a_5_5')\n",
    "    inception_3a_pool = max_pool_2d(pool2_3_3, kernel_size=3, strides=1, name='inception_3a_pool')\n",
    "    inception_3a_pool_1_1 = conv_2d(inception_3a_pool, 32, filter_size=1, activation='relu', name='inception_3a_pool_1_1')\n",
    "    inception_3a_output = merge([inception_3a_1_1, inception_3a_3_3, inception_3a_5_5, inception_3a_pool_1_1], mode='concat', axis=3)\n",
    "\n",
    "    # 3b\n",
    "    inception_3b_1_1 = conv_2d(inception_3a_output, 128, filter_size=1, activation='relu', name='inception_3b_1_1')\n",
    "    inception_3b_3_3_reduce = conv_2d(inception_3a_output, 128, filter_size=1, activation='relu', name='inception_3b_3_3_reduce')\n",
    "    inception_3b_3_3 = conv_2d(inception_3b_3_3_reduce, 192, filter_size=3, activation='relu', name='inception_3b_3_3')\n",
    "    inception_3b_5_5_reduce = conv_2d(inception_3a_output, 32, filter_size=1, activation='relu', name='inception_3b_5_5_reduce')\n",
    "    inception_3b_5_5 = conv_2d(inception_3b_5_5_reduce, 96, filter_size=5,  name='inception_3b_5_5')\n",
    "    inception_3b_pool = max_pool_2d(inception_3a_output, kernel_size=3, strides=1,  name='inception_3b_pool')\n",
    "    inception_3b_pool_1_1 = conv_2d(inception_3b_pool, 64, filter_size=1, activation='relu', name='inception_3b_pool_1_1')\n",
    "    inception_3b_output = merge([inception_3b_1_1, inception_3b_3_3, inception_3b_5_5, inception_3b_pool_1_1], mode='concat', axis=3, name='inception_3b_output')\n",
    "    pool3_3_3 = max_pool_2d(inception_3b_output, kernel_size=3, strides=2, name='pool3_3_3')\n",
    "\n",
    "    # 4a\n",
    "    inception_4a_1_1 = conv_2d(pool3_3_3, 192, filter_size=1, activation='relu', name='inception_4a_1_1')\n",
    "    inception_4a_3_3_reduce = conv_2d(pool3_3_3, 96, filter_size=1, activation='relu', name='inception_4a_3_3_reduce')\n",
    "    inception_4a_3_3 = conv_2d(inception_4a_3_3_reduce, 208, filter_size=3,  activation='relu', name='inception_4a_3_3')\n",
    "    inception_4a_5_5_reduce = conv_2d(pool3_3_3, 16, filter_size=1, activation='relu', name='inception_4a_5_5_reduce')\n",
    "    inception_4a_5_5 = conv_2d(inception_4a_5_5_reduce, 48, filter_size=5,  activation='relu', name='inception_4a_5_5')\n",
    "    inception_4a_pool = max_pool_2d(pool3_3_3, kernel_size=3, strides=1,  name='inception_4a_pool')\n",
    "    inception_4a_pool_1_1 = conv_2d(inception_4a_pool, 64, filter_size=1, activation='relu', name='inception_4a_pool_1_1')\n",
    "    inception_4a_output = merge([inception_4a_1_1, inception_4a_3_3, inception_4a_5_5, inception_4a_pool_1_1], mode='concat', axis=3, name='inception_4a_output')\n",
    "\n",
    "    # 4b\n",
    "    inception_4b_1_1 = conv_2d(inception_4a_output, 160, filter_size=1, activation='relu', name='inception_4a_1_1')\n",
    "    inception_4b_3_3_reduce = conv_2d(inception_4a_output, 112, filter_size=1, activation='relu', name='inception_4b_3_3_reduce')\n",
    "    inception_4b_3_3 = conv_2d(inception_4b_3_3_reduce, 224, filter_size=3, activation='relu', name='inception_4b_3_3')\n",
    "    inception_4b_5_5_reduce = conv_2d(inception_4a_output, 24, filter_size=1, activation='relu', name='inception_4b_5_5_reduce')\n",
    "    inception_4b_5_5 = conv_2d(inception_4b_5_5_reduce, 64, filter_size=5,  activation='relu', name='inception_4b_5_5')\n",
    "    inception_4b_pool = max_pool_2d(inception_4a_output, kernel_size=3, strides=1,  name='inception_4b_pool')\n",
    "    inception_4b_pool_1_1 = conv_2d(inception_4b_pool, 64, filter_size=1, activation='relu', name='inception_4b_pool_1_1')\n",
    "    inception_4b_output = merge([inception_4b_1_1, inception_4b_3_3, inception_4b_5_5, inception_4b_pool_1_1], mode='concat', axis=3, name='inception_4b_output')\n",
    "\n",
    "    # 4c\n",
    "    inception_4c_1_1 = conv_2d(inception_4b_output, 128, filter_size=1, activation='relu', name='inception_4c_1_1')\n",
    "    inception_4c_3_3_reduce = conv_2d(inception_4b_output, 128, filter_size=1, activation='relu', name='inception_4c_3_3_reduce')\n",
    "    inception_4c_3_3 = conv_2d(inception_4c_3_3_reduce, 256,  filter_size=3, activation='relu', name='inception_4c_3_3')\n",
    "    inception_4c_5_5_reduce = conv_2d(inception_4b_output, 24, filter_size=1, activation='relu', name='inception_4c_5_5_reduce')\n",
    "    inception_4c_5_5 = conv_2d(inception_4c_5_5_reduce, 64,  filter_size=5, activation='relu', name='inception_4c_5_5')\n",
    "    inception_4c_pool = max_pool_2d(inception_4b_output, kernel_size=3, strides=1)\n",
    "    inception_4c_pool_1_1 = conv_2d(inception_4c_pool, 64, filter_size=1, activation='relu', name='inception_4c_pool_1_1')\n",
    "    inception_4c_output = merge([inception_4c_1_1, inception_4c_3_3, inception_4c_5_5, inception_4c_pool_1_1], mode='concat', axis=3, name='inception_4c_output')\n",
    "\n",
    "    # 4d\n",
    "    inception_4d_1_1 = conv_2d(inception_4c_output, 112, filter_size=1, activation='relu', name='inception_4d_1_1')\n",
    "    inception_4d_3_3_reduce = conv_2d(inception_4c_output, 144, filter_size=1, activation='relu', name='inception_4d_3_3_reduce')\n",
    "    inception_4d_3_3 = conv_2d(inception_4d_3_3_reduce, 288, filter_size=3, activation='relu', name='inception_4d_3_3')\n",
    "    inception_4d_5_5_reduce = conv_2d(inception_4c_output, 32, filter_size=1, activation='relu', name='inception_4d_5_5_reduce')\n",
    "    inception_4d_5_5 = conv_2d(inception_4d_5_5_reduce, 64, filter_size=5,  activation='relu', name='inception_4d_5_5')\n",
    "    inception_4d_pool = max_pool_2d(inception_4c_output, kernel_size=3, strides=1,  name='inception_4d_pool')\n",
    "    inception_4d_pool_1_1 = conv_2d(inception_4d_pool, 64, filter_size=1, activation='relu', name='inception_4d_pool_1_1')\n",
    "    inception_4d_output = merge([inception_4d_1_1, inception_4d_3_3, inception_4d_5_5, inception_4d_pool_1_1], mode='concat', axis=3, name='inception_4d_output')\n",
    "\n",
    "    # 4e\n",
    "    inception_4e_1_1 = conv_2d(inception_4d_output, 256, filter_size=1, activation='relu', name='inception_4e_1_1')\n",
    "    inception_4e_3_3_reduce = conv_2d(inception_4d_output, 160, filter_size=1, activation='relu', name='inception_4e_3_3_reduce')\n",
    "    inception_4e_3_3 = conv_2d(inception_4e_3_3_reduce, 320, filter_size=3, activation='relu', name='inception_4e_3_3')\n",
    "    inception_4e_5_5_reduce = conv_2d(inception_4d_output, 32, filter_size=1, activation='relu', name='inception_4e_5_5_reduce')\n",
    "    inception_4e_5_5 = conv_2d(inception_4e_5_5_reduce, 128,  filter_size=5, activation='relu', name='inception_4e_5_5')\n",
    "    inception_4e_pool = max_pool_2d(inception_4d_output, kernel_size=3, strides=1,  name='inception_4e_pool')\n",
    "    inception_4e_pool_1_1 = conv_2d(inception_4e_pool, 128, filter_size=1, activation='relu', name='inception_4e_pool_1_1')\n",
    "    inception_4e_output = merge([inception_4e_1_1, inception_4e_3_3, inception_4e_5_5, inception_4e_pool_1_1], axis=3, mode='concat')\n",
    "    pool4_3_3 = max_pool_2d(inception_4e_output, kernel_size=3, strides=2, name='pool_3_3')\n",
    "\n",
    "    # 5a\n",
    "    inception_5a_1_1 = conv_2d(pool4_3_3, 256, filter_size=1, activation='relu', name='inception_5a_1_1')\n",
    "    inception_5a_3_3_reduce = conv_2d(pool4_3_3, 160, filter_size=1, activation='relu', name='inception_5a_3_3_reduce')\n",
    "    inception_5a_3_3 = conv_2d(inception_5a_3_3_reduce, 320, filter_size=3, activation='relu', name='inception_5a_3_3')\n",
    "    inception_5a_5_5_reduce = conv_2d(pool4_3_3, 32, filter_size=1, activation='relu', name='inception_5a_5_5_reduce')\n",
    "    inception_5a_5_5 = conv_2d(inception_5a_5_5_reduce, 128, filter_size=5,  activation='relu', name='inception_5a_5_5')\n",
    "    inception_5a_pool = max_pool_2d(pool4_3_3, kernel_size=3, strides=1,  name='inception_5a_pool')\n",
    "    inception_5a_pool_1_1 = conv_2d(inception_5a_pool, 128, filter_size=1, activation='relu', name='inception_5a_pool_1_1')\n",
    "    inception_5a_output = merge([inception_5a_1_1, inception_5a_3_3, inception_5a_5_5, inception_5a_pool_1_1], axis=3, mode='concat')\n",
    "\n",
    "    # 5b\n",
    "    inception_5b_1_1 = conv_2d(inception_5a_output, 384, filter_size=1, activation='relu', name='inception_5b_1_1')\n",
    "    inception_5b_3_3_reduce = conv_2d(inception_5a_output, 192, filter_size=1, activation='relu', name='inception_5b_3_3_reduce')\n",
    "    inception_5b_3_3 = conv_2d(inception_5b_3_3_reduce, 384,  filter_size=3, activation='relu', name='inception_5b_3_3')\n",
    "    inception_5b_5_5_reduce = conv_2d(inception_5a_output, 48, filter_size=1, activation='relu', name='inception_5b_5_5_reduce')\n",
    "    inception_5b_5_5 = conv_2d(inception_5b_5_5_reduce, 128, filter_size=5, activation='relu', name='inception_5b_5_5')\n",
    "    inception_5b_pool = max_pool_2d(inception_5a_output, kernel_size=3, strides=1,  name='inception_5b_pool')\n",
    "    inception_5b_pool_1_1 = conv_2d(inception_5b_pool, 128, filter_size=1, activation='relu', name='inception_5b_pool_1_1')\n",
    "    inception_5b_output = merge([inception_5b_1_1, inception_5b_3_3, inception_5b_5_5, inception_5b_pool_1_1], axis=3, mode='concat')\n",
    "    pool5_7_7 = avg_pool_2d(inception_5b_output, kernel_size=7, strides=1)\n",
    "    pool5_7_7 = dropout(pool5_7_7, 0.4)\n",
    "\n",
    "    # fc\n",
    "    net = fully_connected(pool5_7_7, classes, activation='softmax')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.63141\u001b[0m\u001b[0m | time: 33.456s\n",
      "| Adam | epoch: 001 | loss: 0.63141 - acc: 0.6456 -- iter: 1100/1200\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.62108\u001b[0m\u001b[0m | time: 37.696s\n",
      "| Adam | epoch: 001 | loss: 0.62108 - acc: 0.6926 | val_loss: 1.72386 - val_acc: 0.5400 -- iter: 1200/1200\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\User\\Desktop\\kaggle\\tmp-12 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "def train(num_epochs=10, mode ='resnet32'):\n",
    "    batch_size = 100\n",
    "    classes = 2\n",
    "\n",
    "    img_prep = tflearn.ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "    img_aug = tflearn.ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_90degrees_rotation(rotations=[0, 1, 2, 3])\n",
    "    img_aug.add_random_crop([75,75,2], padding=4)\n",
    "\n",
    "    Input = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "\n",
    "    Xtr, Ytr, Xte, ref = read_data()\n",
    "    Xtr, Ytr = tflearn.data_utils.shuffle(Xtr, Ytr)\n",
    "    Ytr = onehot_encoding(Ytr, classes)\n",
    "    X, Y = Xtr[:1200], Ytr[:1200]\n",
    "    testX, testY =Xtr[1200:1600], Ytr[1200:1600]\n",
    "    \n",
    "    \n",
    "    if mode =='resnet32':\n",
    "        network = tflearn.regression(resnet32(Input,classes),optimizer='adam',loss='categorical_crossentropy')\n",
    "        model = tflearn.DNN(network,tensorboard_verbose=3, checkpoint_path='./tmp/')\n",
    "        model.fit(X, Y, n_epoch=num_epochs, shuffle=True, validation_set=(testX, testY),\n",
    "                  show_metric=True, batch_size=batch_size, run_id='resnet32')\n",
    "        predict(model, Xte, ref, mode)\n",
    "        \n",
    "    elif mode =='inception_resnet_v2':        \n",
    "        network = tflearn.regression(inception_resnet_v2(Input,classes), optimizer='adam',loss='categorical_crossentropy')\n",
    "        model = tflearn.DNN(network,tensorboard_verbose=3, checkpoint_path='./tmp/')\n",
    "        model.fit(X, Y, n_epoch=num_epochs, shuffle=True, show_metric=True, validation_set=(testX, testY),\n",
    "                  batch_size=batch_size, run_id='inception_resnet_v2')\n",
    "        predict(model, Xte, ref, mode)\n",
    "        \n",
    "    elif mode=='resnext':\n",
    "        network = tflearn.regression(resnext(Input,classes), optimizer='adam',loss='categorical_crossentropy')\n",
    "        model = tflearn.DNN(network, checkpoint_path='./tmp/', tensorboard_verbose=3)\n",
    "        model.fit(X, Y, n_epoch=num_epochs, validation_set=(testX, testY),show_metric=True,  batch_size=batch_size, \n",
    "                  shuffle=True, run_id='resnext')\n",
    "        predict(model, Xte, ref, mode)\n",
    "        \n",
    "    elif mode=='googlenet':\n",
    "        network = regression(googlenet(Input,classes),  optimizer='adam',loss='categorical_crossentropy')\n",
    "        model = tflearn.DNN(network, checkpoint_path='./tmp/', tensorboard_verbose=3)\n",
    "        model.fit(X, Y, n_epoch=num_epochs, validation_set=(testX, testY),show_metric=True,  batch_size=batch_size, \n",
    "                  shuffle=True, run_id='googlenet')\n",
    "        predict(model, Xte, ref, mode)\n",
    "        \n",
    "    elif mode=='vgg':\n",
    "        network = regression(vgg(Input,classes), optimizer='adam',loss='categorical_crossentropy')\n",
    "        model = tflearn.DNN(network, checkpoint_path='./tmp/',tensorboard_verbose=3)\n",
    "        model.fit(X, Y, n_epoch=num_epochs, validation_set=(testX, testY),show_metric=True,  batch_size=batch_size, \n",
    "                  shuffle=True, run_id='vgg')\n",
    "        predict(model, Xte, ref, mode)\n",
    "        \n",
    "    else:\n",
    "        print(\"Please Select Model in ['resnet32', 'inception_resnet_v2', 'resnext', 'googlenet','vgg']\")\n",
    "        \n",
    "    def train(num_epochs=10, mode ='resnet32'):\n",
    "    batch_size = 100\n",
    "    classes = 2\n",
    "\n",
    "    img_prep = tflearn.ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "    img_aug = tflearn.ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_90degrees_rotation(rotations=[0, 1, 2, 3])\n",
    "    img_aug.add_random_crop([75,75,2], padding=4)\n",
    "\n",
    "    Input = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "\n",
    "    Xtr, Ytr, Xte, ref = read_data()\n",
    "    Xtr, Ytr = tflearn.data_utils.shuffle(Xtr, Ytr)\n",
    "    Ytr = onehot_encoding(Ytr, classes)\n",
    "    X, Y = Xtr[:1200], Ytr[:1200]\n",
    "    testX, testY =Xtr[1200:1600], Ytr[1200:1600]\n",
    "    \n",
    "    \n",
    "    if mode =='resnet32':\n",
    "        network = tflearn.regression(resnet32(Input,classes),optimizer='adam',loss='categorical_crossentropy')\n",
    "        model = tflearn.DNN(network,tensorboard_verbose=3, checkpoint_path='./tmp/')\n",
    "        model.fit(X, Y, n_epoch=num_epochs, shuffle=True, validation_set=(testX, testY),\n",
    "                  show_metric=True, batch_size=batch_size, run_id='resnet32')\n",
    "        predict(model, Xte, ref, mode)\n",
    "        \n",
    "    elif mode =='inception_resnet_v2':        \n",
    "        network = tflearn.regression(inception_resnet_v2(Input,classes), optimizer='adam',loss='categorical_crossentropy')\n",
    "        model = tflearn.DNN(network,tensorboard_verbose=3, checkpoint_path='./tmp/')\n",
    "        model.fit(X, Y, n_epoch=num_epochs, shuffle=True, show_metric=True, validation_set=(testX, testY),\n",
    "                  batch_size=batch_size, run_id='inception_resnet_v2')\n",
    "        predict(model, Xte, ref, mode)\n",
    "        \n",
    "    elif mode=='resnext':\n",
    "        network = tflearn.regression(resnext(Input,classes), optimizer='adam',loss='categorical_crossentropy')\n",
    "        model = tflearn.DNN(network, checkpoint_path='./tmp/', tensorboard_verbose=3)\n",
    "        model.fit(X, Y, n_epoch=num_epochs, validation_set=(testX, testY),show_metric=True,  batch_size=batch_size, \n",
    "                  shuffle=True, run_id='resnext')\n",
    "        predict(model, Xte, ref, mode)\n",
    "        \n",
    "    elif mode=='googlenet':\n",
    "        network = regression(googlenet(Input,classes),  optimizer='adam',loss='categorical_crossentropy')\n",
    "        model = tflearn.DNN(network, checkpoint_path='./tmp/', tensorboard_verbose=3)\n",
    "        model.fit(X, Y, n_epoch=num_epochs, validation_set=(testX, testY),show_metric=True,  batch_size=batch_size, \n",
    "                  shuffle=True, run_id='googlenet')\n",
    "        predict(model, Xte, ref, mode)\n",
    "        \n",
    "    elif mode=='vgg':\n",
    "        network = regression(vgg(Input,classes), optimizer='adam',loss='categorical_crossentropy')\n",
    "        model = tflearn.DNN(network, checkpoint_path='./tmp/',tensorboard_verbose=3)\n",
    "        model.fit(X, Y, n_epoch=num_epochs, validation_set=(testX, testY),show_metric=True,  batch_size=batch_size, \n",
    "                  shuffle=True, run_id='vgg')\n",
    "        predict(model, Xte, ref, mode)\n",
    "        \n",
    "    else:\n",
    "        print(\"Please Select Model in ['resnet32', 'inception_resnet_v2', 'resnext', 'googlenet','vgg']\")\n",
    "    \n",
    "    model.save(mode)\n",
    "    \n",
    "def predict(model, test_data, ref, mode):\n",
    "    #8424=8*81*13\n",
    "    batch_size =104\n",
    "    iteration = int(len(test_data)/batch_size)  #iter=81  \n",
    "    with open(\"C:/Users/User/Desktop/kaggle/result{0}.txt\".format(mode), \"w\") as f:\n",
    "        f.write(\"id,is_iceberg\\n\")\n",
    "        for i in range(iteration):\n",
    "            pred = model.predict(test_data[i*batch_size: (i+1)*batch_size])\n",
    "            for j in range(batch_size):\n",
    "                data = \"%s,%f\\n\" % (ref[i*batch_size+j], pred[j][1])\n",
    "                f.write(data)\n",
    "\n",
    "def ensemble(models, num_epochs = 20):\n",
    "    \n",
    "    for k in models: \n",
    "        models[k] = tf.Graph()\n",
    "        with models[k].as_default():\n",
    "            tflearn.init_graph()\n",
    "            train(num_epochs, mode = k)\n",
    "        tf.reset_default_graph()\n",
    "    return model\n",
    "    \n",
    "        hypotheses[n_ensemble]=model.predict(testX)\n",
    "        hypotheses_valid[n_ensemble]=model.predict(validX)\n",
    "        n_ensemble+=1\n",
    "    \n",
    "    \n",
    "tflearn.config.is_training (is_training=False, session=None)\n",
    "# Retrieve variable responsible for managing training mode\n",
    "training_mode = tflearn.get_training_mode()\n",
    "# Define a conditional op\n",
    "my_conditional_op = tf.cond(training_mode, if_yes_op, if_no_op)\n",
    "# Set training mode to True\n",
    "tflearn.is_training(True)\n",
    "session.run(my_conditional_op)\n",
    "if_yes_op\n",
    "# Set training mode to False\n",
    "tflearn.is_training(False)\n",
    "session.run(my_conditional_op)\n",
    "if_no_op\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    models = {'resnet32': m1, 'vgg': m2, 'googlenet': m3, 'resnext': m4, 'inception_resnet_v2': m5}\n",
    "    \n",
    "    train(mode='resnet32', num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected,reshape\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d,highway_conv_2d\n",
    "from tflearn.layers.normalization import local_response_normalization,batch_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.layers.merge_ops import merge\n",
    "# Data loading and preprocessing\n",
    "import tflearn.datasets.mnist as mnist\n",
    "import tflearn.data_utils as du\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def convnet(imgaug,drop_out = 0.5):\n",
    "    network = input_data(shape=[None, 28, 28, 1], name='input', data_augmentation=imgaug)\n",
    "    network = conv_2d(network, 32, 5, activation='elu', regularizer=\"L2\")\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = batch_normalization(network) #try with batch norm\n",
    "    network = conv_2d(network, 64, 5, activation='elu', regularizer=\"L2\")\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = batch_normalization(network)\n",
    "    network = conv_2d(network, 128, 5, activation='elu', regularizer=\"L2\")\n",
    "    network = batch_normalization(network)\n",
    "    network = fully_connected(network, 512, activation='elu')\n",
    "    network = dropout(network, drop_out)\n",
    "    network = fully_connected(network, 2048, activation='elu')\n",
    "    network = dropout(network, drop_out)\n",
    "    network = fully_connected(network, 10, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    return network\n",
    "\n",
    "def rnn(imgaug=None,drop_out = 0.5):\n",
    "    rnn = input_data(shape=[None, 28, 28, 1], name='input', data_augmentation=imgaug)\n",
    "    rnn  = reshape(rnn, [-1,28,28])\n",
    "    rnn = tflearn.lstm(rnn, 256, return_seq=True)\n",
    "    rnn = tflearn.lstm(rnn, 512)\n",
    "    rnn = dropout(rnn,drop_out);\n",
    "    rnn = fully_connected(rnn, 10, activation='softmax')\n",
    "    rnn = regression(rnn, optimizer='adam', learning_rate=0.001,\n",
    "                             loss='categorical_crossentropy', name='target')\n",
    "    return rnn\n",
    "\n",
    "def highnet(imgaug = None,drop_out = 0.5):\n",
    "    highnet = input_data(shape=[None, 28, 28, 1], name='input', data_augmentation=imgaug)\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in [3, 2, 1]:\n",
    "            highnet = highway_conv_2d(highnet, 16, j, activation='elu')\n",
    "        highnet = max_pool_2d(highnet, 2)\n",
    "        highnet = batch_normalization(highnet)\n",
    "\n",
    "    highnet = fully_connected(highnet, 128, activation='elu')\n",
    "    highnet = dropout(highnet,drop_out);\n",
    "    highnet = fully_connected(highnet, 256, activation='elu')\n",
    "    highnet = dropout(highnet,drop_out);\n",
    "    highnet = fully_connected(highnet, 10, activation='softmax')\n",
    "    highnet = regression(highnet, optimizer='adam', learning_rate=0.01,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    return highnet\n",
    "\n",
    "def resnet(imgaug=None, drop_out=0.5):\n",
    "    # Building Residual Network\n",
    "    net = input_data(shape=[None, 28, 28, 1], name='input', data_augmentation=imgaug)\n",
    "    net = tflearn.conv_2d(net, 64, 3, activation='elu', bias=False)\n",
    "    # Residual blocks\n",
    "    net = tflearn.residual_bottleneck(net, 3, 16, 64)\n",
    "    net = tflearn.residual_bottleneck(net, 1, 32, 128, downsample=True)\n",
    "    net = tflearn.residual_bottleneck(net, 2, 32, 128)\n",
    "    net = tflearn.residual_bottleneck(net, 1, 64, 256, downsample=True)\n",
    "    net = tflearn.residual_bottleneck(net, 2, 64, 256)\n",
    "    net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.activation(net, 'elu')\n",
    "    net = tflearn.global_avg_pool(net)\n",
    "    # Regression\n",
    "    net = tflearn.fully_connected(net, 10, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam',\n",
    "                             loss='categorical_crossentropy',\n",
    "                             learning_rate=0.001, name='target')\n",
    "    return net\n",
    "\n",
    "#X, Y, testX, testY = mnist.load_data(one_hot=True)\n",
    "dataset = mnist.read_data_sets(one_hot=True)\n",
    "X = dataset.train.images\n",
    "Y = dataset.train.labels\n",
    "testX = dataset.test.images\n",
    "testY = dataset.test.labels\n",
    "validX=dataset.validation.images\n",
    "validY=dataset.validation.labels\n",
    "\n",
    "X = X.reshape([-1, 28, 28, 1])\n",
    "testX = testX.reshape([-1, 28, 28, 1])\n",
    "validX = validX.reshape([-1, 28, 28, 1])\n",
    "\n",
    "X, mean = du.featurewise_zero_center(X)\n",
    "testX = du.featurewise_zero_center(testX, mean)\n",
    "validX = du.featurewise_zero_center(validX, mean)\n",
    "\n",
    "ensemble=11   #Total no of ensemble models : 11\n",
    "n_ensemble=0   #n_semble counter\n",
    "n_sample=10000 #Number of validation set from training set\n",
    "validX = np.concatenate((validX,X[0:n_sample]),axis=0)\n",
    "validY = np.concatenate((validY,Y[0:n_sample]),axis=0)\n",
    "hypotheses = np.zeros((ensemble,testY.shape[0],10))\n",
    "hypotheses_valid = np.zeros((ensemble,validY.shape[0],10))\n",
    "\n",
    "# Example: pictures of 32x32\n",
    "imgaug = tflearn.ImageAugmentation()\n",
    "imgaug.add_random_crop((28, 28), 4)\n",
    "imgaug.add_random_rotation (max_angle=7.5)\n",
    "imgaug.add_random_blur(0.3)\n",
    "\n",
    "# Training graphs\n",
    "conv_m1 = tf.Graph()\n",
    "conv_m2 = tf.Graph()\n",
    "conv_m3 = tf.Graph()\n",
    "conv_m4 = tf.Graph()\n",
    "conv_m5 = tf.Graph()\n",
    "rnn_m1 = tf.Graph()\n",
    "rnn_m2 = tf.Graph()\n",
    "rnn_m3 = tf.Graph()\n",
    "high_m1 = tf.Graph()\n",
    "high_m2 = tf.Graph()\n",
    "high_m3 = tf.Graph()\n",
    "\n",
    "#res_m1 = tf.Graph()\n",
    "\n",
    "\n",
    "global_epoch=200\n",
    "\n",
    "#ensemble trial\n",
    "with conv_m1.as_default():\n",
    "    tf.set_random_seed(777) #no reproducibility...T.T\n",
    "    tflearn.init_graph(seed=777)\n",
    "    network1 = convnet(imgaug=None,drop_out=0.7)\n",
    "    model = tflearn.DNN(network1, tensorboard_verbose=0)\n",
    "    model.fit({'input': X}, {'target': Y}, n_epoch=int(global_epoch),\n",
    "               validation_set=({'input': validX[:5000]}, {'target': validY[:5000]}),\n",
    "               snapshot_step=55000, show_metric=True, run_id='convnet_mnist1', batch_size=512)\n",
    "    model.save('convnet1')\n",
    "    hypotheses[n_ensemble]=model.predict(testX)\n",
    "    hypotheses_valid[n_ensemble]=model.predict(validX)\n",
    "    n_ensemble+=1\n",
    "tf.reset_default_graph()\n",
    "with conv_m2.as_default():\n",
    "    tf.set_random_seed(777) \n",
    "    tflearn.init_graph(seed=777)\n",
    "    network2 = convnet(imgaug=imgaug,drop_out=0.6)\n",
    "    model2 = tflearn.DNN(network2, tensorboard_verbose=0)\n",
    "    model2.fit({'input': X}, {'target': Y}, n_epoch=int(global_epoch),\n",
    "               validation_set=({'input': validX[:5000]}, {'target': validY[:5000]}),\n",
    "               snapshot_step=55000, show_metric=True, run_id='convnet_mnist1', batch_size=512)\n",
    "    model2.save('convnet2')\n",
    "    hypotheses[n_ensemble]=model2.predict(testX)\n",
    "    hypotheses_valid[n_ensemble]=model2.predict(validX)\n",
    "    n_ensemble+=1\n",
    "tf.reset_default_graph()\n",
    "with conv_m3.as_default():\n",
    "    tf.set_random_seed(779)\n",
    "    tflearn.init_graph(seed=779)\n",
    "    network3 = convnet(imgaug,drop_out=0.6)\n",
    "    model3 = tflearn.DNN(network3, tensorboard_verbose=0)\n",
    "    model3.fit({'input': X}, {'target': Y}, n_epoch=int(global_epoch/1.5),\n",
    "               validation_set=({'input': validX[:5000]}, {'target': validY[:5000]}),\n",
    "               snapshot_step=55000, show_metric=True, run_id='convnet_mnist3', batch_size=512)\n",
    "    model3.save('convnet3')\n",
    "    hypotheses[n_ensemble]=model3.predict(testX)\n",
    "    hypotheses_valid[n_ensemble]=model3.predict(validX)\n",
    "    n_ensemble+=1\n",
    "tf.reset_default_graph()\n",
    "with conv_m4.as_default():\n",
    "    tf.set_random_seed(780)\n",
    "    tflearn.init_graph(seed=780)\n",
    "    network4 = convnet(imgaug,drop_out=0.5)\n",
    "    model4 = tflearn.DNN(network4, tensorboard_verbose=0)\n",
    "    model4.fit({'input': X}, {'target': Y}, n_epoch=int(global_epoch/2),\n",
    "               validation_set=({'input': validX[:5000]}, {'target': validY[:5000]}),\n",
    "               snapshot_step=55000, show_metric=True, run_id='convnet_mnist4', batch_size=512)\n",
    "    model4.save('convnet4')\n",
    "    hypotheses[n_ensemble]=model4.predict(testX)\n",
    "    hypotheses_valid[n_ensemble]=model4.predict(validX)\n",
    "    n_ensemble+=1\n",
    "tf.reset_default_graph()\n",
    "with conv_m5.as_default():\n",
    "    tf.set_random_seed(781)\n",
    "    tflearn.init_graph(seed=781)\n",
    "\n",
    "    network5 = convnet(imgaug,drop_out=0.4)\n",
    "    model5 = tflearn.DNN(network5, tensorboard_verbose=0)\n",
    "\n",
    "    model5.fit({'input': X}, {'target': Y}, n_epoch=int(global_epoch),\n",
    "               validation_set=({'input': validX[:5000]}, {'target': validY[:5000]}),\n",
    "               snapshot_step=55000, show_metric=True, run_id='convnet_mnist5', batch_size=512)\n",
    "    model5.save('convnet5')\n",
    "    hypotheses[n_ensemble]=model5.predict(testX)\n",
    "    hypotheses_valid[n_ensemble]=model5.predict(validX)\n",
    "    n_ensemble+=1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with rnn_m1.as_default():\n",
    "    tf.set_random_seed(782)\n",
    "    tflearn.init_graph(seed=782)\n",
    "\n",
    "    rnn_network1 = rnn(None,drop_out=0.7)\n",
    "    rnn1 = tflearn.DNN(rnn_network1, tensorboard_verbose=0)\n",
    "    rnn1.fit({'input': X}, {'target': Y}, n_epoch=int(global_epoch),\n",
    "               validation_set=({'input': validX[:5000]}, {'target': validY[:5000]}),\n",
    "               snapshot_step=55000, show_metric=True, run_id='rnn_mnist1',batch_size=512)\n",
    "    rnn1.save('rnn1')\n",
    "    hypotheses[n_ensemble]=rnn1.predict(testX)\n",
    "    hypotheses_valid[n_ensemble]=rnn1.predict(validX)\n",
    "    n_ensemble+=1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with rnn_m2.as_default():\n",
    "    tf.set_random_seed(783)\n",
    "    tflearn.init_graph(seed=783)\n",
    "\n",
    "    rnn_network2 = rnn(imgaug,drop_out=0.6)\n",
    "    rnn2 = tflearn.DNN(rnn_network2, tensorboard_verbose=0)\n",
    "    rnn2.fit({'input': X}, {'target': Y}, n_epoch=int(global_epoch/2),\n",
    "               validation_set=({'input': validX[:5000]}, {'target': validY[:5000]}),\n",
    "               snapshot_step=55000, show_metric=True, run_id='rnn_mnist2',batch_size=512)\n",
    "    rnn2.save('rnn2')\n",
    "    hypotheses[n_ensemble]=rnn2.predict(testX)\n",
    "    hypotheses_valid[n_ensemble]=rnn2.predict(validX)\n",
    "    n_ensemble+=1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with rnn_m3.as_default():\n",
    "    tf.set_random_seed(784)\n",
    "    tflearn.init_graph(seed=784)\n",
    "\n",
    "    rnn_network3 = rnn(imgaug,drop_out=0.5)\n",
    "    rnn3 = tflearn.DNN(rnn_network3, tensorboard_verbose=0)\n",
    "    rnn3.fit({'input': X}, {'target': Y}, n_epoch=int(global_epoch/1),\n",
    "               validation_set=({'input': validX[:5000]}, {'target': validY[:5000]}),\n",
    "               snapshot_step=55000, show_metric=True, run_id='rnn_mnist3',batch_size=512)\n",
    "    rnn3.save('rnn3')\n",
    "    hypotheses[n_ensemble]=rnn3.predict(testX)\n",
    "    hypotheses_valid[n_ensemble]=rnn3.predict(validX)\n",
    "    n_ensemble+=1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with high_m1.as_default():\n",
    "    tf.set_random_seed(785)\n",
    "    tflearn.init_graph(seed=785)\n",
    "\n",
    "    highnet_network1=highnet(None,drop_out=0.7)\n",
    "    highnet1 = tflearn.DNN(highnet_network1, tensorboard_verbose=0)\n",
    "    highnet1.fit({'input': X}, {'target': Y}, n_epoch=int(global_epoch),\n",
    "               validation_set=({'input': validX[:5000]}, {'target': validY[:5000]}),\n",
    "               snapshot_step=55000, show_metric=True, run_id='highway_mnist1',batch_size=512)\n",
    "    highnet1.save('highway1')\n",
    "    hypotheses[n_ensemble]=highnet1.predict(testX)\n",
    "    hypotheses_valid[n_ensemble]=highnet1.predict(validX)\n",
    "    n_ensemble+=1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with high_m2.as_default():\n",
    "    tf.set_random_seed(786)\n",
    "    tflearn.init_graph(seed=786)\n",
    "\n",
    "    highnet_network2=highnet(imgaug,drop_out=0.6)\n",
    "    highnet2 = tflearn.DNN(highnet_network2, tensorboard_verbose=0)\n",
    "    highnet2.fit({'input': X}, {'target': Y}, n_epoch=int(global_epoch),\n",
    "               validation_set=({'input': validX[:5000]}, {'target': validY[:5000]}),\n",
    "               snapshot_step=55000, show_metric=True, run_id='highway_mnist2',batch_size=512)\n",
    "    highnet2.save('highway2')\n",
    "    hypotheses[n_ensemble]=highnet2.predict(testX)\n",
    "    hypotheses_valid[n_ensemble]=highnet2.predict(validX)\n",
    "\n",
    "    n_ensemble+=1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with high_m3.as_default():\n",
    "    tf.set_random_seed(787)\n",
    "    tflearn.init_graph(seed=787)\n",
    "\n",
    "    highnet_network3=highnet(imgaug,drop_out=0.5)\n",
    "    highnet3 = tflearn.DNN(highnet_network3, tensorboard_verbose=0)\n",
    "    highnet3.fit({'input': X}, {'target': Y}, n_epoch=int(global_epoch),\n",
    "               validation_set=({'input': validX[:5000]}, {'target': validY[:5000]}),\n",
    "               snapshot_step=55000, show_metric=True, run_id='highway_mnist3',batch_size=512)\n",
    "    highnet3.save('highway3')\n",
    "    hypotheses[n_ensemble]=highnet3.predict(testX)\n",
    "    hypotheses_valid[n_ensemble]=highnet3.predict(validX)\n",
    "    n_ensemble+=1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#End of training\n",
    "\n",
    "print(\"Optimization finished\")\n",
    "\n",
    "hypotheses_valid = hypotheses_valid.transpose([1,0,2])\n",
    "hypotheses = hypotheses.transpose([1,0,2])\n",
    "hypothesis = np.zeros((testY.shape[0],10))\n",
    "\n",
    "#Ensemble weight training : using Validation set\n",
    "ens=tf.Graph()\n",
    "with ens.as_default():\n",
    "    ensemble_net = input_data(shape=[None,n_ensemble, 10], name='input')\n",
    "    ensemble_net = fully_connected(ensemble_net, 10, activation='softplus')\n",
    "    ensemble_net = regression(ensemble_net, optimizer='adam', learning_rate=0.01,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "\n",
    "    ens_model = tflearn.DNN(ensemble_net, tensorboard_verbose=0)\n",
    "    ens_model.fit({'input': hypotheses_valid}, {'target': validY}, n_epoch=20,\n",
    "                  snapshot_step=5000, show_metric=False, run_id='ensemble',batch_size=1000)\n",
    "\n",
    "    hypothesis = ens_model.predict(hypotheses)\n",
    "    ens_model.save('ens_model')\n",
    "\n",
    "\n",
    "correct_prediction = np.equal(np.argmax(hypothesis,1), np.argmax(testY, 1))\n",
    "accuracy_ens = np.mean(correct_prediction)\n",
    "\n",
    "print(\"Final Accuracy :\", accuracy_ens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
