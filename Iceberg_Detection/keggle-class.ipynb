{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_x = '/home/snu/Downloads/자료 정리/X_raw.txt'\n",
    "input_y = '/home/snu/Downloads/자료 정리/y_raw.txt'\n",
    "\n",
    "image_x = 75\n",
    "image_y = 75\n",
    "image_z = 2\n",
    "\n",
    "No_data = 0\n",
    "\n",
    "y_class = 2\n",
    "\n",
    "Jump = 0.00001\n",
    "\n",
    "Batch_X = 30\n",
    "Batch_Y = 101\n",
    "\n",
    "structure ='''\n",
    "\n",
    "#<<< Structure >>>\n",
    "#v3\n",
    "# Convolution\n",
    "conv(1,2,8)\n",
    "conv(2,8,32, relu=0)\n",
    "conv(3,32,32)\n",
    "conv(4,32,64,relu=0)\n",
    "conv(5,32,64,-2); max_p(2)\n",
    "conv(6,64,64,relu=0)\n",
    "conv(7,64,64,2)\n",
    "conv(8,64,64,relu=0)\n",
    "conv(9,64,64,2)\n",
    "conv(10,64,128,relu=0)\n",
    "conv(11,64,128,-2); max_p(2)\n",
    "conv(12,128,128,relu=0)\n",
    "conv(13,128,128,2)\n",
    "conv(14,128,128,relu=0)\n",
    "conv(15,128,128,2)\n",
    "conv(16,128,256,relu=0)\n",
    "conv(17,128,256,-2); max_p(2)\n",
    "conv(18,256,256,relu=0)\n",
    "conv(19,256,256,2)\n",
    "conv(20,256,256,relu=0)\n",
    "conv(21,256,256,2)\n",
    "conv(22,256,512,relu=0)\n",
    "conv(23,256,512,-2); max_p(2)\n",
    "conv(24,512,512,relu=0)\n",
    "conv(25,512,512,2)\n",
    "conv(26,512,512,relu=0)\n",
    "conv(27,512,512,2)\n",
    "conv(28,512,1024,relu=0)\n",
    "conv(29,512,1024,-2)\n",
    "\n",
    "# Fully connected \n",
    "full(30,1024,64)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "for idx in range(0, a.shape) \n",
    "\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in range(0, a.shape[0]+1, batch_size):\n",
    "    print(a[idx])\n",
    "    result = a[idx:idx+batch_size]\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import email_sending\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '['.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4b907e196b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-f709d3978d9a>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '['."
     ]
    }
   ],
   "source": [
    "unpickle(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the CIFAR-10\n",
    "def load_CIFAR10(pos, n_chunks=1):\n",
    "    Xtr = []\n",
    "    Ytr = []\n",
    "    for i in range(n_chunks):\n",
    "        train = unpickle(pos + '/data_batch_{0}'.format(i + 1))\n",
    "        Xtr.extend(train[b'data'])\n",
    "        Ytr.extend(train[b'labels'])\n",
    "        test = unpickle(pos + '/test_batch')\n",
    "        Xte = test[b'data'] \n",
    "        Yte = test[b'labels']\n",
    "    return np.array(Xtr), np.array(Ytr), np.array(Xte), np.array(Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expresses the label data in one-hot encoding.\n",
    "def onehot_encoding (Ytr, Yte):\n",
    "    Ytr_onehot = np.zeros((Ytr.size, int(y_class)))\n",
    "    Yte_onehot = np.zeros((Yte.size, int(y_class)))\n",
    "    for i in range(Ytr.size):\n",
    "        Ytr_onehot[i][Ytr[i]] = 1\n",
    "    for i in range(Yte.size):\n",
    "        Yte_onehot[i][Yte[i]] = 1\n",
    "    return Ytr_onehot, Yte_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the train and test data\n",
    "a=open(input_x,'r')\n",
    "Xt = []\n",
    "while True:\n",
    "    temp = a.readline()\n",
    "    if temp=='':break\n",
    "    else : exec(\"Xt.append(%s)\" %temp)\n",
    "a.close()\n",
    "\n",
    "a=open(input_y,'r')\n",
    "Yt = []\n",
    "exec(\"Yt=\" +a.readline())\n",
    "a.close()\n",
    "\n",
    "Xt = np.array(Xt) - No_data\n",
    "Yt = np.array(Yt) \n",
    "test_idxs = np.random.permutation(len(Xt))\n",
    "Xt = Xt[test_idxs]\n",
    "Yt = Yt[test_idxs]\n",
    "\n",
    "#Train/Valid Sep\n",
    "TVS = 1200\n",
    "Xtr, Ytr, Xte, Yte = Xt[:TVS],Yt[:TVS],Xt[TVS:],Yt[TVS:]\n",
    "\n",
    "\n",
    "# label data of train and test data, label data is represented by one-hot encoding\n",
    "Ytr_onehot, Yte_onehot = onehot_encoding(Ytr, Yte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L0 = tf.placeholder(tf.float32, [None, int(image_x), int(image_y), int(image_z)])\n",
    "Y = tf.placeholder(tf.float32, [None, int(y_class)])\n",
    "dropout_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_layer = 0\n",
    "image_size = int(image_x)\n",
    "last_cha = int(image_z)\n",
    "def max_p(n):\n",
    "    exec(\"global L%s \\nL%s = tf.nn.max_pool(L%s, ksize=[1,n,n,1],strides=[1,n,n,1,],padding='SAME')\" %(last_layer,last_layer,last_layer))\n",
    "    global image_size\n",
    "    image_size = round(image_size/n)\n",
    "    \n",
    "def conv(n, cha_from, cha_to, res=0, relu=1, p=0): \n",
    "    # n:layer번호 / cha_from: 이전 채널 수 / cha_to: 새 채널 수 / res: 양수-only add / relu: relu / p: pool\n",
    "    #exec(\"global L%s\" %n)\n",
    "    global last_cha, last_layer\n",
    "    last_cha =(cha_to)\n",
    "    last_layer = (n)\n",
    "    exec(\"global W%s \\nW%s = tf.Variable(tf.random_normal([3,3,cha_from,cha_to],stddev=0.01))\" %(n,n))\n",
    "    if res == 0:\n",
    "        exec(\"global L%s \\nL%s = tf.nn.conv2d(L%s,W%s,strides=[1,1,1,1],padding='SAME')\" %(n,n,n-1,n))\n",
    "        if relu == 1:\n",
    "            exec(\"global L%s \\nL%s = tf.nn.relu(L%s)\" %(n,n,n))\n",
    "            exec(\"global L%s \\nL%s = tf.contrib.layers.batch_norm(L%s)\" %(n,n,n))\n",
    "\n",
    "    else:\n",
    "        if res > 0:\n",
    "            exec(\"global L%s \\nL%s = tf.add(L%s,L%s)\"   %(n,n,n-res,n-1))\n",
    "            if relu == 1:\n",
    "                exec(\"global L%s \\nL%s = tf.nn.relu(L%s)\" %(n,n,n))\n",
    "                exec(\"global L%s \\nL%s = tf.contrib.layers.batch_norm(L%s)\" %(n,n,n))\n",
    "        else:   \n",
    "            exec(\"global L%s \\nL%s = tf.nn.conv2d(L%s,W%s,strides=[1,1,1,1],padding='SAME')\" %(n,n,n+res,n))\n",
    "            exec(\"global L%s \\ntf.add(L%s,L%s)\"   %(n,n,n-1))\n",
    "            if relu == 1:\n",
    "                exec(\"global L%s \\nL%s = tf.nn.relu(L%s)\" %(n,n,n))\n",
    "                exec(\"global L%s \\nL%s = tf.contrib.layers.batch_norm(L%s)\" %(n,n,n))\n",
    "                \n",
    "def full(n,che_from,che_to):\n",
    "    global last_cha, last_layer,image_size\n",
    "    last_cha =(che_to)\n",
    "    last_layer = (n)\n",
    "    \n",
    "    exec(\"global W%s \\nW%s = tf.Variable(tf.random_normal([%s,%s],stddev=0.01))\" %(n,n,int(image_size*image_size*che_from),int(che_to)))\n",
    "    exec(\"global L%s \\nL%s = tf.reshape(L%s,[-1,%s])\" %(n,n,n-1,(int(image_size*image_size*che_from))))\n",
    "    exec(\"global B%s \\nB%s =  tf.Variable(tf.random_normal([%s],stddev=0.01))\" %(n,n,int(che_to)))\n",
    "    exec(\"global L%s \\nL%s =tf.matmul(L%s,W%s) + B%s \" %(n,n,n,n,n))\n",
    "    exec(\"L%s = tf.nn.relu(L%s)\" %(n,n))\n",
    "    image_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Implement the layers of CNNs ###\n",
    "\n",
    "\n",
    "exec(structure)\n",
    "exec(\"W%s = tf.Variable(tf.random_normal([%s,%s],stddev=0.01))\" %(last_layer+1,int(last_cha),y_class))\n",
    "exec(\"model = tf.matmul(L%s,W%s)\" %(last_layer,last_layer+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost function, you can change the implementation\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(Jump).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph():\n",
    "    \n",
    "    img = Image.open('/home/snu/Downloads/blank.png')\n",
    "    draw_img = ImageDraw.Draw(img)\n",
    "\n",
    "    x = 0\n",
    "    acc_min = min(acc_list) ; acc_max = max(acc_list)\n",
    "    cost_min = min(cost_list); cost_max = max(cost_list)\n",
    "    \n",
    "    draw_img.text(xy=(100,70),text=str('ACCURACY'),fill=(0,0,255,255))\n",
    "    draw_img.text(xy=(100,370),text=str('COST'),fill=(0,0,255,255))\n",
    "    for i in range(len(epoch_list)):\n",
    "        x = x + 30 \n",
    "        y = 300 - (float((acc_list[i]-acc_min)/(acc_max-acc_min))*200 )\n",
    "        draw_img.line((x,300,x,y), width=10, fill=(0,0,255,255))\n",
    "        draw_img.text(xy=(x-6,310),text=str(epoch_list[i]),fill=(255,0,0,255))\n",
    "        draw_img.text(xy=(x-10,320),text=str(round(acc_list[i],2)),fill=(0,0,255,255))\n",
    "        \n",
    "        y = 600 - (float((cost_list[i]-cost_min)/(cost_max-cost_min))*200 )\n",
    "        draw_img.line((x,600,x,y), width=10, fill=(0,0,255,255))\n",
    "        draw_img.text(xy=(x-6,610),text=str(epoch_list[i]),fill=(255,0,0,255))\n",
    "        draw_img.text(xy=(x-10,620),text=str(round(cost_list[i],2)),fill=(0,0,255,255))\n",
    "\n",
    "    img.save('/home/snu/Downloads/blank2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "epoch = 0\n",
    "acc_list= []\n",
    "epoch_list = []\n",
    "cost_list = []\n",
    "def accu(sub_info):\n",
    "    global accuracy\n",
    "    test_batch_size = int(Batch_Y)\n",
    "    test_datasize,_,_,_ = Xte.shape\n",
    "    test_total_batch = int(test_datasize / test_batch_size)\n",
    "\n",
    "    test_idxs = np.random.permutation(test_datasize)\n",
    "    Xte_random = Xte[test_idxs]\n",
    "    Yte_random = Yte_onehot[test_idxs]\n",
    "\n",
    "    correctness = tf.equal(tf.argmax(model, 1), tf.argmax(Y,1))\n",
    "    partial_accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32))\n",
    "\n",
    "    accuracy = 0\n",
    "    #print('Partial Accuracies')\n",
    "    for i in range(test_total_batch):\n",
    "        t_batch_X = Xte_random[i * test_batch_size:(i+1) * test_batch_size].reshape(-1,int(image_x),int(image_y),int(image_z))\n",
    "        t_batch_Y = Yte_random[i * test_batch_size:(i+1) * test_batch_size]\n",
    "\n",
    "        p_acc = sess.run(partial_accuracy,\n",
    "            feed_dict={\n",
    "            L0: t_batch_X.reshape(-1, int(image_x), int(image_y), int(image_z)),\n",
    "            Y: t_batch_Y,\n",
    "            dropout_prob: 1})\n",
    "\n",
    "        #print(p_acc)\n",
    "        accuracy += p_acc\n",
    "\n",
    "    accuracy = accuracy / test_total_batch\n",
    "    print('*** Accuracy : {:,.4f} ***'.format(accuracy))\n",
    "    \n",
    "    epoch_list.append(epoch); acc_list.append(accuracy); cost_list.append(avg_cost)\n",
    "    if len(epoch_list) > 20:\n",
    "        epoch_list.pop(0); acc_list.pop(0) ; cost_list.pop(0)\n",
    "    try:graph()\n",
    "    except:pass\n",
    "    \n",
    "    \n",
    "    global best_acc\n",
    "    if accuracy >= 0.75 and accuracy >= best_acc:\n",
    "        best_acc =  accuracy\n",
    "        email_sending.email2me('New Record! '+str(best_acc), sub_info+'\\n'+structure, '/home/snu/Downloads/blank2.png')\n",
    "        email_sending.email2u('New Record! '+str(best_acc), sub_info+'\\n'+structure, '/home/snu/Downloads/blank2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Implement the train process ###\n",
    "batch_size= int(Batch_X)\n",
    "total_batch = int(len(Xtr)/batch_size)\n",
    "\n",
    "off_switch = 0\n",
    "while True:\n",
    "    \n",
    "    epoch += 1\n",
    "    total_cost = 0 \n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys = Xtr[i*batch_size: (i+1)*batch_size],Ytr_onehot[i*batch_size: (i+1)*batch_size]\n",
    "        batch_xs = batch_xs.reshape(-1,int(image_x),int(image_y),int(image_z))\n",
    "            \n",
    "        _,curr_loss = sess.run([optimizer,cost],   \n",
    "                              feed_dict ={L0:batch_xs,Y:batch_ys,dropout_prob:0.7})\n",
    "        total_cost += curr_loss\n",
    "    \n",
    "    avg_cost = total_cost/total_batch\n",
    "    print('EPOCH : ' , '%04d' % (epoch),\n",
    "         'Avg.cost = ', '{:,.3f}'.format(avg_cost))\n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    accu('EPOCH: '+str(epoch)+' Avg.Cost: '+str(avg_cost))\n",
    "    if best_acc > accuracy:\n",
    "        off_switch += 1\n",
    "        if off_switch == 19: \n",
    "            email_sending.email2me(input_x +'Learning Finished with '+str(best_acc),'Last EPOCH: '+str(epoch)+' Avg.Cost: '+str(total_cost/total_batch)+'\\n'+structure,'/home/snu/Downloads/blank2.png') \n",
    "            email_sending.email2u(input_x +'Learning Finished with '+str(best_acc),'Last EPOCH: '+str(epoch)+' Avg.Cost: '+str(total_cost/total_batch)+'\\n'+structure,'/home/snu/Downloads/blank2.png')\n",
    "            break\n",
    "    elif best_acc <= accuracy: off_switch = 0 \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "\n",
    "models = []\n",
    "num_models = 10\n",
    "for m in range(num_models):\n",
    "    models.append(Model(sess, \"model\" + str(m)))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Learning Started!')\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost_list = np.zeros(len(models))\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # train each model\n",
    "        for m_idx, m in enumerate(models):\n",
    "            c, _ = m.train(batch_xs, batch_ys)\n",
    "            avg_cost_list[m_idx] += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "test_size = len(mnist.test.labels)\n",
    "predictions = np.zeros([test_size, 10])\n",
    "for m_idx, m in enumerate(models):\n",
    "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
    "        mnist.test.images, mnist.test.labels))\n",
    "    p = m.predict(mnist.test.images)\n",
    "    predictions += p\n",
    "\n",
    "ensemble_correct_prediction = tf.equal(\n",
    "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
    "ensemble_accuracy = tf.reduce_mean(\n",
    "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
    "print('Ensemble accuracy:', sess.run(ensemble_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
