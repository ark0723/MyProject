{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, random, copy, platform, os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import email_sending\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img():\n",
    "    with open('GAN', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        band1 = data[0]\n",
    "        band2 = data[1]\n",
    "        label = data[2]\n",
    "\n",
    "    return band1, band2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data load\n",
    "band1 = img()[0]\n",
    "band2 = img()[1]\n",
    "y = img()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img1 = band1.reshape(-1, 75, 75)\n",
    "img2 = band2.reshape(-1, 75, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = band1.reshape(-1, 5625)\n",
    "X2 = band2.reshape(-1, 5625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_epoch = 10000\n",
    "batch_size = 401 #1604 = 401*4\n",
    "learning_rate = 0.00002\n",
    "n_noise = 1024 # the size of noise \n",
    "n_input = 75*75\n",
    "n_hidden =2024\n",
    "n_class = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expresses the label data in one-hot encoding.\n",
    "def onehot_encoding (Y):\n",
    "    Y_onehot = np.zeros((Y.size, int(n_class)))\n",
    "    for i in range(Y.size):\n",
    "        Y_onehot[i][Y[i]] = 1\n",
    "    return Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = onehot_encoding(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-9-be9e3ca24cd2>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-be9e3ca24cd2>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    def model_inputs(self, image_width, image_height, image_channels, z_dim):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "# 노이즈와 실제 이미지에, 그에 해당하는 숫자에 대한 정보를 넣어주기 위해 사용합니다.\n",
    "Y = tf.placeholder(tf.float32, [None, n_class])\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])\n",
    "\n",
    "\n",
    "    def model_inputs(self, image_width, image_height, image_channels, z_dim):\n",
    "        \"\"\"\n",
    "        Create the model inputs/tensors\n",
    "        \"\"\"\n",
    "        inputs_real = tf.placeholder(tf.float32, (None, image_width, image_height, image_channels), name='input_real')\n",
    "        inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "        learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "        k_t = tf.placeholder(tf.float32, name='k_t')\n",
    "\n",
    "        return inputs_real, inputs_z, learning_rate, k_t\n",
    "\n",
    "\n",
    "\n",
    "def fully_connected(x, output_shape):\n",
    "    # flatten and dense\n",
    "    shape = x.get_shape().as_list()\n",
    "    dim = np.prod(shape[1:])\n",
    "\n",
    "    x = tf.reshape(x, [-1, dim])\n",
    "    x = tf.layers.dense(x, output_shape, activation=None)\n",
    "\n",
    "    return x\n",
    "\n",
    "def decoder(h, n, img_dim, channel_dim):\n",
    "    \"\"\"\n",
    "    Reconstruction network\n",
    "    \"\"\"\n",
    "    h = tf.layers.dense(h, img_dim * img_dim * n, activation=None)\n",
    "    h = tf.reshape(h, (-1, img_dim, img_dim, n))\n",
    "\n",
    "    conv1 = tf.layers.conv2d(h, n, 3, padding=\"same\", activation=tf.nn.relu)\n",
    "    conv1 = tf.layers.conv2d(conv1, n, 3, padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    upsample1 = tf.image.resize_nearest_neighbor(conv1, size=(img_dim * 2, img_dim * 2))\n",
    "\n",
    "    conv2 = tf.layers.conv2d(upsample1, n, 3, padding=\"same\", activation=tf.nn.relu)\n",
    "    conv2 = tf.layers.conv2d(conv2, n, 3, padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    upsample2 = tf.image.resize_nearest_neighbor(conv2, size=(img_dim * 4, img_dim * 4))\n",
    "\n",
    "    conv3 = tf.layers.conv2d(upsample2, n, 3, padding=\"same\", activation=tf.nn.relu)\n",
    "    conv3 = tf.layers.conv2d(conv3, n, 3, padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    conv4 = tf.layers.conv2d(conv3, channel_dim, 3, padding=\"same\", activation=None)\n",
    "\n",
    "    return conv4\n",
    "\n",
    "def encoder(self, images, n, z_dim, channel_dim):\n",
    "    \"\"\"\n",
    "    Feature extraction network \n",
    "    \"\"\"\n",
    "    conv1 = tf.layers.conv2d(images, n, 3, padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    conv2 = tf.layers.conv2d(conv1, n, 3, padding=\"same\", activation=tf.nn.relu)\n",
    "    conv2 = tf.layers.conv2d(conv2, n * 2, 3, padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    subsample1 = tf.layers.conv2d(conv2, n * 2, 3, strides=2, padding='same')\n",
    "\n",
    "    conv3 = tf.layers.conv2d(subsample1, n * 2, 3,padding=\"same\", activation=tf.nn.relu)\n",
    "    conv3 = tf.layers.conv2d(conv3, n * 3, 3, padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    subsample2 = tf.layers.conv2d(conv3, n * 3, 3, strides=2, padding='same')\n",
    "\n",
    "    conv4 = tf.layers.conv2d(subsample2, n * 3, 3,padding=\"same\", activation=tf.nn.relu)\n",
    "    conv4 = tf.layers.conv2d(conv4, n * 3, 3, padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    h = fully_connected(conv4, z_dim)\n",
    "\n",
    "    return h\n",
    "\n",
    "def generator(self, z, channel_dim, is_train=True):\n",
    "    \"\"\"\n",
    "    Create the generator network: Only the encoder part\n",
    "    \"\"\"\n",
    "    reuse = False if is_train else True\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        x = self.decoder(z, 512, 75 // 4, channel_dim) #def decoder(h, n, img_dim, channel_dim):\n",
    "\n",
    "        return x\n",
    "    \n",
    "def discriminator(images, z_dim, channel_dim, reuse=False):\n",
    "    \"\"\"\n",
    "    Create the discriminator network: The autoencoder\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        x = encoder(images, 512, z_dim, channel_dim)\n",
    "        x = decoder(x, 512, 75 // 4, channel_dim)\n",
    "\n",
    "        return x\n",
    "'''\n",
    "def generator(noise, labels):\n",
    "    with tf.variable_scope('generator'):\n",
    "        # noise 값에 labels 정보를 추가합니다.\n",
    "        inputs = tf.concat([noise, labels], 1)\n",
    "\n",
    "        # TensorFlow 에서 제공하는 유틸리티 함수를 이용해 신경망을 매우 간단하게 구성할 수 있습니다.\n",
    "        hidden = tf.layers.dense(inputs, n_hidden,\n",
    "                                 activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, n_input,\n",
    "                                 activation=tf.nn.sigmoid)\n",
    "\n",
    "    return output\n",
    "\n",
    "def discriminator(inputs, labels, reuse=None):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        # 노이즈에서 생성한 이미지와 실제 이미지를 판별하는 모델의 변수를 동일하게 하기 위해,\n",
    "        # 이전에 사용되었던 변수를 재사용하도록 합니다.\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        inputs = tf.concat([inputs, labels], 1)\n",
    "\n",
    "        hidden = tf.layers.dense(inputs, n_hidden,\n",
    "                                 activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, 1,\n",
    "                                 activation=None)\n",
    "\n",
    "    return output\n",
    "\n",
    "'''\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.uniform(-1., 1., size=[batch_size, n_noise])\n",
    "\n",
    "# 생성 모델과 판별 모델에 Y 즉, labels 정보를 추가하여\n",
    "# labels 정보에 해당하는 이미지를 생성할 수 있도록 유도합니다.\n",
    "G = generator(Z, Y)\n",
    "D_real = discriminator(X, Y)\n",
    "D_gene = discriminator(G, Y, True)\n",
    "\n",
    "# 손실함수는 다음을 참고하여 GAN 논문에 나온 방식과는 약간 다르게 작성하였습니다.\n",
    "# http://bamos.github.io/2016/08/09/deep-completion/\n",
    "# 진짜 이미지를 판별하는 D_real 값은 1에 가깝도록,\n",
    "# 가짜 이미지를 판별하는 D_gene 값은 0에 가깝도록 하는 손실 함수입니다.\n",
    "loss_D_real = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                        logits=D_real, labels=tf.ones_like(D_real)))\n",
    "loss_D_gene = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                        logits=D_gene, labels=tf.zeros_like(D_gene)))\n",
    "# loss_D_real 과 loss_D_gene 을 더한 뒤 이 값을 최소화 하도록 최적화합니다.\n",
    "loss_D = loss_D_real + loss_D_gene\n",
    "# 가짜 이미지를 진짜에 가깝게 만들도록 생성망을 학습시키기 위해, D_gene 을 최대한 1에 가깝도록 만드는 손실함수입니다.\n",
    "loss_G = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                        logits=D_gene, labels=tf.ones_like(D_gene)))\n",
    "\n",
    "# TensorFlow 에서 제공하는 유틸리티 함수를 이용해\n",
    "# discriminator 와 generator scope 에서 사용된 변수들을 쉽게 가져올 수 있습니다.\n",
    "vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                           scope='discriminator')\n",
    "vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                           scope='generator')\n",
    "\n",
    "train_D = tf.train.AdamOptimizer().minimize(loss_D,\n",
    "                                            var_list=vars_D)\n",
    "train_G = tf.train.AdamOptimizer().minimize(loss_G,\n",
    "                                            var_list=vars_G)\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(1604/batch_size)\n",
    "loss_val_D, loss_val_G = 0, 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = X1[i*batch_size:(i+1)*batch_size], labels[i*batch_size:(i+1)*batch_size] #band1\n",
    "        #batch_xs, batch_ys = X2[i*batch_size:(i+1)*batch_size], labels[i*batch_size:(i+1)*batch_size] #band2\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "\n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                                 feed_dict={X: batch_xs, Y: batch_ys, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G],\n",
    "                                 feed_dict={Y: batch_ys, Z: noise})\n",
    "\n",
    "    print('Epoch:', '%04d' % epoch,\n",
    "          'D loss: {:.4}'.format(loss_val_D),\n",
    "          'G loss: {:.4}'.format(loss_val_G))\n",
    "\n",
    "    #########\n",
    "    # 학습이 되어가는 모습을 보기 위해 주기적으로 레이블에 따른 이미지를 생성하여 저장\n",
    "    ######\n",
    "    if epoch == 0 or (epoch + 1) % 5 == 0:\n",
    "        sample_size = 5\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G,\n",
    "                           feed_dict={Y: labels[:sample_size],\n",
    "                                      Z: noise})\n",
    "\n",
    "        fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n",
    "\n",
    "        for i in range(sample_size):\n",
    "            ax[0][i].set_axis_off()\n",
    "            ax[1][i].set_axis_off()\n",
    "\n",
    "            ax[0][i].imshow(np.reshape(X1[i], (75, 75)), cmap=plt.cm.bone, interpolation='nearest')\n",
    "            ax[1][i].imshow(np.reshape(samples[i], (75, 75)), cmap=plt.cm.bone, interpolation='nearest')\n",
    "\n",
    "        plt.savefig('samples/{}.png'.format(str(epoch).zfill(4)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "print('최적화 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def decoder(self, h, n, img_dim, channel_dim):\n",
    "        \"\"\"\n",
    "        Reconstruction network\n",
    "        \"\"\"\n",
    "        h = tf.layers.dense(h, img_dim * img_dim * n, activation=None)\n",
    "        h = tf.reshape(h, (-1, img_dim, img_dim, n))\n",
    "\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            h, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            conv1, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        upsample1 = tf.image.resize_nearest_neighbor(\n",
    "            conv1, size=(img_dim * 2, img_dim * 2))\n",
    "\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            upsample1, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            conv2, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        upsample2 = tf.image.resize_nearest_neighbor(\n",
    "            conv2, size=(img_dim * 4, img_dim * 4))\n",
    "\n",
    "        conv3 = tf.layers.conv2d(\n",
    "            upsample2, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "        conv3 = tf.layers.conv2d(\n",
    "            conv3, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        conv4 = tf.layers.conv2d(conv3, channel_dim, 3,\n",
    "                                 padding=\"same\", activation=None)\n",
    "\n",
    "        return conv4\n",
    "\n",
    "    def encoder(self, images, n, z_dim, channel_dim):\n",
    "        \"\"\"\n",
    "        Feature extraction network \n",
    "        \"\"\"\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            images, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            conv1, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            conv2, n * 2, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        subsample1 = tf.layers.conv2d(\n",
    "            conv2, n * 2, 3, strides=2, padding='same')\n",
    "\n",
    "        conv3 = tf.layers.conv2d(subsample1, n * 2, 3,\n",
    "                                 padding=\"same\", activation=self.leaky_relu)\n",
    "        conv3 = tf.layers.conv2d(\n",
    "            conv3, n * 3, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        subsample2 = tf.layers.conv2d(\n",
    "            conv3, n * 3, 3, strides=2, padding='same')\n",
    "\n",
    "        conv4 = tf.layers.conv2d(subsample2, n * 3, 3,\n",
    "                                 padding=\"same\", activation=self.leaky_relu)\n",
    "        conv4 = tf.layers.conv2d(\n",
    "            conv4, n * 3, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        h = self.fully_connected(conv4, z_dim)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BEGAN(object):\n",
    "    def __init__(self, place_holder=''):\n",
    "        self.place_holder = place_holder\n",
    "\n",
    "    def model_inputs(self, image_width, image_height, image_channels, z_dim):\n",
    "        \"\"\"\n",
    "        Create the model inputs/tensors\n",
    "        \"\"\"\n",
    "        inputs_real = tf.placeholder(tf.float32, (None, image_width, image_height, image_channels), name='input_real')\n",
    "        inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "        learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "        k_t = tf.placeholder(tf.float32, name='k_t')\n",
    "\n",
    "        return inputs_real, inputs_z, learning_rate, k_t\n",
    "\n",
    "    # default aplha is 0.2, 0.01 works best for this example\n",
    "    # Function from TensorFlow v1.4 for backwards compatability\n",
    "    def leaky_relu(self, features, alpha=0.01, name=None):\n",
    "        with ops.name_scope(name, \"LeakyRelu\", [features, alpha]):\n",
    "            features = ops.convert_to_tensor(features, name=\"features\")\n",
    "            alpha = ops.convert_to_tensor(alpha, name=\"alpha\")\n",
    "\n",
    "            return math_ops.maximum(alpha * features, features)\n",
    "\n",
    "    def fully_connected(self, x, output_shape):\n",
    "        # flatten and dense\n",
    "        shape = x.get_shape().as_list()\n",
    "        dim = np.prod(shape[1:])\n",
    "\n",
    "        x = tf.reshape(x, [-1, dim])\n",
    "        x = tf.layers.dense(x, output_shape, activation=None)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decoder(self, h, n, img_dim, channel_dim):\n",
    "        \"\"\"\n",
    "        Reconstruction network\n",
    "        \"\"\"\n",
    "        h = tf.layers.dense(h, img_dim * img_dim * n, activation=None)\n",
    "        h = tf.reshape(h, (-1, img_dim, img_dim, n))\n",
    "\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            h, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            conv1, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        upsample1 = tf.image.resize_nearest_neighbor(\n",
    "            conv1, size=(img_dim * 2, img_dim * 2))\n",
    "\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            upsample1, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            conv2, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        upsample2 = tf.image.resize_nearest_neighbor(\n",
    "            conv2, size=(img_dim * 4, img_dim * 4))\n",
    "\n",
    "        conv3 = tf.layers.conv2d(\n",
    "            upsample2, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "        conv3 = tf.layers.conv2d(\n",
    "            conv3, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        conv4 = tf.layers.conv2d(conv3, channel_dim, 3,\n",
    "                                 padding=\"same\", activation=None)\n",
    "\n",
    "        return conv4\n",
    "\n",
    "    def encoder(self, images, n, z_dim, channel_dim):\n",
    "        \"\"\"\n",
    "        Feature extraction network \n",
    "        \"\"\"\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            images, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            conv1, n, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            conv2, n * 2, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        subsample1 = tf.layers.conv2d(\n",
    "            conv2, n * 2, 3, strides=2, padding='same')\n",
    "\n",
    "        conv3 = tf.layers.conv2d(subsample1, n * 2, 3,\n",
    "                                 padding=\"same\", activation=self.leaky_relu)\n",
    "        conv3 = tf.layers.conv2d(\n",
    "            conv3, n * 3, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        subsample2 = tf.layers.conv2d(\n",
    "            conv3, n * 3, 3, strides=2, padding='same')\n",
    "\n",
    "        conv4 = tf.layers.conv2d(subsample2, n * 3, 3,\n",
    "                                 padding=\"same\", activation=self.leaky_relu)\n",
    "        conv4 = tf.layers.conv2d(\n",
    "            conv4, n * 3, 3, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "        h = self.fully_connected(conv4, z_dim)\n",
    "\n",
    "        return h\n",
    "\n",
    "    def discriminator(self, images, z_dim, channel_dim, reuse=False):\n",
    "        \"\"\"\n",
    "        Create the discriminator network: The autoencoder\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('discriminator', reuse=reuse):\n",
    "            x = self.encoder(images, 64, z_dim, channel_dim)\n",
    "            x = self.decoder(x, 64, 64 // 4, channel_dim)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def generator(self, z, channel_dim, is_train=True):\n",
    "        \"\"\"\n",
    "        Create the generator network: Only the encoder part\n",
    "        \"\"\"\n",
    "        reuse = False if is_train else True\n",
    "        with tf.variable_scope('generator', reuse=reuse):\n",
    "            x = self.decoder(z, 64, 64 // 4, channel_dim)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def model_loss(self, input_real, input_z, channel_dim, z_dim, k_t):\n",
    "        \"\"\"\n",
    "        Get the loss for the discriminator and generator\n",
    "        \"\"\"\n",
    "        g_model_fake = self.generator(input_z, channel_dim, is_train=True)\n",
    "        d_model_real = self.discriminator(input_real, z_dim, channel_dim)\n",
    "        d_model_fake = self.discriminator(\n",
    "            g_model_fake, z_dim, channel_dim, reuse=True)\n",
    "\n",
    "        # l1 loss\n",
    "        d_real = tf.reduce_mean(tf.abs(input_real - d_model_real))\n",
    "        d_fake = tf.reduce_mean(tf.abs(g_model_fake - d_model_fake))\n",
    "\n",
    "        d_loss = d_real - k_t * d_fake\n",
    "        g_loss = d_fake\n",
    "\n",
    "        return d_loss, g_loss, d_real, d_fake\n",
    "\n",
    "    def model_opt(self, d_loss, g_loss, learning_rate, beta1, beta2=0.999):\n",
    "        \"\"\"\n",
    "        Get optimization operations\n",
    "        \"\"\"\n",
    "        # Get variables\n",
    "        g_vars = tf.get_collection(\n",
    "            tf.GraphKeys.GLOBAL_VARIABLES, \"generator\")\n",
    "        d_vars = tf.get_collection(\n",
    "            tf.GraphKeys.GLOBAL_VARIABLES, \"discriminator\")\n",
    "\n",
    "        # Optimize\n",
    "        d_train_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate, beta1=beta1, beta2=beta2).minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate, beta1=beta1, beta2=beta2).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "        return d_train_opt, g_train_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GAN 모델을 이용해 단순히 랜덤한 숫자를 생성하는 아닌,\n",
    "# 원하는 손글씨 숫자를 생성하는 모델을 만들어봅니다.\n",
    "# 이런 방식으로 흑백 사진을 컬러로 만든다든가, 또는 선화를 채색한다든가 하는 응용이 가능합니다.\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "#########\n",
    "# 옵션 설정\n",
    "######\n",
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128\n",
    "n_class = 10\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "# 노이즈와 실제 이미지에, 그에 해당하는 숫자에 대한 정보를 넣어주기 위해 사용합니다.\n",
    "Y = tf.placeholder(tf.float32, [None, n_class])\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])\n",
    "\n",
    "\n",
    "def generator(noise, labels):\n",
    "    with tf.variable_scope('generator'):\n",
    "        # noise 값에 labels 정보를 추가합니다.\n",
    "        inputs = tf.concat([noise, labels], 1)\n",
    "\n",
    "        # TensorFlow 에서 제공하는 유틸리티 함수를 이용해 신경망을 매우 간단하게 구성할 수 있습니다.\n",
    "        hidden = tf.layers.dense(inputs, n_hidden,\n",
    "                                 activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, n_input,\n",
    "                                 activation=tf.nn.sigmoid)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def discriminator(inputs, labels, reuse=None):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        # 노이즈에서 생성한 이미지와 실제 이미지를 판별하는 모델의 변수를 동일하게 하기 위해,\n",
    "        # 이전에 사용되었던 변수를 재사용하도록 합니다.\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        inputs = tf.concat([inputs, labels], 1)\n",
    "\n",
    "        hidden = tf.layers.dense(inputs, n_hidden,\n",
    "                                 activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, 1,\n",
    "                                 activation=None)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.uniform(-1., 1., size=[batch_size, n_noise])\n",
    "\n",
    "# 생성 모델과 판별 모델에 Y 즉, labels 정보를 추가하여\n",
    "# labels 정보에 해당하는 이미지를 생성할 수 있도록 유도합니다.\n",
    "G = generator(Z, Y)\n",
    "D_real = discriminator(X, Y)\n",
    "D_gene = discriminator(G, Y, True)\n",
    "\n",
    "# 손실함수는 다음을 참고하여 GAN 논문에 나온 방식과는 약간 다르게 작성하였습니다.\n",
    "# http://bamos.github.io/2016/08/09/deep-completion/\n",
    "# 진짜 이미지를 판별하는 D_real 값은 1에 가깝도록,\n",
    "# 가짜 이미지를 판별하는 D_gene 값은 0에 가깝도록 하는 손실 함수입니다.\n",
    "loss_D_real = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                        logits=D_real, labels=tf.ones_like(D_real)))\n",
    "loss_D_gene = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                        logits=D_gene, labels=tf.zeros_like(D_gene)))\n",
    "# loss_D_real 과 loss_D_gene 을 더한 뒤 이 값을 최소화 하도록 최적화합니다.\n",
    "loss_D = loss_D_real + loss_D_gene\n",
    "# 가짜 이미지를 진짜에 가깝게 만들도록 생성망을 학습시키기 위해, D_gene 을 최대한 1에 가깝도록 만드는 손실함수입니다.\n",
    "loss_G = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                        logits=D_gene, labels=tf.ones_like(D_gene)))\n",
    "\n",
    "# TensorFlow 에서 제공하는 유틸리티 함수를 이용해\n",
    "# discriminator 와 generator scope 에서 사용된 변수들을 쉽게 가져올 수 있습니다.\n",
    "vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                           scope='discriminator')\n",
    "vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                           scope='generator')\n",
    "\n",
    "train_D = tf.train.AdamOptimizer().minimize(loss_D,\n",
    "                                            var_list=vars_D)\n",
    "train_G = tf.train.AdamOptimizer().minimize(loss_G,\n",
    "                                            var_list=vars_G)\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "loss_val_D, loss_val_G = 0, 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "\n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                                 feed_dict={X: batch_xs, Y: batch_ys, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G],\n",
    "                                 feed_dict={Y: batch_ys, Z: noise})\n",
    "\n",
    "    print('Epoch:', '%04d' % epoch,\n",
    "          'D loss: {:.4}'.format(loss_val_D),\n",
    "          'G loss: {:.4}'.format(loss_val_G))\n",
    "\n",
    "    #########\n",
    "    # 학습이 되어가는 모습을 보기 위해 주기적으로 레이블에 따른 이미지를 생성하여 저장\n",
    "    ######\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G,\n",
    "                           feed_dict={Y: mnist.test.labels[:sample_size],\n",
    "                                      Z: noise})\n",
    "\n",
    "        fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n",
    "\n",
    "        for i in range(sample_size):\n",
    "            ax[0][i].set_axis_off()\n",
    "            ax[1][i].set_axis_off()\n",
    "\n",
    "            ax[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
    "            ax[1][i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "\n",
    "        plt.savefig('samples2/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "print('최적화 완료!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
