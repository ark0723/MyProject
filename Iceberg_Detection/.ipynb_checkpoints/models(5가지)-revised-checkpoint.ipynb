{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import pickle, random, copy, platform, os, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import email_sending\n",
    "import tflearn\n",
    "import tflearn.activations as activations\n",
    "from PIL import Image, ImageDraw\n",
    "from __future__ import division, print_function, absolute_import\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.activations import relu\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.conv import avg_pool_2d, conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import dropout, flatten, fully_connected, input_data\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "from tflearn.utils import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    with open('resnet_data', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        Xtr = data[0]\n",
    "        Xtr = np.reshape(Xtr, (-1,75,75,2))\n",
    "        Ytr = data[1]\n",
    "        Xte =data[2]\n",
    "        Xte = np.reshape(Xte, (-1,75,75,2))\n",
    "        rf =data[3]\n",
    "    return Xtr, Ytr, Xte, rf\n",
    "\n",
    "# expresses the label data in one-hot encoding.\n",
    "def onehot_encoding (Y, y_class):\n",
    "    Y_onehot = np.zeros((Y.size, int(y_class)))\n",
    "    for i in range(Y.size):\n",
    "        Y_onehot[i][Y[i]] = 1\n",
    "    return Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet32(x, classes, n = 5): #classes =2\n",
    "    net = tflearn.conv_2d(x, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "    net = tflearn.residual_block(net, n, 16)\n",
    "    net = tflearn.residual_block(net, 1, 32, downsample=True)\n",
    "    net = tflearn.residual_block(net, n - 1, 32)\n",
    "    net = tflearn.residual_block(net, 1, 64, downsample=True)\n",
    "    net = tflearn.residual_block(net, n - 1, 64)\n",
    "    net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.activation(net, 'relu')\n",
    "    net = tflearn.global_avg_pool(net)\n",
    "    net = tflearn.fully_connected(net, classes, activation='softmax')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnext(x, classes, n =5):\n",
    "    net = tflearn.conv_2d(x, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "    net = tflearn.resnext_block(net, n, 16, 32)\n",
    "    net = tflearn.resnext_block(net, 1, 32, 32, downsample=True)\n",
    "    net = tflearn.resnext_block(net, n-1, 32, 32)\n",
    "    net = tflearn.resnext_block(net, 1, 64, 32, downsample=True)\n",
    "    net = tflearn.resnext_block(net, n-1, 64, 32)\n",
    "    net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.activation(net, 'relu')\n",
    "    net = tflearn.global_avg_pool(net)\n",
    "    net = tflearn.fully_connected(net, classes, activation='softmax')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg(x, classes):\n",
    "    \n",
    "    network = conv_2d(x, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, classes, activation='softmax')\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def block35(net, scale=1.0, activation=\"relu\"):\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None,name='Conv2d_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 32, 3, bias=False, activation=None,name='Conv2d_0b_3x3')))\n",
    "    tower_conv2_0 = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2_0, 48,3, bias=False, activation=None, name='Conv2d_0b_3x3')))\n",
    "    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 64,3, bias=False, activation=None, name='Conv2d_0c_3x3')))\n",
    "    tower_mixed = merge([tower_conv, tower_conv1_1, tower_conv2_2], mode='concat', axis=3)\n",
    "    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    net += scale * tower_out\n",
    "    if activation:\n",
    "        if isinstance(activation, str):\n",
    "            net = activations.get(activation)(net)\n",
    "        elif hasattr(activation, '__call__'):\n",
    "            net = activation(net)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation.\")\n",
    "    return net\n",
    "\n",
    "def block17(net, scale=1.0, activation=\"relu\"):\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    tower_conv_1_0 = relu(batch_normalization(conv_2d(net, 128, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv_1_1 = relu(batch_normalization(conv_2d(tower_conv_1_0, 160,[1,7], bias=False, activation=None,name='Conv2d_0b_1x7')))\n",
    "    tower_conv_1_2 = relu(batch_normalization(conv_2d(tower_conv_1_1, 192, [7,1], bias=False, activation=None,name='Conv2d_0c_7x1')))\n",
    "    tower_mixed = merge([tower_conv,tower_conv_1_2], mode='concat', axis=3)\n",
    "    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    net += scale * tower_out\n",
    "    if activation:\n",
    "        if isinstance(activation, str):\n",
    "            net = activations.get(activation)(net)\n",
    "        elif hasattr(activation, '__call__'):\n",
    "            net = activation(net)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation.\")\n",
    "    return net\n",
    "\n",
    "\n",
    "def block8(net, scale=1.0, activation=\"relu\"):\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 224, [1,3], bias=False, activation=None, name='Conv2d_0b_1x3')))\n",
    "    tower_conv1_2 = relu(batch_normalization(conv_2d(tower_conv1_1, 256, [3,1], bias=False, name='Conv2d_0c_3x1')))\n",
    "    tower_mixed = merge([tower_conv,tower_conv1_2], mode='concat', axis=3)\n",
    "    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    net += scale * tower_out\n",
    "    if activation:\n",
    "        if isinstance(activation, str):\n",
    "            net = activations.get(activation)(net)\n",
    "        elif hasattr(activation, '__call__'):\n",
    "            net = activation(net)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation.\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inception_resnet_v2(x, classes):\n",
    "\n",
    "    dropout_keep_prob = 0.8\n",
    "\n",
    "    conv1a_3_3 = relu(batch_normalization(conv_2d(x, 32, 3, strides=2, bias=False, padding='VALID',activation=None,name='Conv2d_1a_3x3')))\n",
    "    conv2a_3_3 = relu(batch_normalization(conv_2d(conv1a_3_3, 32, 3, bias=False, padding='VALID',activation=None, name='Conv2d_2a_3x3')))\n",
    "    conv2b_3_3 = relu(batch_normalization(conv_2d(conv2a_3_3, 64, 3, bias=False, activation=None, name='Conv2d_2b_3x3')))\n",
    "    maxpool3a_3_3 = max_pool_2d(conv2b_3_3, 3, strides=2, padding='VALID', name='MaxPool_3a_3x3')\n",
    "    conv3b_1_1 = relu(batch_normalization(conv_2d(maxpool3a_3_3, 80, 1, bias=False, padding='VALID',activation=None, name='Conv2d_3b_1x1')))\n",
    "    conv4a_3_3 = relu(batch_normalization(conv_2d(conv3b_1_1, 192, 3, bias=False, padding='VALID',activation=None, name='Conv2d_4a_3x3')))\n",
    "    maxpool5a_3_3 = max_pool_2d(conv4a_3_3, 3, strides=2, padding='VALID', name='MaxPool_5a_3x3')\n",
    "\n",
    "    tower_conv = relu(batch_normalization(conv_2d(maxpool5a_3_3, 96, 1, bias=False, activation=None, name='Conv2d_5b_b0_1x1')))\n",
    "\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(maxpool5a_3_3, 48, 1, bias=False, activation=None, name='Conv2d_5b_b1_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 64, 5, bias=False, activation=None, name='Conv2d_5b_b1_0b_5x5')))\n",
    "\n",
    "    tower_conv2_0 = relu(batch_normalization(conv_2d(maxpool5a_3_3, 64, 1, bias=False, activation=None, name='Conv2d_5b_b2_0a_1x1')))\n",
    "    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2_0, 96, 3, bias=False, activation=None, name='Conv2d_5b_b2_0b_3x3')))\n",
    "    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 96, 3, bias=False, activation=None,name='Conv2d_5b_b2_0c_3x3')))\n",
    "\n",
    "    tower_pool3_0 = avg_pool_2d(maxpool5a_3_3, 3, strides=1, padding='same', name='AvgPool_5b_b3_0a_3x3')\n",
    "    tower_conv3_1 = relu(batch_normalization(conv_2d(tower_pool3_0, 64, 1, bias=False, activation=None,name='Conv2d_5b_b3_0b_1x1')))\n",
    "\n",
    "    tower_5b_out = merge([tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_1], mode='concat', axis=3)\n",
    "\n",
    "    net = repeat(tower_5b_out, 10, block35, scale=0.17)\n",
    "\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 384, 3, bias=False, strides=2,activation=None, padding='VALID', name='Conv2d_6a_b0_0a_3x3')))\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, activation=None, name='Conv2d_6a_b1_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 256, 3, bias=False, activation=None, name='Conv2d_6a_b1_0b_3x3')))\n",
    "    tower_conv1_2 = relu(batch_normalization(conv_2d(tower_conv1_1, 384, 3, bias=False, strides=2, padding='VALID', activation=None,name='Conv2d_6a_b1_0c_3x3')))\n",
    "    tower_pool = max_pool_2d(net, 3, strides=2, padding='VALID',name='MaxPool_1a_3x3')\n",
    "    net = merge([tower_conv, tower_conv1_2, tower_pool], mode='concat', axis=3)\n",
    "    net = repeat(net, 20, block17, scale=0.1)\n",
    "\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv0_1 = relu(batch_normalization(conv_2d(tower_conv, 384, 3, bias=False, strides=2, padding='VALID', activation=None,name='Conv2d_0a_1x1')))\n",
    "\n",
    "    tower_conv1 = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, padding='VALID', activation=None,name='Conv2d_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1,288,3, bias=False, strides=2, padding='VALID',activation=None, name='COnv2d_1a_3x3')))\n",
    "\n",
    "    tower_conv2 = relu(batch_normalization(conv_2d(net, 256,1, bias=False, activation=None,name='Conv2d_0a_1x1')))\n",
    "    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2, 288,3, bias=False, name='Conv2d_0b_3x3',activation=None)))\n",
    "    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 320, 3, bias=False, strides=2, padding='VALID',activation=None, name='Conv2d_1a_3x3')))\n",
    "\n",
    "    tower_pool = max_pool_2d(net, 3, strides=2, padding='VALID', name='MaxPool_1a_3x3')\n",
    "    net = merge([tower_conv0_1, tower_conv1_1,tower_conv2_2, tower_pool], mode='concat', axis=3)\n",
    "\n",
    "    net = repeat(net, 9, block8, scale=0.2)\n",
    "    net = block8(net, activation=None)\n",
    "\n",
    "    net = relu(batch_normalization(conv_2d(net, 1536, 1, bias=False, activation=None, name='Conv2d_7b_1x1')))\n",
    "    net = avg_pool_2d(net, net.get_shape().as_list()[1:3],strides=2, padding='VALID', name='AvgPool_1a_8x8')\n",
    "    net = flatten(net)\n",
    "    net = dropout(net, dropout_keep_prob)\n",
    "    loss = fully_connected(net, classes,activation='softmax')\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def googlenet(x, classes):\n",
    "    conv1_7_7 = conv_2d(x, 64, 7, strides=2, activation='relu', name='conv1_7_7_s2')\n",
    "    pool1_3_3 = max_pool_2d(conv1_7_7, 3, strides=2)\n",
    "    pool1_3_3 = local_response_normalization(pool1_3_3)\n",
    "    conv2_3_3_reduce = conv_2d(pool1_3_3, 64, 1, activation='relu', name='conv2_3_3_reduce')\n",
    "    conv2_3_3 = conv_2d(conv2_3_3_reduce, 192, 3, activation='relu', name='conv2_3_3')\n",
    "    conv2_3_3 = local_response_normalization(conv2_3_3)\n",
    "    pool2_3_3 = max_pool_2d(conv2_3_3, kernel_size=3, strides=2, name='pool2_3_3_s2')\n",
    "\n",
    "    # 3a\n",
    "    inception_3a_1_1 = conv_2d(pool2_3_3, 64, 1, activation='relu', name='inception_3a_1_1')\n",
    "    inception_3a_3_3_reduce = conv_2d(pool2_3_3, 96, 1, activation='relu', name='inception_3a_3_3_reduce')\n",
    "    inception_3a_3_3 = conv_2d(inception_3a_3_3_reduce, 128, filter_size=3,  activation='relu', name='inception_3a_3_3')\n",
    "    inception_3a_5_5_reduce = conv_2d(pool2_3_3, 16, filter_size=1, activation='relu', name='inception_3a_5_5_reduce')\n",
    "    inception_3a_5_5 = conv_2d(inception_3a_5_5_reduce, 32, filter_size=5, activation='relu', name='inception_3a_5_5')\n",
    "    inception_3a_pool = max_pool_2d(pool2_3_3, kernel_size=3, strides=1, name='inception_3a_pool')\n",
    "    inception_3a_pool_1_1 = conv_2d(inception_3a_pool, 32, filter_size=1, activation='relu', name='inception_3a_pool_1_1')\n",
    "    inception_3a_output = merge([inception_3a_1_1, inception_3a_3_3, inception_3a_5_5, inception_3a_pool_1_1], mode='concat', axis=3)\n",
    "\n",
    "    # 3b\n",
    "    inception_3b_1_1 = conv_2d(inception_3a_output, 128, filter_size=1, activation='relu', name='inception_3b_1_1')\n",
    "    inception_3b_3_3_reduce = conv_2d(inception_3a_output, 128, filter_size=1, activation='relu', name='inception_3b_3_3_reduce')\n",
    "    inception_3b_3_3 = conv_2d(inception_3b_3_3_reduce, 192, filter_size=3, activation='relu', name='inception_3b_3_3')\n",
    "    inception_3b_5_5_reduce = conv_2d(inception_3a_output, 32, filter_size=1, activation='relu', name='inception_3b_5_5_reduce')\n",
    "    inception_3b_5_5 = conv_2d(inception_3b_5_5_reduce, 96, filter_size=5,  name='inception_3b_5_5')\n",
    "    inception_3b_pool = max_pool_2d(inception_3a_output, kernel_size=3, strides=1,  name='inception_3b_pool')\n",
    "    inception_3b_pool_1_1 = conv_2d(inception_3b_pool, 64, filter_size=1, activation='relu', name='inception_3b_pool_1_1')\n",
    "    inception_3b_output = merge([inception_3b_1_1, inception_3b_3_3, inception_3b_5_5, inception_3b_pool_1_1], mode='concat', axis=3, name='inception_3b_output')\n",
    "    pool3_3_3 = max_pool_2d(inception_3b_output, kernel_size=3, strides=2, name='pool3_3_3')\n",
    "\n",
    "    # 4a\n",
    "    inception_4a_1_1 = conv_2d(pool3_3_3, 192, filter_size=1, activation='relu', name='inception_4a_1_1')\n",
    "    inception_4a_3_3_reduce = conv_2d(pool3_3_3, 96, filter_size=1, activation='relu', name='inception_4a_3_3_reduce')\n",
    "    inception_4a_3_3 = conv_2d(inception_4a_3_3_reduce, 208, filter_size=3,  activation='relu', name='inception_4a_3_3')\n",
    "    inception_4a_5_5_reduce = conv_2d(pool3_3_3, 16, filter_size=1, activation='relu', name='inception_4a_5_5_reduce')\n",
    "    inception_4a_5_5 = conv_2d(inception_4a_5_5_reduce, 48, filter_size=5,  activation='relu', name='inception_4a_5_5')\n",
    "    inception_4a_pool = max_pool_2d(pool3_3_3, kernel_size=3, strides=1,  name='inception_4a_pool')\n",
    "    inception_4a_pool_1_1 = conv_2d(inception_4a_pool, 64, filter_size=1, activation='relu', name='inception_4a_pool_1_1')\n",
    "    inception_4a_output = merge([inception_4a_1_1, inception_4a_3_3, inception_4a_5_5, inception_4a_pool_1_1], mode='concat', axis=3, name='inception_4a_output')\n",
    "\n",
    "    # 4b\n",
    "    inception_4b_1_1 = conv_2d(inception_4a_output, 160, filter_size=1, activation='relu', name='inception_4a_1_1')\n",
    "    inception_4b_3_3_reduce = conv_2d(inception_4a_output, 112, filter_size=1, activation='relu', name='inception_4b_3_3_reduce')\n",
    "    inception_4b_3_3 = conv_2d(inception_4b_3_3_reduce, 224, filter_size=3, activation='relu', name='inception_4b_3_3')\n",
    "    inception_4b_5_5_reduce = conv_2d(inception_4a_output, 24, filter_size=1, activation='relu', name='inception_4b_5_5_reduce')\n",
    "    inception_4b_5_5 = conv_2d(inception_4b_5_5_reduce, 64, filter_size=5,  activation='relu', name='inception_4b_5_5')\n",
    "    inception_4b_pool = max_pool_2d(inception_4a_output, kernel_size=3, strides=1,  name='inception_4b_pool')\n",
    "    inception_4b_pool_1_1 = conv_2d(inception_4b_pool, 64, filter_size=1, activation='relu', name='inception_4b_pool_1_1')\n",
    "    inception_4b_output = merge([inception_4b_1_1, inception_4b_3_3, inception_4b_5_5, inception_4b_pool_1_1], mode='concat', axis=3, name='inception_4b_output')\n",
    "\n",
    "    # 4c\n",
    "    inception_4c_1_1 = conv_2d(inception_4b_output, 128, filter_size=1, activation='relu', name='inception_4c_1_1')\n",
    "    inception_4c_3_3_reduce = conv_2d(inception_4b_output, 128, filter_size=1, activation='relu', name='inception_4c_3_3_reduce')\n",
    "    inception_4c_3_3 = conv_2d(inception_4c_3_3_reduce, 256,  filter_size=3, activation='relu', name='inception_4c_3_3')\n",
    "    inception_4c_5_5_reduce = conv_2d(inception_4b_output, 24, filter_size=1, activation='relu', name='inception_4c_5_5_reduce')\n",
    "    inception_4c_5_5 = conv_2d(inception_4c_5_5_reduce, 64,  filter_size=5, activation='relu', name='inception_4c_5_5')\n",
    "    inception_4c_pool = max_pool_2d(inception_4b_output, kernel_size=3, strides=1)\n",
    "    inception_4c_pool_1_1 = conv_2d(inception_4c_pool, 64, filter_size=1, activation='relu', name='inception_4c_pool_1_1')\n",
    "    inception_4c_output = merge([inception_4c_1_1, inception_4c_3_3, inception_4c_5_5, inception_4c_pool_1_1], mode='concat', axis=3, name='inception_4c_output')\n",
    "\n",
    "    # 4d\n",
    "    inception_4d_1_1 = conv_2d(inception_4c_output, 112, filter_size=1, activation='relu', name='inception_4d_1_1')\n",
    "    inception_4d_3_3_reduce = conv_2d(inception_4c_output, 144, filter_size=1, activation='relu', name='inception_4d_3_3_reduce')\n",
    "    inception_4d_3_3 = conv_2d(inception_4d_3_3_reduce, 288, filter_size=3, activation='relu', name='inception_4d_3_3')\n",
    "    inception_4d_5_5_reduce = conv_2d(inception_4c_output, 32, filter_size=1, activation='relu', name='inception_4d_5_5_reduce')\n",
    "    inception_4d_5_5 = conv_2d(inception_4d_5_5_reduce, 64, filter_size=5,  activation='relu', name='inception_4d_5_5')\n",
    "    inception_4d_pool = max_pool_2d(inception_4c_output, kernel_size=3, strides=1,  name='inception_4d_pool')\n",
    "    inception_4d_pool_1_1 = conv_2d(inception_4d_pool, 64, filter_size=1, activation='relu', name='inception_4d_pool_1_1')\n",
    "    inception_4d_output = merge([inception_4d_1_1, inception_4d_3_3, inception_4d_5_5, inception_4d_pool_1_1], mode='concat', axis=3, name='inception_4d_output')\n",
    "\n",
    "    # 4e\n",
    "    inception_4e_1_1 = conv_2d(inception_4d_output, 256, filter_size=1, activation='relu', name='inception_4e_1_1')\n",
    "    inception_4e_3_3_reduce = conv_2d(inception_4d_output, 160, filter_size=1, activation='relu', name='inception_4e_3_3_reduce')\n",
    "    inception_4e_3_3 = conv_2d(inception_4e_3_3_reduce, 320, filter_size=3, activation='relu', name='inception_4e_3_3')\n",
    "    inception_4e_5_5_reduce = conv_2d(inception_4d_output, 32, filter_size=1, activation='relu', name='inception_4e_5_5_reduce')\n",
    "    inception_4e_5_5 = conv_2d(inception_4e_5_5_reduce, 128,  filter_size=5, activation='relu', name='inception_4e_5_5')\n",
    "    inception_4e_pool = max_pool_2d(inception_4d_output, kernel_size=3, strides=1,  name='inception_4e_pool')\n",
    "    inception_4e_pool_1_1 = conv_2d(inception_4e_pool, 128, filter_size=1, activation='relu', name='inception_4e_pool_1_1')\n",
    "    inception_4e_output = merge([inception_4e_1_1, inception_4e_3_3, inception_4e_5_5, inception_4e_pool_1_1], axis=3, mode='concat')\n",
    "    pool4_3_3 = max_pool_2d(inception_4e_output, kernel_size=3, strides=2, name='pool_3_3')\n",
    "\n",
    "    # 5a\n",
    "    inception_5a_1_1 = conv_2d(pool4_3_3, 256, filter_size=1, activation='relu', name='inception_5a_1_1')\n",
    "    inception_5a_3_3_reduce = conv_2d(pool4_3_3, 160, filter_size=1, activation='relu', name='inception_5a_3_3_reduce')\n",
    "    inception_5a_3_3 = conv_2d(inception_5a_3_3_reduce, 320, filter_size=3, activation='relu', name='inception_5a_3_3')\n",
    "    inception_5a_5_5_reduce = conv_2d(pool4_3_3, 32, filter_size=1, activation='relu', name='inception_5a_5_5_reduce')\n",
    "    inception_5a_5_5 = conv_2d(inception_5a_5_5_reduce, 128, filter_size=5,  activation='relu', name='inception_5a_5_5')\n",
    "    inception_5a_pool = max_pool_2d(pool4_3_3, kernel_size=3, strides=1,  name='inception_5a_pool')\n",
    "    inception_5a_pool_1_1 = conv_2d(inception_5a_pool, 128, filter_size=1, activation='relu', name='inception_5a_pool_1_1')\n",
    "    inception_5a_output = merge([inception_5a_1_1, inception_5a_3_3, inception_5a_5_5, inception_5a_pool_1_1], axis=3, mode='concat')\n",
    "\n",
    "    # 5b\n",
    "    inception_5b_1_1 = conv_2d(inception_5a_output, 384, filter_size=1, activation='relu', name='inception_5b_1_1')\n",
    "    inception_5b_3_3_reduce = conv_2d(inception_5a_output, 192, filter_size=1, activation='relu', name='inception_5b_3_3_reduce')\n",
    "    inception_5b_3_3 = conv_2d(inception_5b_3_3_reduce, 384,  filter_size=3, activation='relu', name='inception_5b_3_3')\n",
    "    inception_5b_5_5_reduce = conv_2d(inception_5a_output, 48, filter_size=1, activation='relu', name='inception_5b_5_5_reduce')\n",
    "    inception_5b_5_5 = conv_2d(inception_5b_5_5_reduce, 128, filter_size=5, activation='relu', name='inception_5b_5_5')\n",
    "    inception_5b_pool = max_pool_2d(inception_5a_output, kernel_size=3, strides=1,  name='inception_5b_pool')\n",
    "    inception_5b_pool_1_1 = conv_2d(inception_5b_pool, 128, filter_size=1, activation='relu', name='inception_5b_pool_1_1')\n",
    "    inception_5b_output = merge([inception_5b_1_1, inception_5b_3_3, inception_5b_5_5, inception_5b_pool_1_1], axis=3, mode='concat')\n",
    "    pool5_7_7 = avg_pool_2d(inception_5b_output, kernel_size=7, strides=1)\n",
    "    pool5_7_7 = dropout(pool5_7_7, 0.4)\n",
    "\n",
    "    # fc\n",
    "    net = fully_connected(pool5_7_7, classes, activation='softmax')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, test_data, ref, mode):\n",
    "    #8424=8*81*13\n",
    "    batch_size =104\n",
    "    iteration = int(len(test_data)/batch_size)  #iter=81  \n",
    "    # Load a model\n",
    "    #model.load(get_save_path(mode))\n",
    "    with open(\"C:/Users/User/Desktop/kaggle/result_{0}.txt\".format(mode), \"w\") as f:\n",
    "        f.write(\"id,is_iceberg\\n\")\n",
    "        for i in range(iteration):\n",
    "            pred = model.predict(test_data[i*batch_size: (i+1)*batch_size])\n",
    "            for j in range(batch_size):\n",
    "                data = \"%s,%f\\n\" % (ref[i*batch_size+j], pred[j][1])\n",
    "                f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dir = 'checkpoints/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "def get_save_path(net_name):\n",
    "    return save_dir + str(net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(tflearn.callbacks.Callback):\n",
    "    def __init__(self, val_acc_thresh):\n",
    "        \"\"\" Note: We are free to define our init function however we please. \"\"\"\n",
    "        self.val_acc_thresh = val_acc_thresh\n",
    "    \n",
    "    def on_epoch_end(self, training_state):\n",
    "        \"\"\" \"\"\"\n",
    "        # Apparently this can happen.\n",
    "        if training_state.val_acc is None: return\n",
    "        if training_state.val_acc > self.val_acc_thresh:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "classes = 2\n",
    "\n",
    "#load data\n",
    "Xtr, Ytr, Xte, ref = read_data()\n",
    "Xtr, Ytr = tflearn.data_utils.shuffle(Xtr, Ytr)\n",
    "Ytr = onehot_encoding(Ytr, classes)\n",
    "X, Y = Xtr[:1200], Ytr[:1200]\n",
    "validX, validY =Xtr[1200:1500], Ytr[1200:1500]\n",
    "testX, testY = Xtr[1500:],Ytr[1500:]\n",
    "hypotheses = np.zeros([1, testY.shape[0], 2])\n",
    "hypotheses_valid =np.zeros([1, validY.shape[0], 2])\n",
    "\n",
    "#data preprocessing\n",
    "img_prep = tflearn.ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center()\n",
    "img_prep.add_featurewise_stdnorm()\n",
    "#img_prep.add_custom_preprocessing (func) <-함수 적용가능\n",
    "#img_prep.add_custom_preprocessing (func)\n",
    "#img_prep.add_zca_whitening (pc=None)\n",
    "\n",
    "img_aug = tflearn.ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "img_aug.add_random_90degrees_rotation(rotations=[0, 1, 2, 3])\n",
    "img_aug.add_random_crop([75,75,2], padding=4)\n",
    "\n",
    "# Initializae our callback.\n",
    "#early_stopping_cb = EarlyStoppingCallback(val_acc_thresh=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(mode, num_epochs = 20):\n",
    "    #{'input': X},{'target': Y} ({'input': validX}, {'target': validY})  \n",
    "    #training graph\n",
    "    m1 =tf.Graph()\n",
    "    m2 =tf.Graph()\n",
    "    m3 =tf.Graph()\n",
    "    m4 =tf.Graph()\n",
    "    m5 =tf.Graph()\n",
    "    \n",
    "    if mode =='resnet32':\n",
    "        with m1.as_default():\n",
    "            early_stopping_cb = EarlyStoppingCallback(val_acc_thresh=0.95)\n",
    "            Input = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input', \n",
    "                                       data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "            network = tflearn.regression(resnet32(Input,classes),optimizer='adam',loss='categorical_crossentropy', name ='target')\n",
    "            model = tflearn.DNN(network,tensorboard_verbose=3,tensorboard_dir='/tmp/tflearn_logs/')\n",
    "            model.fit({'input': X}, {'target': Y}, n_epoch=num_epochs, validation_set=({'input': validX}, {'target': validY}),\n",
    "                      show_metric=True, callbacks=early_stopping_cb,batch_size=100, run_id='resnet32')\n",
    "            predict(model, Xte, ref, mode)\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    elif mode =='inception_resnet_v2':\n",
    "        with m2.as_default():\n",
    "            early_stopping_cb = EarlyStoppingCallback(val_acc_thresh=0.95)\n",
    "            Input1 = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input', \n",
    "                                        data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "            network = tflearn.regression(inception_resnet_v2(Input1,classes),\n",
    "                                         optimizer='adam',loss='categorical_crossentropy',name ='target')\n",
    "            model = tflearn.DNN(network,tensorboard_verbose=3,tensorboard_dir='/tmp/tflearn_logs/')\n",
    "            model.fit({'input': X}, {'target': Y}, n_epoch=num_epochs, show_metric=True, batch_size=batch_size,\n",
    "                      validation_set=({'input': validX}, {'target': validY}),callbacks=early_stopping_cb, run_id ='inception_resnet_v2')\n",
    "            predict(model, Xte, ref, mode)\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    elif mode=='resnext':\n",
    "        with m3.as_default():\n",
    "            early_stopping_cb = EarlyStoppingCallback(val_acc_thresh=0.95)\n",
    "            Input2 = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input',\n",
    "                                        data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "            network = tflearn.regression(resnext(Input2,classes), optimizer='adam',loss='categorical_crossentropy', name ='target')\n",
    "            model = tflearn.DNN(network, tensorboard_verbose=3,tensorboard_dir='/tmp/tflearn_logs/')\n",
    "            model.fit({'input': X}, {'target': Y}, n_epoch=num_epochs, validation_set=({'input': validX}, {'target': validY}),\n",
    "                      show_metric=True, batch_size=batch_size, callbacks=early_stopping_cb,run_id='resnext')\n",
    "            predict(model, Xte, ref, mode)\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    elif mode=='googlenet':\n",
    "        with m4.as_default():\n",
    "            early_stopping_cb = EarlyStoppingCallback(val_acc_thresh=0.95)\n",
    "            Input3 = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input',\n",
    "                                        data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "            network = regression(googlenet(Input3,classes),  optimizer='adam',loss='categorical_crossentropy',name ='target')\n",
    "            model = tflearn.DNN(network, tensorboard_verbose=3, tensorboard_dir='/tmp/tflearn_logs/')\n",
    "            model.fit({'input': X}, {'target': Y}, n_epoch=num_epochs, validation_set=({'input': validX}, {'target': validY}),\n",
    "                      show_metric=True,  batch_size=batch_size, callbacks=early_stopping_cb, run_id='googlenet')\n",
    "            predict(model, Xte, ref, mode)\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    elif mode=='vgg':\n",
    "        with m5.as_default():\n",
    "            early_stopping_cb = EarlyStoppingCallback(val_acc_thresh=0.95)\n",
    "            Input4 = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input',\n",
    "                                        data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "            network = regression(vgg(Input4,classes), optimizer='adam',loss='categorical_crossentropy', name ='target')\n",
    "            model = tflearn.DNN(network,tensorboard_verbose=3, tensorboard_dir='/tmp/tflearn_logs/')\n",
    "            model.fit({'input': X}, {'target': Y}, n_epoch=num_epochs, validation_set=({'input': validX}, {'target': validY}),\n",
    "                      show_metric=True,  batch_size=batch_size, callbacks=early_stopping_cb, run_id='vgg')\n",
    "            predict(model, Xte, ref, mode)\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    else:\n",
    "        print(\"Please Select Mode in ['resnet32', 'inception_resnet_v2', 'resnext', 'googlenet','vgg']\")\n",
    "\n",
    "    model.save(get_save_path(mode))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "---------------------------------\n",
      "Run id: resnet32\n",
      "Log directory: /tmp/tflearn_logs/\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0d2aad6bd35c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_save_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mhypotheses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-80e296f8ad08>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(mode, num_epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtensorboard_verbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtensorboard_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/tmp/tflearn_logs/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             model.fit({'input': X}, {'target': Y}, n_epoch=num_epochs, validation_set=({'input': validX}, {'target': validY}),\n\u001b[1;32m---> 18\u001b[1;33m                       show_metric=True, callbacks=early_stopping_cb,batch_size=100, run_id='resnet32')\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXte\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[0;32m    214\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                          callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 train_op.initialize_fit(feed_dicts[i], vd, dprep_dict,\n\u001b[0;32m    287\u001b[0m                                         \u001b[0mdaug_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_metric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m                                         self.summ_writer, self.coord)\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[1;31m# Prepare TermLogger for training diplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36minitialize_fit\u001b[1;34m(self, feed_dict, val_feed_dict, dprep_dict, daug_dict, show_metric, summ_writer, coord)\u001b[0m\n\u001b[0;32m    769\u001b[0m                 \u001b[1;32massert\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m                     \u001b[1;34m\"Unknown DataPreprocessing dict key!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m                 \u001b[0mdprep_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         self.train_dflow = data_flow.FeedDictFlow(feed_dict, coord,\n\u001b[0;32m    773\u001b[0m                                                   \u001b[0mcontinuous\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\data_preprocessing.py\u001b[0m in \u001b[0;36minitialize\u001b[1;34m(self, dataset, session, limit)\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_mean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_mean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# Otherwise, if it has not been restored, compute it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_mean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_restored\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"---------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 print(\"Preprocessing... Calculating mean over all dataset \"\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\data_preprocessing.py\u001b[0m in \u001b[0;36mis_restored\u001b[1;34m(self, session)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mis_restored\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_r\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, session)\u001b[0m\n\u001b[0;32m    508\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     \"\"\"\n\u001b[1;32m--> 510\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0minitialized_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m     \"\"\"\n\u001b[1;32m--> 570\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_dup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4450\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4452\u001b[1;33m       raise ValueError(\"Cannot use the given session to evaluate tensor: \"\n\u001b[0m\u001b[0;32m   4453\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4454\u001b[0m                        \"graph.\")\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    models = ['resnet32', 'inception_resnet_v2', 'resnext', 'googlenet','vgg']\n",
    "    ensemble = len(models)\n",
    "    hypotheses = np.zeros([ensemble, testY.shape[0], 2])\n",
    "    hypotheses_valid =np.zeros([ensemble, validY.shape[0], 2]) \n",
    "\n",
    "    #ensemble\n",
    "    for i in range(ensemble):\n",
    "        mode = models[i]\n",
    "        model = train(mode, 1)\n",
    "        model.load(get_save_path(mode))\n",
    "        hypotheses[i]= model.predict(testX)\n",
    "        hypotheses_valid[i]= model.predict(validX)       \n",
    "    print(\"Training finished\")\n",
    "\n",
    "    #ensemble_model weight training: usinsg validation data\n",
    "    hypotheses_valid = hypotheses_valid.transpose([1,0,2])\n",
    "    hypotheses = hypotheses.transpose([1,0,2])\n",
    "    hypothesis = np.zeros([testY.shape[0], 2])\n",
    "\n",
    "    ens=tf.Graph()\n",
    "\n",
    "    with ens.as_default():\n",
    "        ens_net =input_data(shape = [None, n_ensemble, 2], name = 'input')\n",
    "        ens_net = fully_connected(ens_net, 2, activation = 'softplus')\n",
    "        ens_net = regression(ens_net, optimizer ='adam', loss='categorical_crossentropy')\n",
    "\n",
    "        ens_model = tflearn.DNN(ens_net, tensorboard_verbose=3)\n",
    "        ens_model.fit(hypotheses_valid, validY, n_epoch = num_epochs, \n",
    "                      callbacks=early_stopping_cb, show_metric=True, run_id='ensemble')\n",
    "\n",
    "        hypothesis = ens_model.predict(hypotheses)\n",
    "        predict(ens_model, Xte, ref, ensemble) # text 파일 저장\n",
    "        ens_model.save(get_save_path('ensemble'))\n",
    "\n",
    "    correction = np.equal(np.argmax(hypothesis, 1), np.argmax(testY, 1))\n",
    "    accuracy_ens = np.mean(correction)\n",
    "\n",
    "    print(\"Ensemble Accuracy :\", accuracy_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. resnet32\n",
    "Training Step: 600  | total loss: 0.32918 | time: 33.578s\n",
    "| Adam | epoch: 050 | loss: 0.32918 - acc: 0.8812 | val_loss: 0.27509 - val_acc: 0.8867 -- iter: 1200/1200\n",
    "\n",
    "2. resnet32\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
