{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import pickle, random, copy, platform, os, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import email_sending\n",
    "import tflearn\n",
    "import tflearn.activations as activations\n",
    "from PIL import Image, ImageDraw\n",
    "from __future__ import division, print_function, absolute_import\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.activations import relu\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.conv import avg_pool_2d, conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import dropout, flatten, fully_connected, input_data\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "from tflearn.utils import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    with open('resnet_data', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        Xtr = data[0]\n",
    "        Xtr = np.reshape(Xtr, (-1,75,75,2))\n",
    "        Ytr = data[1]\n",
    "        Xte =data[2]\n",
    "        Xte = np.reshape(Xte, (-1,75,75,2))\n",
    "        rf =data[3]\n",
    "    return Xtr, Ytr, Xte, rf\n",
    "\n",
    "# expresses the label data in one-hot encoding.\n",
    "def onehot_encoding (Y, y_class):\n",
    "    Y_onehot = np.zeros((Y.size, int(y_class)))\n",
    "    for i in range(Y.size):\n",
    "        Y_onehot[i][Y[i]] = 1\n",
    "    return Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet32(classes, n = 5): #classes =2\n",
    "    x = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input',data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "    net = tflearn.conv_2d(x, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "    net = tflearn.residual_block(net, n, 16)\n",
    "    net = tflearn.residual_block(net, 1, 32, downsample=True)\n",
    "    net = tflearn.residual_block(net, n - 1, 32)\n",
    "    net = tflearn.residual_block(net, 1, 64, downsample=True)\n",
    "    net = tflearn.residual_block(net, n - 1, 64)\n",
    "    net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.activation(net, 'relu')\n",
    "    net = tflearn.global_avg_pool(net)\n",
    "    net = tflearn.fully_connected(net, classes, activation='softmax')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnext(classes, n =5):\n",
    "    x = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input',data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "    net = tflearn.conv_2d(x, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "    net = tflearn.resnext_block(net, n, 16, 32)\n",
    "    net = tflearn.resnext_block(net, 1, 32, 32, downsample=True)\n",
    "    net = tflearn.resnext_block(net, n-1, 32, 32)\n",
    "    net = tflearn.resnext_block(net, 1, 64, 32, downsample=True)\n",
    "    net = tflearn.resnext_block(net, n-1, 64, 32)\n",
    "    net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.activation(net, 'relu')\n",
    "    net = tflearn.global_avg_pool(net)\n",
    "    net = tflearn.fully_connected(net, classes, activation='softmax')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg(classes):\n",
    "    x = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input',data_preprocessing=img_prep,data_augmentation=img_aug)    \n",
    "    network = conv_2d(x, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = conv_2d(network, 128, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = conv_2d(network, 512, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, classes, activation='softmax')\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def block35(net, scale=1.0, activation=\"relu\"):\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None,name='Conv2d_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 32, 3, bias=False, activation=None,name='Conv2d_0b_3x3')))\n",
    "    tower_conv2_0 = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2_0, 48,3, bias=False, activation=None, name='Conv2d_0b_3x3')))\n",
    "    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 64,3, bias=False, activation=None, name='Conv2d_0c_3x3')))\n",
    "    tower_mixed = merge([tower_conv, tower_conv1_1, tower_conv2_2], mode='concat', axis=3)\n",
    "    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    net += scale * tower_out\n",
    "    if activation:\n",
    "        if isinstance(activation, str):\n",
    "            net = activations.get(activation)(net)\n",
    "        elif hasattr(activation, '__call__'):\n",
    "            net = activation(net)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation.\")\n",
    "    return net\n",
    "\n",
    "def block17(net, scale=1.0, activation=\"relu\"):\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    tower_conv_1_0 = relu(batch_normalization(conv_2d(net, 128, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv_1_1 = relu(batch_normalization(conv_2d(tower_conv_1_0, 160,[1,7], bias=False, activation=None,name='Conv2d_0b_1x7')))\n",
    "    tower_conv_1_2 = relu(batch_normalization(conv_2d(tower_conv_1_1, 192, [7,1], bias=False, activation=None,name='Conv2d_0c_7x1')))\n",
    "    tower_mixed = merge([tower_conv,tower_conv_1_2], mode='concat', axis=3)\n",
    "    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    net += scale * tower_out\n",
    "    if activation:\n",
    "        if isinstance(activation, str):\n",
    "            net = activations.get(activation)(net)\n",
    "        elif hasattr(activation, '__call__'):\n",
    "            net = activation(net)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation.\")\n",
    "    return net\n",
    "\n",
    "\n",
    "def block8(net, scale=1.0, activation=\"relu\"):\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 224, [1,3], bias=False, activation=None, name='Conv2d_0b_1x3')))\n",
    "    tower_conv1_2 = relu(batch_normalization(conv_2d(tower_conv1_1, 256, [3,1], bias=False, name='Conv2d_0c_3x1')))\n",
    "    tower_mixed = merge([tower_conv,tower_conv1_2], mode='concat', axis=3)\n",
    "    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    net += scale * tower_out\n",
    "    if activation:\n",
    "        if isinstance(activation, str):\n",
    "            net = activations.get(activation)(net)\n",
    "        elif hasattr(activation, '__call__'):\n",
    "            net = activation(net)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation.\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inception_resnet_v2(classes):\n",
    "    x = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input',data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "    dropout_keep_prob = 0.8\n",
    "\n",
    "    conv1a_3_3 = relu(batch_normalization(conv_2d(x, 32, 3, strides=2, bias=False, padding='VALID',activation=None,name='Conv2d_1a_3x3')))\n",
    "    conv2a_3_3 = relu(batch_normalization(conv_2d(conv1a_3_3, 32, 3, bias=False, padding='VALID',activation=None, name='Conv2d_2a_3x3')))\n",
    "    conv2b_3_3 = relu(batch_normalization(conv_2d(conv2a_3_3, 64, 3, bias=False, activation=None, name='Conv2d_2b_3x3')))\n",
    "    maxpool3a_3_3 = max_pool_2d(conv2b_3_3, 3, strides=2, padding='VALID', name='MaxPool_3a_3x3')\n",
    "    conv3b_1_1 = relu(batch_normalization(conv_2d(maxpool3a_3_3, 80, 1, bias=False, padding='VALID',activation=None, name='Conv2d_3b_1x1')))\n",
    "    conv4a_3_3 = relu(batch_normalization(conv_2d(conv3b_1_1, 192, 3, bias=False, padding='VALID',activation=None, name='Conv2d_4a_3x3')))\n",
    "    maxpool5a_3_3 = max_pool_2d(conv4a_3_3, 3, strides=2, padding='VALID', name='MaxPool_5a_3x3')\n",
    "\n",
    "    tower_conv = relu(batch_normalization(conv_2d(maxpool5a_3_3, 96, 1, bias=False, activation=None, name='Conv2d_5b_b0_1x1')))\n",
    "\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(maxpool5a_3_3, 48, 1, bias=False, activation=None, name='Conv2d_5b_b1_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 64, 5, bias=False, activation=None, name='Conv2d_5b_b1_0b_5x5')))\n",
    "\n",
    "    tower_conv2_0 = relu(batch_normalization(conv_2d(maxpool5a_3_3, 64, 1, bias=False, activation=None, name='Conv2d_5b_b2_0a_1x1')))\n",
    "    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2_0, 96, 3, bias=False, activation=None, name='Conv2d_5b_b2_0b_3x3')))\n",
    "    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 96, 3, bias=False, activation=None,name='Conv2d_5b_b2_0c_3x3')))\n",
    "\n",
    "    tower_pool3_0 = avg_pool_2d(maxpool5a_3_3, 3, strides=1, padding='same', name='AvgPool_5b_b3_0a_3x3')\n",
    "    tower_conv3_1 = relu(batch_normalization(conv_2d(tower_pool3_0, 64, 1, bias=False, activation=None,name='Conv2d_5b_b3_0b_1x1')))\n",
    "\n",
    "    tower_5b_out = merge([tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_1], mode='concat', axis=3)\n",
    "\n",
    "    net = repeat(tower_5b_out, 10, block35, scale=0.17)\n",
    "\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 384, 3, bias=False, strides=2,activation=None, padding='VALID', name='Conv2d_6a_b0_0a_3x3')))\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, activation=None, name='Conv2d_6a_b1_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 256, 3, bias=False, activation=None, name='Conv2d_6a_b1_0b_3x3')))\n",
    "    tower_conv1_2 = relu(batch_normalization(conv_2d(tower_conv1_1, 384, 3, bias=False, strides=2, padding='VALID', activation=None,name='Conv2d_6a_b1_0c_3x3')))\n",
    "    tower_pool = max_pool_2d(net, 3, strides=2, padding='VALID',name='MaxPool_1a_3x3')\n",
    "    net = merge([tower_conv, tower_conv1_2, tower_pool], mode='concat', axis=3)\n",
    "    net = repeat(net, 20, block17, scale=0.1)\n",
    "\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv0_1 = relu(batch_normalization(conv_2d(tower_conv, 384, 3, bias=False, strides=2, padding='VALID', activation=None,name='Conv2d_0a_1x1')))\n",
    "\n",
    "    tower_conv1 = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, padding='VALID', activation=None,name='Conv2d_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1,288,3, bias=False, strides=2, padding='VALID',activation=None, name='COnv2d_1a_3x3')))\n",
    "\n",
    "    tower_conv2 = relu(batch_normalization(conv_2d(net, 256,1, bias=False, activation=None,name='Conv2d_0a_1x1')))\n",
    "    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2, 288,3, bias=False, name='Conv2d_0b_3x3',activation=None)))\n",
    "    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 320, 3, bias=False, strides=2, padding='VALID',activation=None, name='Conv2d_1a_3x3')))\n",
    "\n",
    "    tower_pool = max_pool_2d(net, 3, strides=2, padding='VALID', name='MaxPool_1a_3x3')\n",
    "    net = merge([tower_conv0_1, tower_conv1_1,tower_conv2_2, tower_pool], mode='concat', axis=3)\n",
    "\n",
    "    net = repeat(net, 9, block8, scale=0.2)\n",
    "    net = block8(net, activation=None)\n",
    "\n",
    "    net = relu(batch_normalization(conv_2d(net, 1536, 1, bias=False, activation=None, name='Conv2d_7b_1x1')))\n",
    "    net = avg_pool_2d(net, net.get_shape().as_list()[1:3],strides=2, padding='VALID', name='AvgPool_1a_8x8')\n",
    "    net = flatten(net)\n",
    "    net = dropout(net, dropout_keep_prob)\n",
    "    loss = fully_connected(net, classes,activation='softmax')\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def googlenet(classes):\n",
    "    x = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input',data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "    conv1_7_7 = conv_2d(x, 64, 7, strides=2, activation='relu', name='conv1_7_7_s2')\n",
    "    pool1_3_3 = max_pool_2d(conv1_7_7, 3, strides=2)\n",
    "    pool1_3_3 = local_response_normalization(pool1_3_3)\n",
    "    conv2_3_3_reduce = conv_2d(pool1_3_3, 64, 1, activation='relu', name='conv2_3_3_reduce')\n",
    "    conv2_3_3 = conv_2d(conv2_3_3_reduce, 192, 3, activation='relu', name='conv2_3_3')\n",
    "    conv2_3_3 = local_response_normalization(conv2_3_3)\n",
    "    pool2_3_3 = max_pool_2d(conv2_3_3, kernel_size=3, strides=2, name='pool2_3_3_s2')\n",
    "\n",
    "    # 3a\n",
    "    inception_3a_1_1 = conv_2d(pool2_3_3, 64, 1, activation='relu', name='inception_3a_1_1')\n",
    "    inception_3a_3_3_reduce = conv_2d(pool2_3_3, 96, 1, activation='relu', name='inception_3a_3_3_reduce')\n",
    "    inception_3a_3_3 = conv_2d(inception_3a_3_3_reduce, 128, filter_size=3,  activation='relu', name='inception_3a_3_3')\n",
    "    inception_3a_5_5_reduce = conv_2d(pool2_3_3, 16, filter_size=1, activation='relu', name='inception_3a_5_5_reduce')\n",
    "    inception_3a_5_5 = conv_2d(inception_3a_5_5_reduce, 32, filter_size=5, activation='relu', name='inception_3a_5_5')\n",
    "    inception_3a_pool = max_pool_2d(pool2_3_3, kernel_size=3, strides=1, name='inception_3a_pool')\n",
    "    inception_3a_pool_1_1 = conv_2d(inception_3a_pool, 32, filter_size=1, activation='relu', name='inception_3a_pool_1_1')\n",
    "    inception_3a_output = merge([inception_3a_1_1, inception_3a_3_3, inception_3a_5_5, inception_3a_pool_1_1], mode='concat', axis=3)\n",
    "\n",
    "    # 3b\n",
    "    inception_3b_1_1 = conv_2d(inception_3a_output, 128, filter_size=1, activation='relu', name='inception_3b_1_1')\n",
    "    inception_3b_3_3_reduce = conv_2d(inception_3a_output, 128, filter_size=1, activation='relu', name='inception_3b_3_3_reduce')\n",
    "    inception_3b_3_3 = conv_2d(inception_3b_3_3_reduce, 192, filter_size=3, activation='relu', name='inception_3b_3_3')\n",
    "    inception_3b_5_5_reduce = conv_2d(inception_3a_output, 32, filter_size=1, activation='relu', name='inception_3b_5_5_reduce')\n",
    "    inception_3b_5_5 = conv_2d(inception_3b_5_5_reduce, 96, filter_size=5,  name='inception_3b_5_5')\n",
    "    inception_3b_pool = max_pool_2d(inception_3a_output, kernel_size=3, strides=1,  name='inception_3b_pool')\n",
    "    inception_3b_pool_1_1 = conv_2d(inception_3b_pool, 64, filter_size=1, activation='relu', name='inception_3b_pool_1_1')\n",
    "    inception_3b_output = merge([inception_3b_1_1, inception_3b_3_3, inception_3b_5_5, inception_3b_pool_1_1], mode='concat', axis=3, name='inception_3b_output')\n",
    "    pool3_3_3 = max_pool_2d(inception_3b_output, kernel_size=3, strides=2, name='pool3_3_3')\n",
    "\n",
    "    # 4a\n",
    "    inception_4a_1_1 = conv_2d(pool3_3_3, 192, filter_size=1, activation='relu', name='inception_4a_1_1')\n",
    "    inception_4a_3_3_reduce = conv_2d(pool3_3_3, 96, filter_size=1, activation='relu', name='inception_4a_3_3_reduce')\n",
    "    inception_4a_3_3 = conv_2d(inception_4a_3_3_reduce, 208, filter_size=3,  activation='relu', name='inception_4a_3_3')\n",
    "    inception_4a_5_5_reduce = conv_2d(pool3_3_3, 16, filter_size=1, activation='relu', name='inception_4a_5_5_reduce')\n",
    "    inception_4a_5_5 = conv_2d(inception_4a_5_5_reduce, 48, filter_size=5,  activation='relu', name='inception_4a_5_5')\n",
    "    inception_4a_pool = max_pool_2d(pool3_3_3, kernel_size=3, strides=1,  name='inception_4a_pool')\n",
    "    inception_4a_pool_1_1 = conv_2d(inception_4a_pool, 64, filter_size=1, activation='relu', name='inception_4a_pool_1_1')\n",
    "    inception_4a_output = merge([inception_4a_1_1, inception_4a_3_3, inception_4a_5_5, inception_4a_pool_1_1], mode='concat', axis=3, name='inception_4a_output')\n",
    "\n",
    "    # 4b\n",
    "    inception_4b_1_1 = conv_2d(inception_4a_output, 160, filter_size=1, activation='relu', name='inception_4a_1_1')\n",
    "    inception_4b_3_3_reduce = conv_2d(inception_4a_output, 112, filter_size=1, activation='relu', name='inception_4b_3_3_reduce')\n",
    "    inception_4b_3_3 = conv_2d(inception_4b_3_3_reduce, 224, filter_size=3, activation='relu', name='inception_4b_3_3')\n",
    "    inception_4b_5_5_reduce = conv_2d(inception_4a_output, 24, filter_size=1, activation='relu', name='inception_4b_5_5_reduce')\n",
    "    inception_4b_5_5 = conv_2d(inception_4b_5_5_reduce, 64, filter_size=5,  activation='relu', name='inception_4b_5_5')\n",
    "    inception_4b_pool = max_pool_2d(inception_4a_output, kernel_size=3, strides=1,  name='inception_4b_pool')\n",
    "    inception_4b_pool_1_1 = conv_2d(inception_4b_pool, 64, filter_size=1, activation='relu', name='inception_4b_pool_1_1')\n",
    "    inception_4b_output = merge([inception_4b_1_1, inception_4b_3_3, inception_4b_5_5, inception_4b_pool_1_1], mode='concat', axis=3, name='inception_4b_output')\n",
    "\n",
    "    # 4c\n",
    "    inception_4c_1_1 = conv_2d(inception_4b_output, 128, filter_size=1, activation='relu', name='inception_4c_1_1')\n",
    "    inception_4c_3_3_reduce = conv_2d(inception_4b_output, 128, filter_size=1, activation='relu', name='inception_4c_3_3_reduce')\n",
    "    inception_4c_3_3 = conv_2d(inception_4c_3_3_reduce, 256,  filter_size=3, activation='relu', name='inception_4c_3_3')\n",
    "    inception_4c_5_5_reduce = conv_2d(inception_4b_output, 24, filter_size=1, activation='relu', name='inception_4c_5_5_reduce')\n",
    "    inception_4c_5_5 = conv_2d(inception_4c_5_5_reduce, 64,  filter_size=5, activation='relu', name='inception_4c_5_5')\n",
    "    inception_4c_pool = max_pool_2d(inception_4b_output, kernel_size=3, strides=1)\n",
    "    inception_4c_pool_1_1 = conv_2d(inception_4c_pool, 64, filter_size=1, activation='relu', name='inception_4c_pool_1_1')\n",
    "    inception_4c_output = merge([inception_4c_1_1, inception_4c_3_3, inception_4c_5_5, inception_4c_pool_1_1], mode='concat', axis=3, name='inception_4c_output')\n",
    "\n",
    "    # 4d\n",
    "    inception_4d_1_1 = conv_2d(inception_4c_output, 112, filter_size=1, activation='relu', name='inception_4d_1_1')\n",
    "    inception_4d_3_3_reduce = conv_2d(inception_4c_output, 144, filter_size=1, activation='relu', name='inception_4d_3_3_reduce')\n",
    "    inception_4d_3_3 = conv_2d(inception_4d_3_3_reduce, 288, filter_size=3, activation='relu', name='inception_4d_3_3')\n",
    "    inception_4d_5_5_reduce = conv_2d(inception_4c_output, 32, filter_size=1, activation='relu', name='inception_4d_5_5_reduce')\n",
    "    inception_4d_5_5 = conv_2d(inception_4d_5_5_reduce, 64, filter_size=5,  activation='relu', name='inception_4d_5_5')\n",
    "    inception_4d_pool = max_pool_2d(inception_4c_output, kernel_size=3, strides=1,  name='inception_4d_pool')\n",
    "    inception_4d_pool_1_1 = conv_2d(inception_4d_pool, 64, filter_size=1, activation='relu', name='inception_4d_pool_1_1')\n",
    "    inception_4d_output = merge([inception_4d_1_1, inception_4d_3_3, inception_4d_5_5, inception_4d_pool_1_1], mode='concat', axis=3, name='inception_4d_output')\n",
    "\n",
    "    # 4e\n",
    "    inception_4e_1_1 = conv_2d(inception_4d_output, 256, filter_size=1, activation='relu', name='inception_4e_1_1')\n",
    "    inception_4e_3_3_reduce = conv_2d(inception_4d_output, 160, filter_size=1, activation='relu', name='inception_4e_3_3_reduce')\n",
    "    inception_4e_3_3 = conv_2d(inception_4e_3_3_reduce, 320, filter_size=3, activation='relu', name='inception_4e_3_3')\n",
    "    inception_4e_5_5_reduce = conv_2d(inception_4d_output, 32, filter_size=1, activation='relu', name='inception_4e_5_5_reduce')\n",
    "    inception_4e_5_5 = conv_2d(inception_4e_5_5_reduce, 128,  filter_size=5, activation='relu', name='inception_4e_5_5')\n",
    "    inception_4e_pool = max_pool_2d(inception_4d_output, kernel_size=3, strides=1,  name='inception_4e_pool')\n",
    "    inception_4e_pool_1_1 = conv_2d(inception_4e_pool, 128, filter_size=1, activation='relu', name='inception_4e_pool_1_1')\n",
    "    inception_4e_output = merge([inception_4e_1_1, inception_4e_3_3, inception_4e_5_5, inception_4e_pool_1_1], axis=3, mode='concat')\n",
    "    pool4_3_3 = max_pool_2d(inception_4e_output, kernel_size=3, strides=2, name='pool_3_3')\n",
    "\n",
    "    # 5a\n",
    "    inception_5a_1_1 = conv_2d(pool4_3_3, 256, filter_size=1, activation='relu', name='inception_5a_1_1')\n",
    "    inception_5a_3_3_reduce = conv_2d(pool4_3_3, 160, filter_size=1, activation='relu', name='inception_5a_3_3_reduce')\n",
    "    inception_5a_3_3 = conv_2d(inception_5a_3_3_reduce, 320, filter_size=3, activation='relu', name='inception_5a_3_3')\n",
    "    inception_5a_5_5_reduce = conv_2d(pool4_3_3, 32, filter_size=1, activation='relu', name='inception_5a_5_5_reduce')\n",
    "    inception_5a_5_5 = conv_2d(inception_5a_5_5_reduce, 128, filter_size=5,  activation='relu', name='inception_5a_5_5')\n",
    "    inception_5a_pool = max_pool_2d(pool4_3_3, kernel_size=3, strides=1,  name='inception_5a_pool')\n",
    "    inception_5a_pool_1_1 = conv_2d(inception_5a_pool, 128, filter_size=1, activation='relu', name='inception_5a_pool_1_1')\n",
    "    inception_5a_output = merge([inception_5a_1_1, inception_5a_3_3, inception_5a_5_5, inception_5a_pool_1_1], axis=3, mode='concat')\n",
    "\n",
    "    # 5b\n",
    "    inception_5b_1_1 = conv_2d(inception_5a_output, 384, filter_size=1, activation='relu', name='inception_5b_1_1')\n",
    "    inception_5b_3_3_reduce = conv_2d(inception_5a_output, 192, filter_size=1, activation='relu', name='inception_5b_3_3_reduce')\n",
    "    inception_5b_3_3 = conv_2d(inception_5b_3_3_reduce, 384,  filter_size=3, activation='relu', name='inception_5b_3_3')\n",
    "    inception_5b_5_5_reduce = conv_2d(inception_5a_output, 48, filter_size=1, activation='relu', name='inception_5b_5_5_reduce')\n",
    "    inception_5b_5_5 = conv_2d(inception_5b_5_5_reduce, 128, filter_size=5, activation='relu', name='inception_5b_5_5')\n",
    "    inception_5b_pool = max_pool_2d(inception_5a_output, kernel_size=3, strides=1,  name='inception_5b_pool')\n",
    "    inception_5b_pool_1_1 = conv_2d(inception_5b_pool, 128, filter_size=1, activation='relu', name='inception_5b_pool_1_1')\n",
    "    inception_5b_output = merge([inception_5b_1_1, inception_5b_3_3, inception_5b_5_5, inception_5b_pool_1_1], axis=3, mode='concat')\n",
    "    pool5_7_7 = avg_pool_2d(inception_5b_output, kernel_size=7, strides=1)\n",
    "    pool5_7_7 = dropout(pool5_7_7, 0.4)\n",
    "\n",
    "    # fc\n",
    "    net = fully_connected(pool5_7_7, classes, activation='softmax')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, test_data, ref, mode):\n",
    "    #8424=8*81*13\n",
    "    batch_size =104\n",
    "    iteration = int(len(test_data)/batch_size)  #iter=81  \n",
    "    # Load a model\n",
    "    #model.load(get_save_path(mode))\n",
    "    with open(\"C:/Users/User/Desktop/kaggle/result_{0}.txt\".format(mode), \"w\") as f:\n",
    "        f.write(\"id,is_iceberg\\n\")\n",
    "        for i in range(iteration):\n",
    "            pred = model.predict(test_data[i*batch_size: (i+1)*batch_size])\n",
    "            for j in range(batch_size):\n",
    "                data = \"%s,%f\\n\" % (ref[i*batch_size+j], pred[j][1])\n",
    "                f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dir = 'checkpoints/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "def get_save_path(net_name):\n",
    "    return save_dir + str(net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(tflearn.callbacks.Callback):\n",
    "    def __init__(self, val_acc_thresh):\n",
    "        \"\"\" Note: We are free to define our init function however we please. \"\"\"\n",
    "        self.val_acc_thresh = val_acc_thresh\n",
    "    \n",
    "    def on_epoch_end(self, training_state):\n",
    "        \"\"\" \"\"\"\n",
    "        # Apparently this can happen.\n",
    "        if training_state.val_acc is None: return\n",
    "        if training_state.val_acc > self.val_acc_thresh:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "classes = 2\n",
    "\n",
    "#load data\n",
    "Xtr, Ytr, Xte, ref = read_data()\n",
    "Xtr, Ytr = tflearn.data_utils.shuffle(Xtr, Ytr)\n",
    "Ytr = onehot_encoding(Ytr, classes)\n",
    "X, Y = Xtr[:1200], Ytr[:1200]\n",
    "validX, validY =Xtr[1200:1500], Ytr[1200:1500]\n",
    "testX, testY = Xtr[1500:],Ytr[1500:]\n",
    "hypotheses = np.zeros([1, testY.shape[0], 2])\n",
    "hypotheses_valid =np.zeros([1, validY.shape[0], 2])\n",
    "\n",
    "#data preprocessing\n",
    "img_prep = tflearn.ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center()\n",
    "img_prep.add_featurewise_stdnorm()\n",
    "#img_prep.add_custom_preprocessing (func) <-함수 적용가능\n",
    "#img_prep.add_custom_preprocessing (func)\n",
    "#img_prep.add_zca_whitening (pc=None)\n",
    "\n",
    "img_aug = tflearn.ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "img_aug.add_random_90degrees_rotation(rotations=[0, 1, 2, 3])\n",
    "img_aug.add_random_crop([75,75,2], padding=4)\n",
    "\n",
    "# Initializae our callback.\n",
    "#early_stopping_cb = EarlyStoppingCallback(val_acc_thresh=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def data_process(classes=2):\n",
    "    \n",
    "    with open('resnet_data', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        Xtr = data[0]\n",
    "        Xtr = np.reshape(Xtr, (-1,75,75,2))\n",
    "        Ytr = data[1]\n",
    "        Xte =data[2]\n",
    "        Xte = np.reshape(Xte, (-1,75,75,2))\n",
    "        rf =data[3]\n",
    "        \n",
    "    Xtr, Ytr = tflearn.data_utils.shuffle(Xtr, Ytr)\n",
    "    Y_onehot = np.zeros((Y.size, classes)\n",
    "    for i in range(Y.size):\n",
    "        Y_onehot[i][Y[i]] = 1\n",
    "\n",
    "    X, Y = Xtr[:1200], Y_onehot[:1200]\n",
    "    validX, validY =Xtr[1200:1500], Y_onehot[1200:1500]\n",
    "    testX, testY = Xtr[1500:],Y_onehot[1500:]\n",
    "    hypotheses = np.zeros([1, testY.shape[0], 2])\n",
    "    hypotheses_valid =np.zeros([1, validY.shape[0], 2])\n",
    "\n",
    "    #data preprocessing\n",
    "    img_prep = tflearn.ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "    #img_prep.add_custom_preprocessing (func) <-함수 적용가능: 추가사항\n",
    "\n",
    "    img_aug = tflearn.ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_90degrees_rotation(rotations=[0, 1, 2, 3])\n",
    "    img_aug.add_random_crop([75,75,2], padding=4)\n",
    "    \n",
    "    Input = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input', \n",
    "                               data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "    return Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mode, num_epochs = 20, batch_size=100):\n",
    "    #{'input': X},{'target': Y} ({'input': validX}, {'target': validY})\n",
    "    # Initializae our callback.\n",
    "    early_stopping_cb = EarlyStoppingCallback(val_acc_thresh=0.95)\n",
    "    \n",
    "    if mode =='resnet32':\n",
    "        network = tflearn.regression(resnet32(classes),optimizer='adam',loss='categorical_crossentropy', name ='target')\n",
    "        model = tflearn.DNN(network,tensorboard_verbose=3,tensorboard_dir='c:\\\\tflearn_logs\\\\')\n",
    "        model.fit({'input': X}, {'target': Y}, n_epoch=num_epochs, validation_set=({'input': validX}, {'target': validY}),\n",
    "                  show_metric=True, callbacks=early_stopping_cb,batch_size=batch_size, run_id='resnet32')\n",
    "        predict(model, Xte, ref, mode)\n",
    "\n",
    "    elif mode =='inception_resnet_v2':        \n",
    "        #Input1 = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input', data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "        network = tflearn.regression(inception_resnet_v2(classes),\n",
    "                                     optimizer='adam',loss='categorical_crossentropy',name ='target')\n",
    "        model = tflearn.DNN(network,tensorboard_verbose=3,tensorboard_dir='c:\\\\tflearn_logs\\\\')\n",
    "        model.fit({'input': X}, {'target': Y}, n_epoch=num_epochs, show_metric=True, batch_size=batch_size,\n",
    "                  validation_set=({'input': validX}, {'target': validY}),callbacks=early_stopping_cb, run_id ='inception_resnet_v2')\n",
    "        predict(model, Xte, ref, mode)\n",
    "\n",
    "    elif mode=='resnext':\n",
    "        #Input2 = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input',data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "        network = tflearn.regression(resnext(classes), optimizer='adam',loss='categorical_crossentropy', name ='target')\n",
    "        model = tflearn.DNN(network, tensorboard_verbose=3,tensorboard_dir='c:\\\\tflearn_logs\\\\')\n",
    "        model.fit({'input': X}, {'target': Y}, n_epoch=num_epochs, validation_set=({'input': validX}, {'target': validY}),\n",
    "                  show_metric=True, batch_size=batch_size, callbacks=early_stopping_cb,run_id='resnext')\n",
    "        predict(model, Xte, ref, mode)\n",
    "        \n",
    "    elif mode=='googlenet':\n",
    "        #Input3 = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input',data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "        network = regression(googlenet(classes),  optimizer='adam',loss='categorical_crossentropy',name ='target')\n",
    "        model = tflearn.DNN(network, tensorboard_verbose=3, tensorboard_dir='c:\\\\tflearn_logs\\\\')\n",
    "        model.fit({'input': X}, {'target': Y}, n_epoch=num_epochs, validation_set=({'input': validX}, {'target': validY}),\n",
    "                  show_metric=True,  batch_size=batch_size, callbacks=early_stopping_cb, run_id='googlenet')\n",
    "        predict(model, Xte, ref, mode)\n",
    "\n",
    "    elif mode=='vgg':\n",
    "        #Input4 = tflearn.input_data(shape=[None, 75, 75, 2], dtype='float', name ='input',data_preprocessing=img_prep,data_augmentation=img_aug)\n",
    "        network = regression(vgg(classes), optimizer='adam',loss='categorical_crossentropy', name ='target')\n",
    "        model = tflearn.DNN(network,tensorboard_verbose=3, tensorboard_dir='c:\\\\tflearn_logs\\\\')\n",
    "        model.fit({'input': X}, {'target': Y}, n_epoch=num_epochs, validation_set=({'input': validX}, {'target': validY}),\n",
    "                  show_metric=True,  batch_size=batch_size, callbacks=early_stopping_cb, run_id='vgg')\n",
    "        predict(model, Xte, ref, mode)\n",
    "\n",
    "    else:\n",
    "        print(\"Please Select Mode in ['resnet32', 'inception_resnet_v2', 'resnext', 'googlenet','vgg']\")\n",
    "\n",
    "    model.save(get_save_path(mode))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tflearn\\initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "---------------------------------\n",
      "Run id: resnet32\n",
      "Log directory: c:\\tflearn_logs\\/\n",
      "---------------------------------\n",
      "Preprocessing... Calculating mean over all dataset (this may take long)...\n",
      "Mean: -23.4786838639 (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\n",
      "---------------------------------\n",
      "Preprocessing... Calculating std over all dataset (this may take long)...\n",
      "STD: 5.23588131069 (To avoid repetitive computation, add it to argument 'std' of `add_featurewise_stdnorm`)\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1200\n",
      "Validation samples: 300\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    models = ['resnet32','googlenet','vgg','inception_resnet_v2', 'resnext']\n",
    "    ensemble = len(models)\n",
    "    hypotheses = np.zeros([ensemble, testY.shape[0], 2])\n",
    "    hypotheses_valid =np.zeros([ensemble, validY.shape[0], 2]) \n",
    "\n",
    "    #ensemble\n",
    "    for i in range(ensemble):\n",
    "        mode = models[i]\n",
    "        model = train(mode, 1)\n",
    "        model.load(get_save_path(mode))\n",
    "        hypotheses[i]= model.predict(testX)\n",
    "        hypotheses_valid[i]= model.predict(validX)       \n",
    "    print(\"Training finished\")\n",
    "\n",
    "    #ensemble_model weight training: usinsg validation data\n",
    "    hypotheses_valid = hypotheses_valid.transpose([1,0,2])\n",
    "    hypotheses = hypotheses.transpose([1,0,2])\n",
    "    hypothesis = np.zeros([testY.shape[0], 2])\n",
    "\n",
    "    ens=tf.Graph()\n",
    "\n",
    "    with ens.as_default():\n",
    "        ens_net =input_data(shape = [None, n_ensemble, 2], name = 'input')\n",
    "        ens_net = fully_connected(ens_net, 2, activation = 'softplus')\n",
    "        ens_net = regression(ens_net, optimizer ='adam', loss='categorical_crossentropy')\n",
    "\n",
    "        ens_model = tflearn.DNN(ens_net, tensorboard_verbose=3)\n",
    "        ens_model.fit(hypotheses_valid, validY, n_epoch = num_epochs, \n",
    "                      callbacks=early_stopping_cb, show_metric=True, run_id='ensemble')\n",
    "\n",
    "        hypothesis = ens_model.predict(hypotheses)\n",
    "        predict(ens_model, Xte, ref, ensemble) # text 파일 저장\n",
    "        ens_model.save(get_save_path('ensemble'))\n",
    "\n",
    "    correction = np.equal(np.argmax(hypothesis, 1), np.argmax(testY, 1))\n",
    "    accuracy_ens = np.mean(correction)\n",
    "\n",
    "    print(\"Ensemble Accuracy :\", accuracy_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. resnet32\n",
    "Training Step: 600  | total loss: 0.32918 | time: 33.578s\n",
    "| Adam | epoch: 050 | loss: 0.32918 - acc: 0.8812 | val_loss: 0.27509 - val_acc: 0.8867 -- iter: 1200/1200\n",
    "\n",
    "2. resnet32\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
